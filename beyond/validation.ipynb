{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation\n",
    "\n",
    "Model validation is a crucial step in the machine learning workflow that ensures the performance and reliability of a model before it is deployed in real-world applications. It involves assessing how well a model generalizes to unseen data, which helps in identifying potential issues such as overfitting or underfitting.\n",
    "\n",
    "\n",
    "\n",
    "## Training and Test Sets\n",
    "\n",
    "For effective model validation, the dataset is typically divided into atleast two subsets:\n",
    "\n",
    "1. **Training Set**: This subset is used to train the model. Training involves feeding the model with input data and corresponding labels so that it can learn patterns and relationships.\n",
    "\n",
    "2. **Test Set**: This subset is used to assess the final performance of the model after training and validation. It provides an unbiased evaluation of the model's generalization ability. <u>It is extremely important that the test set remains completely unseen during the training to ensure an unbiased evaluation of the model's performance.</u>\n",
    "\n",
    "\n",
    "<center><img src=\"https://fahadsultan.com/csc272_f23/_images/train_test.png\" alt=\"Train, Validation, Test Split\" width=\"100%\" style=\"filter:invert(1)\"/></center>\n",
    "\n",
    "In some cases, a third subset called the **Validation Set** is also used during the training process to tune hyperparameters and make decisions about model architecture. However, in simpler workflows, cross-validation techniques can be employed instead of a separate validation set.\n",
    "\n",
    "In sklearn, the `train_test_split` function from the `model_selection` module is commonly used to split the dataset into training and test sets. Here is an example:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Candidate</th>\n",
       "      <th>Party</th>\n",
       "      <th>Popular vote</th>\n",
       "      <th>Result</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1824</td>\n",
       "      <td>Andrew Jackson</td>\n",
       "      <td>Democratic-Republican</td>\n",
       "      <td>151271</td>\n",
       "      <td>loss</td>\n",
       "      <td>57.210122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1824</td>\n",
       "      <td>John Quincy Adams</td>\n",
       "      <td>Democratic-Republican</td>\n",
       "      <td>113142</td>\n",
       "      <td>win</td>\n",
       "      <td>42.789878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1828</td>\n",
       "      <td>Andrew Jackson</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>642806</td>\n",
       "      <td>win</td>\n",
       "      <td>56.203927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1828</td>\n",
       "      <td>John Quincy Adams</td>\n",
       "      <td>National Republican</td>\n",
       "      <td>500897</td>\n",
       "      <td>loss</td>\n",
       "      <td>43.796073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1832</td>\n",
       "      <td>Andrew Jackson</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>702735</td>\n",
       "      <td>win</td>\n",
       "      <td>54.574789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year          Candidate                  Party  Popular vote Result  \\\n",
       "0  1824     Andrew Jackson  Democratic-Republican        151271   loss   \n",
       "1  1824  John Quincy Adams  Democratic-Republican        113142    win   \n",
       "2  1828     Andrew Jackson             Democratic        642806    win   \n",
       "3  1828  John Quincy Adams    National Republican        500897   loss   \n",
       "4  1832     Andrew Jackson             Democratic        702735    win   \n",
       "\n",
       "           %  \n",
       "0  57.210122  \n",
       "1  42.789878  \n",
       "2  56.203927  \n",
       "3  43.796073  \n",
       "4  54.574789  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv\"\n",
    "\n",
    "elections = pd.read_csv(url)\n",
    "\n",
    "elections.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = elections[['Year', 'Popular vote']]\n",
    "\n",
    "y = elections['Result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((127, 2), (127,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55, 2), (55,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Performance Metrics\n",
    "\n",
    "Depending on the type of problem (classification, regression, etc.), different metrics are used to evaluate model performance. Common metrics include accuracy, precision, recall, F1-score for classification tasks, and mean squared error (MSE), R-squared for regression tasks.\n",
    "\n",
    "The most common metric for evaluating a classifier is **accuracy**. Accuracy is the proportion of correct predictions. It is the number of correct predictions divided by the total number of predictions.\n",
    "\n",
    "$$Accuracy = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$$\n",
    "\n",
    "For example, if we have a test set of 100 documents, and our classifier correctly predicts the class of 80 of them, then the accuracy is 80%.\n",
    "\n",
    "Accuracy is a good metric when the classes are _balanced_ $N_{class1} \\approx N_{class2}$. However, when the classes are imbalanced, accuracy can be misleading. For example, if we have a test set of 100 documents, and 95 of them are positive and 5 of them are negative, then a classifier that always predicts positive will have an accuracy of 95%. However, this classifier is not useful, because it never predicts negative.\n",
    "\n",
    "**Multi-class classification as multiple Binary classifications**\n",
    "\n",
    "Every multi-class classification problem can be decomposed into multiple binary classification problems. For example, if we have a multi-class classification problem with 3 classes, we can decompose it into 3 binary classification problems.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src=\"../assets/binary_multiclass.png\" width=\"100%\" style=\"filter:invert(1)\"/>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "Assuming the categorical variable that we are trying to predict is binary, we can define the accuracy in terms of the four possible outcomes of a binary classifier: \n",
    "\n",
    "1. True Positive (TP): The classifier correctly predicted the positive class.\n",
    "2. False Positive (FP): The classifier **incorrectly** predicted the negative class as positive.\n",
    "3. True Negative (TN): The classifier correctly predicted the negative class.\n",
    "4. False Negative (FN):  The classifier **incorrectly** predicted the positive class as negative.\n",
    "\n",
    "True positive means that the classifier correctly predicted the positive class. False positive means that the classifier incorrectly predicted the positive class. True negative means that the classifier correctly predicted the negative class. False negative means that the classifier incorrectly predicted the negative class.\n",
    "\n",
    "These definitions are summarized in the table below: \n",
    "\n",
    "|       | Prediction $\\hat{y} = f'(x)$ | Truth $y = f(x)$     |\n",
    "| :---        |    :----:   |          ---: |\n",
    "| True Negative (TN)    | 0        | 0   |\n",
    "| False Negative (FN)   | 0        | 1      |\n",
    "| False Positive (FP)   | 1        | 0      |\n",
    "| True Positive (TP)   | 1        | 1      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of the four outcomes above, the accuracy is:\n",
    "\n",
    "$$ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "Accuracy is a useful metric, but it can be misleading. \n",
    "\n",
    "Other metrics that are often used to evaluate classifiers are: \n",
    "\n",
    "* **Precision**: The proportion of positive predictions that are correct. Mathematically, it is defined as:\n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "* **Recall**: The proportion of positive instances that are correctly predicted. Mathematically, it is defined as:\n",
    "\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "The precision and recall are often combined into a single metric called the **F1 score**. The F1 score is the harmonic mean of precision and recall. The harmonic mean of two numbers is given by:\n",
    "\n",
    "* **F1 Score**: The harmonic mean of precision and recall.\n",
    "\n",
    "$$\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "\n",
    "<!-- $$Baseline\\ Accuracy = \\frac{Number\\ of\\ majority\\ class\\ predictions}{Total\\ number\\ of\\ predictions}$$ -->\n",
    "\n",
    "<!-- The baseline accuracy is the accuracy of the majority class classifier. It is the accuracy we would get if we just guessed the majority class for every instance. It is a useful baseline to compare our classifier to. If our classifier is not better than the baseline, then we should probably just use the baseline classifier.\n",
    "\n",
    "Another way to evaluate a classifier is to look at the confusion matrix. A confusion matrix is a table that shows the number of correct and incorrect predictions for each class. For example, if we have a test set of 100 documents, and our classifier correctly predicts the class of 80 of them, then the accuracy is 80%. But if we had just guessed the majority class for all of them, we would have gotten 50% accuracy. This is called the baseline accuracy.\n",
    "\n",
    "<img src=\"../assets/confusion_matrix.png\">\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../assets/classification.png\">\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../assets/training_testing.png\">\n",
    " -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Cross-Validation\n",
    "\n",
    "- A technique used to assess how the results of a statistical analysis will generalize to an independent dataset. Common methods include k-fold cross-validation and leave-one-out cross-validation.\n",
    "\n",
    "<img src=\"../assets/cross_validation.png\" width=\"100%\" style=\"filter:invert(1)\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and Underfitting\n",
    "\n",
    "- **Overfitting**: When a model learns the training data too well, including noise and outliers, leading to poor generalization on new data.\n",
    "\n",
    "- **Underfitting**: When a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both training and test sets.\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "- The process of optimizing the parameters that govern the training process of the model (e.g., learning rate, number of trees in a random forest) to improve performance on the validation set.\n",
    "\n",
    "## Baselines \n",
    "\n",
    "Establishing a baseline is an essential step in model validation. A baseline provides a reference point against which the performance of more complex models can be compared. It helps to determine whether a new model is actually improving upon simpler approaches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models\n",
    "\n",
    "1. **Simple Heuristic Baseline**:\n",
    "   - A straightforward approach that uses basic rules or averages. For example, in a classification task, predicting the majority class for all instances.\n",
    "\n",
    "2. **Random Baseline**:\n",
    "\n",
    "    - A model that makes random predictions. This is often used to demonstrate that a more sophisticated model performs better than chance.\n",
    "\n",
    "3. **Majority Class Baseline**:\n",
    "\n",
    "    - In classification tasks, this baseline predicts the most frequent class in the training data for all instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
