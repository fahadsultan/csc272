[
  {
    "objectID": "pandas/aggregation.html",
    "href": "pandas/aggregation.html",
    "title": "Aggregation",
    "section": "",
    "text": "In this notebook, we will learn how to aggregate data using pandas. This generally entails grouping data by a certain column’s values and then applying a function to the groups."
  },
  {
    "objectID": "pandas/preliminaries.html",
    "href": "pandas/preliminaries.html",
    "title": "Flat vs. Hierarchical Data",
    "section": "",
    "text": "Format is a term used to describe the way data is stored. For example, a single image is stored as a 2D array of pixels. A video is stored as a sequence of images. A sound is stored as a 1D array of samples. A text is stored as a sequence of characters.\n\n\n\nFlat data formats are native to pandas and are the simplest and the most ubiquitous file formats in general.\nTo avoid data redundancy, data is often factored into multiple tables. For example, in a database of a school, there may be a table for students, a table for teachers, a table for classes, a table for grades, etc. Depending on the question of interest, these tables are then joined together to form a single table.\n\n\nCSV is an open format used to store tabular data. It is a text file where each line is a row of data. In other words, each line is separated by newline character \\n. Within a row, each column is separated by a comma ,. The first row is optionally the header row containing the names of the columns.\nExample of a CSV file is the elections.csv file.\n\nimport pandas as pd \n\nurl  = 'https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv'\ndata = pd.read_csv(url)\ndata.head()\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n\n\n\n\n\nThe CSV format has many variations, each different in regard of the separator used. For instance, a particularly popular variant called TSV (Tab Separated Values) uses tab character \\t to separate columns instead of a comma. In other variants the column separator can also be a semicolon ; or a pipe |.\nExample of a TSV file is restaurants.tsv file.\nNote the use of sep='\\t' parameter in pd.read_csv() in the code below:\n\nimport pandas as pd \nurl  = \"https://raw.githubusercontent.com/fahadsultan/csc272/main/data/restaurants.tsv\"\ndata = pd.read_csv(url, sep='\\t', index_col=0)\ndata.head()\n\n\n\n\n\n\n\n\nbusiness_id\nbusiness_name\nbusiness_address\nbusiness_city\nbusiness_state\nbusiness_postal_code\nbusiness_latitude\nbusiness_longitude\nbusiness_location\nbusiness_phone_number\n...\ninspection_score\ninspection_type\nviolation_id\nviolation_description\nrisk_category\nNeighborhoods\nSF Find Neighborhoods\nCurrent Police Districts\nCurrent Supervisor Districts\nAnalysis Neighborhoods\n\n\n\n\n0\n835\nKam Po Kitchen\n801 Broadway St\nSan Francisco\nCA\n94133\n37.797223\n-122.410513\nPOINT (-122.410513 37.797223)\nNaN\n...\n88.0\nRoutine - Unscheduled\n835_20180917_103139\nImproper food storage\nLow Risk\n107.0\n107.0\n6.0\n3.0\n6.0\n\n\n1\n905\nWorking Girls' Cafe'\n0259 Kearny St\nSan Francisco\nCA\n94108\n37.790477\n-122.404033\nPOINT (-122.404033 37.790477)\nNaN\n...\n87.0\nRoutine - Unscheduled\n905_20190415_103114\nHigh risk vermin infestation\nHigh Risk\n19.0\n19.0\n6.0\n3.0\n8.0\n\n\n2\n1203\nTAWAN'S THAI FOOD\n4403 GEARY Blvd\nSan Francisco\nCA\n94118\n37.780834\n-122.466590\nPOINT (-122.46659 37.780834)\n1.415576e+10\n...\n77.0\nRoutine - Unscheduled\n1203_20170803_103120\nModerate risk food holding temperature\nModerate Risk\n5.0\n5.0\n8.0\n4.0\n11.0\n\n\n3\n1345\nCordon Bleu\n1574 California St\nSan Francisco\nCA\n94109\n37.790683\n-122.420264\nPOINT (-122.420264 37.790683)\nNaN\n...\n81.0\nRoutine - Unscheduled\n1345_20170928_103105\nImproper cooling methods\nHigh Risk\n105.0\n105.0\n4.0\n3.0\n21.0\n\n\n4\n1352\nLA TORTILLA\n495 Castro St B\nSan Francisco\nCA\n94114\n37.760954\n-122.434935\nPOINT (-122.434935 37.760954)\n1.415586e+10\n...\n74.0\nRoutine - Unscheduled\n1352_20180620_103177\nNon service animal\nLow Risk\n38.0\n38.0\n3.0\n5.0\n5.0\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n\n\nXLSX is a proprietary format also used to store tabular data. Unlike a CSV, an XLSX is not a plain text file that you can simply read in any text editor. In contrast, an XLSX is a binary file which can be read only in specific software such as Microsft Excel or OpenOffice Calc. An excel spreedsheet does a lot more than just store(tabular) data such as storing formulas, charts and images, etc.\nHowever, for our purposes here, the only distinction between a CSV file and an XLSX file is that a) an XLSX file can contain multiple sheets where each sheet is a table and b) you can read an XLSX file in pandas using the pd.read_excel() function.\npd.read_excel is very similar to pd.read_csv where the first input is the filename or filepath that you want to read. Other inputs such as header, names, index_col are the same as pd.read_csv. The only additional input is:\n\nsheet_name: (default: sheet_name=0) specifies the sheet number or sheet name to be read. Default is 0 which means the first sheet is read. If the file contains multiple sheets, then sheet_name=None should be used.\n\nYou can download a sample excel spreadsheet from here\nNote that the file contains two sheets: “All Data” and “Just US”\nThe line of code below reads in just the sheet labeled “Just US” from the spreadsheet, using the sheet_name parameter:\n\npd.read_excel?\n\n\nimport pandas as pd \ndata = pd.read_excel('../data/Financial Sample.xlsx', sheet_name='US Only')\ndata.head()\n\n\n\n\n\n\n\n\nSegment\nCountry\nProduct\nDiscount Band\nUnits Sold\nManufacturing Price\nSale Price\nGross Sales\nDiscounts\nSales\nCOGS\nProfit\nDate\nMonth Number\nMonth Name\nYear\n\n\n\n\n0\nMidmarket\nUnited States of America\nMontana\nNone\n615.0\n5\n15\n9225.0\n0.0\n9225.0\n6150.0\n3075.0\n41974\n12\nDecember\n2014\n\n\n1\nGovernment\nUnited States of America\nPaseo\nNone\n1143.0\n10\n7\n8001.0\n0.0\n8001.0\n5715.0\n2286.0\n41913\n10\nOctober\n2014\n\n\n2\nChannel Partners\nUnited States of America\nPaseo\nNone\n912.0\n10\n12\n10944.0\n0.0\n10944.0\n2736.0\n8208.0\n41579\n11\nNovember\n2013\n\n\n3\nEnterprise\nUnited States of America\nVelo\nNone\n2821.0\n120\n125\n352625.0\n0.0\n352625.0\n338520.0\n14105.0\n41852\n8\nAugust\n2014\n\n\n4\nChannel Partners\nUnited States of America\nAmarilla\nNone\n1953.0\n260\n12\n23436.0\n0.0\n23436.0\n5859.0\n17577.0\n41730\n4\nApril\n2014\n\n\n\n\n\n\n\nJust as you can write any pandas DataFrame to CSV file using df.to_csv(), you can write any DataFrame to XLSX file using the df.to_excel() function.\n\n\n\n\nHierarchical data formats are used to store data that is inherently hierarchical. These formats are particularly popular on the internet for exchange of information with web services using APIs (Application Programming Interfaces).\n\n\nXML is a format used to store hierarchical data. Data in XML is stored as a tree structure. This tree is constituent of nodes. Each node has a start tag and an end tag. The start tag is enclosed in angle brackets &lt; and &gt; e.g. &lt;name&gt;. The end tag is also enclosed in angle brackets but it also has a forward slash / after the opening angle bracket e.g. &lt;/name&gt;.\nThe start tag and the end tag together are called an element.\nThe start tag can optionally contain attributes. Attributes are name-value pairs. The value is enclosed in double quotes \". The start tag can optionally contain child elements. Child elements are enclosed between the start tag and the end tag. The end tag can optionally contain text. Text is the value of the node. The text is enclosed between the start tag and the end tag.\nThe first node in an XML file is called the root node.\nExample of an XML file is\n&lt;note&gt;\n&lt;to&gt;Tove&lt;/to&gt;\n&lt;from&gt;Jani&lt;/from&gt;\n&lt;heading&gt;Reminder&lt;/heading&gt;\n&lt;body&gt;Don't forget me this weekend!&lt;/body&gt;\n&lt;/note&gt;\n&lt;breakfast_menu&gt;\n    &lt;food&gt;\n        &lt;name&gt;Belgian Waffles&lt;/name&gt;\n        &lt;price&gt;$5.95&lt;/price&gt;\n        &lt;description&gt;Two of our famous Belgian Waffles with plenty of real maple syrup&lt;/description&gt;\n        &lt;calories&gt;650&lt;/calories&gt;\n    &lt;/food&gt;\n    &lt;food&gt;\n        &lt;name&gt;Strawberry Belgian Waffles&lt;/name&gt;\n        &lt;price&gt;$7.95&lt;/price&gt;\n        &lt;description&gt;Light Belgian waffles covered with strawberries and whipped cream&lt;/description&gt;\n        &lt;calories&gt;900&lt;/calories&gt;\n    &lt;/food&gt;\n    &lt;food&gt;\n        &lt;name&gt;Berry-Berry Belgian Waffles&lt;/name&gt;\n        &lt;price&gt;$8.95&lt;/price&gt;\n        &lt;description&gt;Light Belgian waffles covered with an assortment of fresh berries and whipped cream&lt;/description&gt;\n        &lt;calories&gt;900&lt;/calories&gt;\n    &lt;/food&gt;\n    &lt;food&gt;\n        &lt;name&gt;French Toast&lt;/name&gt;\n        &lt;price&gt;$4.50&lt;/price&gt;\n        &lt;description&gt;Thick slices made from our homemade sourdough bread&lt;/description&gt;\n        &lt;calories&gt;600&lt;/calories&gt;\n    &lt;/food&gt;\n    &lt;food&gt;\n        &lt;name&gt;Homestyle Breakfast&lt;/name&gt;\n        &lt;price&gt;$6.95&lt;/price&gt;\n        &lt;description&gt;Two eggs, bacon or sausage, toast, and our ever-popular hash browns&lt;/description&gt;\n        &lt;calories&gt;950&lt;/calories&gt;\n    &lt;/food&gt;\n&lt;/breakfast_menu&gt;\nOther XML files are: Breakfast Menu, Plant Catalog and CD catalog.\nIt is a text file where each line is a node of data. Each node has a name and a value. The name is separated from the value by a colon. The value is separated from the name by a colon. The first node is the root node. The root node contains the names of the nodes. The root node is separated from the data nodes by a blank line.\nIn pandas, you can read an XML file using the pd.read_xml() function. The first input to pd.read_xml() is the filename or filepath that you want to read. Other important parameters of the pd.read_xml() function are:\n\nxpath: (default: xpath=None) specifies the path to the node(s) to be read. Default is None which means the entire XML file is read. If you want to read a specific node, then xpath should be used.\nnamespaces: (default: namespaces=None) specifies the namespaces used in the XML file. Default is None which means no namespaces are used. If the XML file uses namespaces, then namespaces should be used.\nencoding: (default: encoding=None) specifies the encoding of the XML file. Default is None which means the encoding is automatically detected. If the XML file uses a specific encoding, then encoding should be used.\nerrors: (default: errors=None) specifies how to handle errors. Default is None which means the errors are ignored. If the XML file contains errors, then errors should be used.\n\n\ndata = pd.read_xml('https://www.w3schools.com/xml/simple.xml')\ndata.head()\n\n\n\n\n\n\n\n\nname\nprice\ndescription\ncalories\n\n\n\n\n0\nBelgian Waffles\n$5.95\nTwo of our famous Belgian Waffles with plenty ...\n650\n\n\n1\nStrawberry Belgian Waffles\n$7.95\nLight Belgian waffles covered with strawberrie...\n900\n\n\n2\nBerry-Berry Belgian Waffles\n$8.95\nLight Belgian waffles covered with an assortme...\n900\n\n\n3\nFrench Toast\n$4.50\nThick slices made from our homemade sourdough ...\n600\n\n\n4\nHomestyle Breakfast\n$6.95\nTwo eggs, bacon or sausage, toast, and our eve...\n950\n\n\n\n\n\n\n\n\n\n\nJSON is an open data format used to store hierarchical data. JSON data resembles a Python dictionary. It is a text file where each line is a key-value pair. The key is separated from the value by a colon. Similar to a Python dictionary, the value can be a string, a number, a dictionary, a boolean or a list.\nJSON data can be oriented in two ways: records and columns.\nIn the records orientation, each line is a record.\n[\n  {\n    \"name\": \"Belgian Waffles\",\n    \"price\": \"$5.95\",\n    \"description\": \"Two of our famous Belgian Waffles with plenty of real maple syrup\",\n    \"calories\": 650\n  },\n  {\n    \"name\": \"Strawberry Belgian Waffles\",\n    \"price\": \"$7.95\",\n    \"description\": \"Light Belgian waffles covered with strawberries and whipped cream\",\n    \"calories\": 900\n  },\n  {\n    \"name\": \"Berry-Berry Belgian Waffles\",\n    \"price\": \"$8.95\",\n    \"description\": \"Light Belgian waffles covered with an assortment of fresh berries and whipped cream\",\n    \"calories\": 900\n  },\n  {\n    \"name\": \"French Toast\",\n    \"price\": \"$4.50\",\n    \"description\": \"Thick slices made from our homemade sourdough bread\",\n    \"calories\": 600\n  },\n  {\n    \"name\": \"Homestyle Breakfast\",\n    \"price\": \"$6.95\",\n    \"description\": \"Two eggs, bacon or sausage, toast, and our ever-popular hash browns\",\n    \"calories\": 950\n  }\n]\nIn the columns orientation, each line is a column. The same JSON data can be represented as a table as follows:\n{\n  \"name\": {\n    \"0\": \"Belgian Waffles\",\n    \"1\": \"Strawberry Belgian Waffles\",\n    \"2\": \"Berry-Berry Belgian Waffles\",\n    \"3\": \"French Toast\",\n    \"4\": \"Homestyle Breakfast\"\n  },\n  \"price\": {\n    \"0\": \"$5.95\",\n    \"1\": \"$7.95\",\n    \"2\": \"$8.95\",\n    \"3\": \"$4.50\",\n    \"4\": \"$6.95\"\n  },\n  \"description\": {\n    \"0\": \"Two of our famous Belgian Waffles with plenty of real maple syrup\",\n    \"1\": \"Light Belgian waffles covered with strawberries and whipped cream\",\n    \"2\": \"Light Belgian waffles covered with an assortment of fresh berries and whipped cream\",\n    \"3\": \"Thick slices made from our homemade sourdough bread\",\n    \"4\": \"Two eggs, bacon or sausage, toast, and our ever-popular hash browns\"\n  },\n  \"calories\": {\n    \"0\": 650,\n    \"1\": 900,\n    \"2\": 900,\n    \"3\": 600,\n    \"4\": 950\n  }\n}\nSimilar to pd.read_csv, pd.read_excel and pd.read_xml, you can read a JSON file using the pd.read_json() function.\nThe first input is filepath. There is no sep, header or index_col parameter because the JSON files don’t have flat structure.\n\nimport pandas as pd \ndata = pd.read_json('../data/sample.json')\ndata.head()\n\n\n\n\n\n\n\n\nname\nprice\ndescription\ncalories\n\n\n\n\n0\nBelgian Waffles\n$5.95\nTwo of our famous Belgian Waffles with plenty ...\n650\n\n\n1\nStrawberry Belgian Waffles\n$7.95\nLight Belgian waffles covered with strawberrie...\n900\n\n\n2\nBerry-Berry Belgian Waffles\n$8.95\nLight Belgian waffles covered with an assortme...\n900\n\n\n3\nFrench Toast\n$4.50\nThick slices made from our homemade sourdough ...\n600\n\n\n4\nHomestyle Breakfast\n$6.95\nTwo eggs, bacon or sausage, toast, and our eve...\n950\n\n\n\n\n\n\n\nSimilarly, pandas has a df.to_json() function to write a DataFrame to a JSON file. The parameter orient specifies the orientation of the JSON file. The default is orient='records' which means the JSON file is written in the records orientation. If you want to write the JSON file in the columns orientation, then orient='columns' should be used.\n\njson_data = data.to_json(orient='records', lines=True)\nprint(json_data)\n\n{\"name\":\"Belgian Waffles\",\"price\":\"$5.95\",\"description\":\"Two of our famous Belgian Waffles with plenty of real maple syrup\",\"calories\":650}\n{\"name\":\"Strawberry Belgian Waffles\",\"price\":\"$7.95\",\"description\":\"Light Belgian waffles covered with strawberries and whipped cream\",\"calories\":900}\n{\"name\":\"Berry-Berry Belgian Waffles\",\"price\":\"$8.95\",\"description\":\"Light Belgian waffles covered with an assortment of fresh berries and whipped cream\",\"calories\":900}\n{\"name\":\"French Toast\",\"price\":\"$4.50\",\"description\":\"Thick slices made from our homemade sourdough bread\",\"calories\":600}\n{\"name\":\"Homestyle Breakfast\",\"price\":\"$6.95\",\"description\":\"Two eggs, bacon or sausage, toast, and our ever-popular hash browns\",\"calories\":950}\n\n\n\n\nimport pprint \n\npprint.pprint(data.to_json(orient='columns'))\n\n('{\"name\":{\"0\":\"Belgian Waffles\",\"1\":\"Strawberry Belgian '\n 'Waffles\",\"2\":\"Berry-Berry Belgian Waffles\",\"3\":\"French Toast\",\"4\":\"Homestyle '\n 'Breakfast\"},\"price\":{\"0\":\"$5.95\",\"1\":\"$7.95\",\"2\":\"$8.95\",\"3\":\"$4.50\",\"4\":\"$6.95\"},\"description\":{\"0\":\"Two '\n 'of our famous Belgian Waffles with plenty of real maple syrup\",\"1\":\"Light '\n 'Belgian waffles covered with strawberries and whipped cream\",\"2\":\"Light '\n 'Belgian waffles covered with an assortment of fresh berries and whipped '\n 'cream\",\"3\":\"Thick slices made from our homemade sourdough bread\",\"4\":\"Two '\n 'eggs, bacon or sausage, toast, and our ever-popular hash '\n 'browns\"},\"calories\":{\"0\":650,\"1\":900,\"2\":900,\"3\":600,\"4\":950}}')",
    "crumbs": [
      "Home",
      "Flat vs. Hierarchical Data"
    ]
  },
  {
    "objectID": "pandas/preliminaries.html#flat-formats",
    "href": "pandas/preliminaries.html#flat-formats",
    "title": "Flat vs. Hierarchical Data",
    "section": "",
    "text": "Flat data formats are native to pandas and are the simplest and the most ubiquitous file formats in general.\nTo avoid data redundancy, data is often factored into multiple tables. For example, in a database of a school, there may be a table for students, a table for teachers, a table for classes, a table for grades, etc. Depending on the question of interest, these tables are then joined together to form a single table.\n\n\nCSV is an open format used to store tabular data. It is a text file where each line is a row of data. In other words, each line is separated by newline character \\n. Within a row, each column is separated by a comma ,. The first row is optionally the header row containing the names of the columns.\nExample of a CSV file is the elections.csv file.\n\nimport pandas as pd \n\nurl  = 'https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv'\ndata = pd.read_csv(url)\ndata.head()\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n\n\n\n\n\nThe CSV format has many variations, each different in regard of the separator used. For instance, a particularly popular variant called TSV (Tab Separated Values) uses tab character \\t to separate columns instead of a comma. In other variants the column separator can also be a semicolon ; or a pipe |.\nExample of a TSV file is restaurants.tsv file.\nNote the use of sep='\\t' parameter in pd.read_csv() in the code below:\n\nimport pandas as pd \nurl  = \"https://raw.githubusercontent.com/fahadsultan/csc272/main/data/restaurants.tsv\"\ndata = pd.read_csv(url, sep='\\t', index_col=0)\ndata.head()\n\n\n\n\n\n\n\n\nbusiness_id\nbusiness_name\nbusiness_address\nbusiness_city\nbusiness_state\nbusiness_postal_code\nbusiness_latitude\nbusiness_longitude\nbusiness_location\nbusiness_phone_number\n...\ninspection_score\ninspection_type\nviolation_id\nviolation_description\nrisk_category\nNeighborhoods\nSF Find Neighborhoods\nCurrent Police Districts\nCurrent Supervisor Districts\nAnalysis Neighborhoods\n\n\n\n\n0\n835\nKam Po Kitchen\n801 Broadway St\nSan Francisco\nCA\n94133\n37.797223\n-122.410513\nPOINT (-122.410513 37.797223)\nNaN\n...\n88.0\nRoutine - Unscheduled\n835_20180917_103139\nImproper food storage\nLow Risk\n107.0\n107.0\n6.0\n3.0\n6.0\n\n\n1\n905\nWorking Girls' Cafe'\n0259 Kearny St\nSan Francisco\nCA\n94108\n37.790477\n-122.404033\nPOINT (-122.404033 37.790477)\nNaN\n...\n87.0\nRoutine - Unscheduled\n905_20190415_103114\nHigh risk vermin infestation\nHigh Risk\n19.0\n19.0\n6.0\n3.0\n8.0\n\n\n2\n1203\nTAWAN'S THAI FOOD\n4403 GEARY Blvd\nSan Francisco\nCA\n94118\n37.780834\n-122.466590\nPOINT (-122.46659 37.780834)\n1.415576e+10\n...\n77.0\nRoutine - Unscheduled\n1203_20170803_103120\nModerate risk food holding temperature\nModerate Risk\n5.0\n5.0\n8.0\n4.0\n11.0\n\n\n3\n1345\nCordon Bleu\n1574 California St\nSan Francisco\nCA\n94109\n37.790683\n-122.420264\nPOINT (-122.420264 37.790683)\nNaN\n...\n81.0\nRoutine - Unscheduled\n1345_20170928_103105\nImproper cooling methods\nHigh Risk\n105.0\n105.0\n4.0\n3.0\n21.0\n\n\n4\n1352\nLA TORTILLA\n495 Castro St B\nSan Francisco\nCA\n94114\n37.760954\n-122.434935\nPOINT (-122.434935 37.760954)\n1.415586e+10\n...\n74.0\nRoutine - Unscheduled\n1352_20180620_103177\nNon service animal\nLow Risk\n38.0\n38.0\n3.0\n5.0\n5.0\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n\n\nXLSX is a proprietary format also used to store tabular data. Unlike a CSV, an XLSX is not a plain text file that you can simply read in any text editor. In contrast, an XLSX is a binary file which can be read only in specific software such as Microsft Excel or OpenOffice Calc. An excel spreedsheet does a lot more than just store(tabular) data such as storing formulas, charts and images, etc.\nHowever, for our purposes here, the only distinction between a CSV file and an XLSX file is that a) an XLSX file can contain multiple sheets where each sheet is a table and b) you can read an XLSX file in pandas using the pd.read_excel() function.\npd.read_excel is very similar to pd.read_csv where the first input is the filename or filepath that you want to read. Other inputs such as header, names, index_col are the same as pd.read_csv. The only additional input is:\n\nsheet_name: (default: sheet_name=0) specifies the sheet number or sheet name to be read. Default is 0 which means the first sheet is read. If the file contains multiple sheets, then sheet_name=None should be used.\n\nYou can download a sample excel spreadsheet from here\nNote that the file contains two sheets: “All Data” and “Just US”\nThe line of code below reads in just the sheet labeled “Just US” from the spreadsheet, using the sheet_name parameter:\n\npd.read_excel?\n\n\nimport pandas as pd \ndata = pd.read_excel('../data/Financial Sample.xlsx', sheet_name='US Only')\ndata.head()\n\n\n\n\n\n\n\n\nSegment\nCountry\nProduct\nDiscount Band\nUnits Sold\nManufacturing Price\nSale Price\nGross Sales\nDiscounts\nSales\nCOGS\nProfit\nDate\nMonth Number\nMonth Name\nYear\n\n\n\n\n0\nMidmarket\nUnited States of America\nMontana\nNone\n615.0\n5\n15\n9225.0\n0.0\n9225.0\n6150.0\n3075.0\n41974\n12\nDecember\n2014\n\n\n1\nGovernment\nUnited States of America\nPaseo\nNone\n1143.0\n10\n7\n8001.0\n0.0\n8001.0\n5715.0\n2286.0\n41913\n10\nOctober\n2014\n\n\n2\nChannel Partners\nUnited States of America\nPaseo\nNone\n912.0\n10\n12\n10944.0\n0.0\n10944.0\n2736.0\n8208.0\n41579\n11\nNovember\n2013\n\n\n3\nEnterprise\nUnited States of America\nVelo\nNone\n2821.0\n120\n125\n352625.0\n0.0\n352625.0\n338520.0\n14105.0\n41852\n8\nAugust\n2014\n\n\n4\nChannel Partners\nUnited States of America\nAmarilla\nNone\n1953.0\n260\n12\n23436.0\n0.0\n23436.0\n5859.0\n17577.0\n41730\n4\nApril\n2014\n\n\n\n\n\n\n\nJust as you can write any pandas DataFrame to CSV file using df.to_csv(), you can write any DataFrame to XLSX file using the df.to_excel() function.",
    "crumbs": [
      "Home",
      "Flat vs. Hierarchical Data"
    ]
  },
  {
    "objectID": "pandas/preliminaries.html#hierarchical-nested-formats",
    "href": "pandas/preliminaries.html#hierarchical-nested-formats",
    "title": "Flat vs. Hierarchical Data",
    "section": "",
    "text": "Hierarchical data formats are used to store data that is inherently hierarchical. These formats are particularly popular on the internet for exchange of information with web services using APIs (Application Programming Interfaces).\n\n\nXML is a format used to store hierarchical data. Data in XML is stored as a tree structure. This tree is constituent of nodes. Each node has a start tag and an end tag. The start tag is enclosed in angle brackets &lt; and &gt; e.g. &lt;name&gt;. The end tag is also enclosed in angle brackets but it also has a forward slash / after the opening angle bracket e.g. &lt;/name&gt;.\nThe start tag and the end tag together are called an element.\nThe start tag can optionally contain attributes. Attributes are name-value pairs. The value is enclosed in double quotes \". The start tag can optionally contain child elements. Child elements are enclosed between the start tag and the end tag. The end tag can optionally contain text. Text is the value of the node. The text is enclosed between the start tag and the end tag.\nThe first node in an XML file is called the root node.\nExample of an XML file is\n&lt;note&gt;\n&lt;to&gt;Tove&lt;/to&gt;\n&lt;from&gt;Jani&lt;/from&gt;\n&lt;heading&gt;Reminder&lt;/heading&gt;\n&lt;body&gt;Don't forget me this weekend!&lt;/body&gt;\n&lt;/note&gt;\n&lt;breakfast_menu&gt;\n    &lt;food&gt;\n        &lt;name&gt;Belgian Waffles&lt;/name&gt;\n        &lt;price&gt;$5.95&lt;/price&gt;\n        &lt;description&gt;Two of our famous Belgian Waffles with plenty of real maple syrup&lt;/description&gt;\n        &lt;calories&gt;650&lt;/calories&gt;\n    &lt;/food&gt;\n    &lt;food&gt;\n        &lt;name&gt;Strawberry Belgian Waffles&lt;/name&gt;\n        &lt;price&gt;$7.95&lt;/price&gt;\n        &lt;description&gt;Light Belgian waffles covered with strawberries and whipped cream&lt;/description&gt;\n        &lt;calories&gt;900&lt;/calories&gt;\n    &lt;/food&gt;\n    &lt;food&gt;\n        &lt;name&gt;Berry-Berry Belgian Waffles&lt;/name&gt;\n        &lt;price&gt;$8.95&lt;/price&gt;\n        &lt;description&gt;Light Belgian waffles covered with an assortment of fresh berries and whipped cream&lt;/description&gt;\n        &lt;calories&gt;900&lt;/calories&gt;\n    &lt;/food&gt;\n    &lt;food&gt;\n        &lt;name&gt;French Toast&lt;/name&gt;\n        &lt;price&gt;$4.50&lt;/price&gt;\n        &lt;description&gt;Thick slices made from our homemade sourdough bread&lt;/description&gt;\n        &lt;calories&gt;600&lt;/calories&gt;\n    &lt;/food&gt;\n    &lt;food&gt;\n        &lt;name&gt;Homestyle Breakfast&lt;/name&gt;\n        &lt;price&gt;$6.95&lt;/price&gt;\n        &lt;description&gt;Two eggs, bacon or sausage, toast, and our ever-popular hash browns&lt;/description&gt;\n        &lt;calories&gt;950&lt;/calories&gt;\n    &lt;/food&gt;\n&lt;/breakfast_menu&gt;\nOther XML files are: Breakfast Menu, Plant Catalog and CD catalog.\nIt is a text file where each line is a node of data. Each node has a name and a value. The name is separated from the value by a colon. The value is separated from the name by a colon. The first node is the root node. The root node contains the names of the nodes. The root node is separated from the data nodes by a blank line.\nIn pandas, you can read an XML file using the pd.read_xml() function. The first input to pd.read_xml() is the filename or filepath that you want to read. Other important parameters of the pd.read_xml() function are:\n\nxpath: (default: xpath=None) specifies the path to the node(s) to be read. Default is None which means the entire XML file is read. If you want to read a specific node, then xpath should be used.\nnamespaces: (default: namespaces=None) specifies the namespaces used in the XML file. Default is None which means no namespaces are used. If the XML file uses namespaces, then namespaces should be used.\nencoding: (default: encoding=None) specifies the encoding of the XML file. Default is None which means the encoding is automatically detected. If the XML file uses a specific encoding, then encoding should be used.\nerrors: (default: errors=None) specifies how to handle errors. Default is None which means the errors are ignored. If the XML file contains errors, then errors should be used.\n\n\ndata = pd.read_xml('https://www.w3schools.com/xml/simple.xml')\ndata.head()\n\n\n\n\n\n\n\n\nname\nprice\ndescription\ncalories\n\n\n\n\n0\nBelgian Waffles\n$5.95\nTwo of our famous Belgian Waffles with plenty ...\n650\n\n\n1\nStrawberry Belgian Waffles\n$7.95\nLight Belgian waffles covered with strawberrie...\n900\n\n\n2\nBerry-Berry Belgian Waffles\n$8.95\nLight Belgian waffles covered with an assortme...\n900\n\n\n3\nFrench Toast\n$4.50\nThick slices made from our homemade sourdough ...\n600\n\n\n4\nHomestyle Breakfast\n$6.95\nTwo eggs, bacon or sausage, toast, and our eve...\n950\n\n\n\n\n\n\n\n\n\n\nJSON is an open data format used to store hierarchical data. JSON data resembles a Python dictionary. It is a text file where each line is a key-value pair. The key is separated from the value by a colon. Similar to a Python dictionary, the value can be a string, a number, a dictionary, a boolean or a list.\nJSON data can be oriented in two ways: records and columns.\nIn the records orientation, each line is a record.\n[\n  {\n    \"name\": \"Belgian Waffles\",\n    \"price\": \"$5.95\",\n    \"description\": \"Two of our famous Belgian Waffles with plenty of real maple syrup\",\n    \"calories\": 650\n  },\n  {\n    \"name\": \"Strawberry Belgian Waffles\",\n    \"price\": \"$7.95\",\n    \"description\": \"Light Belgian waffles covered with strawberries and whipped cream\",\n    \"calories\": 900\n  },\n  {\n    \"name\": \"Berry-Berry Belgian Waffles\",\n    \"price\": \"$8.95\",\n    \"description\": \"Light Belgian waffles covered with an assortment of fresh berries and whipped cream\",\n    \"calories\": 900\n  },\n  {\n    \"name\": \"French Toast\",\n    \"price\": \"$4.50\",\n    \"description\": \"Thick slices made from our homemade sourdough bread\",\n    \"calories\": 600\n  },\n  {\n    \"name\": \"Homestyle Breakfast\",\n    \"price\": \"$6.95\",\n    \"description\": \"Two eggs, bacon or sausage, toast, and our ever-popular hash browns\",\n    \"calories\": 950\n  }\n]\nIn the columns orientation, each line is a column. The same JSON data can be represented as a table as follows:\n{\n  \"name\": {\n    \"0\": \"Belgian Waffles\",\n    \"1\": \"Strawberry Belgian Waffles\",\n    \"2\": \"Berry-Berry Belgian Waffles\",\n    \"3\": \"French Toast\",\n    \"4\": \"Homestyle Breakfast\"\n  },\n  \"price\": {\n    \"0\": \"$5.95\",\n    \"1\": \"$7.95\",\n    \"2\": \"$8.95\",\n    \"3\": \"$4.50\",\n    \"4\": \"$6.95\"\n  },\n  \"description\": {\n    \"0\": \"Two of our famous Belgian Waffles with plenty of real maple syrup\",\n    \"1\": \"Light Belgian waffles covered with strawberries and whipped cream\",\n    \"2\": \"Light Belgian waffles covered with an assortment of fresh berries and whipped cream\",\n    \"3\": \"Thick slices made from our homemade sourdough bread\",\n    \"4\": \"Two eggs, bacon or sausage, toast, and our ever-popular hash browns\"\n  },\n  \"calories\": {\n    \"0\": 650,\n    \"1\": 900,\n    \"2\": 900,\n    \"3\": 600,\n    \"4\": 950\n  }\n}\nSimilar to pd.read_csv, pd.read_excel and pd.read_xml, you can read a JSON file using the pd.read_json() function.\nThe first input is filepath. There is no sep, header or index_col parameter because the JSON files don’t have flat structure.\n\nimport pandas as pd \ndata = pd.read_json('../data/sample.json')\ndata.head()\n\n\n\n\n\n\n\n\nname\nprice\ndescription\ncalories\n\n\n\n\n0\nBelgian Waffles\n$5.95\nTwo of our famous Belgian Waffles with plenty ...\n650\n\n\n1\nStrawberry Belgian Waffles\n$7.95\nLight Belgian waffles covered with strawberrie...\n900\n\n\n2\nBerry-Berry Belgian Waffles\n$8.95\nLight Belgian waffles covered with an assortme...\n900\n\n\n3\nFrench Toast\n$4.50\nThick slices made from our homemade sourdough ...\n600\n\n\n4\nHomestyle Breakfast\n$6.95\nTwo eggs, bacon or sausage, toast, and our eve...\n950\n\n\n\n\n\n\n\nSimilarly, pandas has a df.to_json() function to write a DataFrame to a JSON file. The parameter orient specifies the orientation of the JSON file. The default is orient='records' which means the JSON file is written in the records orientation. If you want to write the JSON file in the columns orientation, then orient='columns' should be used.\n\njson_data = data.to_json(orient='records', lines=True)\nprint(json_data)\n\n{\"name\":\"Belgian Waffles\",\"price\":\"$5.95\",\"description\":\"Two of our famous Belgian Waffles with plenty of real maple syrup\",\"calories\":650}\n{\"name\":\"Strawberry Belgian Waffles\",\"price\":\"$7.95\",\"description\":\"Light Belgian waffles covered with strawberries and whipped cream\",\"calories\":900}\n{\"name\":\"Berry-Berry Belgian Waffles\",\"price\":\"$8.95\",\"description\":\"Light Belgian waffles covered with an assortment of fresh berries and whipped cream\",\"calories\":900}\n{\"name\":\"French Toast\",\"price\":\"$4.50\",\"description\":\"Thick slices made from our homemade sourdough bread\",\"calories\":600}\n{\"name\":\"Homestyle Breakfast\",\"price\":\"$6.95\",\"description\":\"Two eggs, bacon or sausage, toast, and our ever-popular hash browns\",\"calories\":950}\n\n\n\n\nimport pprint \n\npprint.pprint(data.to_json(orient='columns'))\n\n('{\"name\":{\"0\":\"Belgian Waffles\",\"1\":\"Strawberry Belgian '\n 'Waffles\",\"2\":\"Berry-Berry Belgian Waffles\",\"3\":\"French Toast\",\"4\":\"Homestyle '\n 'Breakfast\"},\"price\":{\"0\":\"$5.95\",\"1\":\"$7.95\",\"2\":\"$8.95\",\"3\":\"$4.50\",\"4\":\"$6.95\"},\"description\":{\"0\":\"Two '\n 'of our famous Belgian Waffles with plenty of real maple syrup\",\"1\":\"Light '\n 'Belgian waffles covered with strawberries and whipped cream\",\"2\":\"Light '\n 'Belgian waffles covered with an assortment of fresh berries and whipped '\n 'cream\",\"3\":\"Thick slices made from our homemade sourdough bread\",\"4\":\"Two '\n 'eggs, bacon or sausage, toast, and our ever-popular hash '\n 'browns\"},\"calories\":{\"0\":650,\"1\":900,\"2\":900,\"3\":600,\"4\":950}}')",
    "crumbs": [
      "Home",
      "Flat vs. Hierarchical Data"
    ]
  },
  {
    "objectID": "pandas/apply.html",
    "href": "pandas/apply.html",
    "title": "Applying custom functions",
    "section": "",
    "text": "Applying a custom user-defined function to a DataFrame is a common operation in pandas.\nIt allows us to implement custom transformations on the data that are not available in the standard library.\nThe apply method can be used in the following way:\nEach of these operations is distinct in terms of the shape of the output and the arguments that the function being applied takes.",
    "crumbs": [
      "Home",
      "Applying custom functions"
    ]
  },
  {
    "objectID": "pandas/apply.html#applying-a-function-to-each-value-of-a-pd.series",
    "href": "pandas/apply.html#applying-a-function-to-each-value-of-a-pd.series",
    "title": "Applying custom functions",
    "section": "1. Applying a function to each value of a pd.Series",
    "text": "1. Applying a function to each value of a pd.Series\nIn our elections dataset, we have a column Candidate that contains the name of the candidate that each vote was cast for.\nLet’s say we wanted to extract the first name of each candidate. We can use the apply method to apply a custom function that extracts the first name from a string.\n\nimport pandas as pd\n\nelections = pd.read_csv('../data/elections.csv')\n\ndef extract_first_name(name):\n    # name is of type string e.g. \"Andrew Jackson\"\n    space_separated_substrings = name.split(\" \")\n    first_name = space_separated_substrings[0]\n    return first_name\n\ncandidate_column = elections['Candidate']\nelections['first_name'] = candidate_column.apply(extract_first_name)\n\nNote that the function extract_first_name takes a scalar input (a string): Candidate and returns a scalar output (a string): first_name.\nOur function extract_first_name is called on each value of the Candidate column so \\(n\\) times where \\(n\\) is the number of rows in the DataFrame.\n\n\n\n\nThe function extract_first_name returns back a scalar value for each input value. All together, we get a pd.Series of length \\(n\\) where each value is the first name of the candidate.",
    "crumbs": [
      "Home",
      "Applying custom functions"
    ]
  },
  {
    "objectID": "pandas/apply.html#applying-a-function-to-each-column-of-a-pd.dataframe-axis0",
    "href": "pandas/apply.html#applying-a-function-to-each-column-of-a-pd.dataframe-axis0",
    "title": "Applying custom functions",
    "section": "2. Applying a function to each column of a pd.DataFrame (axis=0)",
    "text": "2. Applying a function to each column of a pd.DataFrame (axis=0)\n\ndata.apply(f, axis=0) applies the function f to each column of the DataFrame data.\n\n\n\nFor example, if we wanted to find the number of unique values in each column of a DataFrame data, we could use the following code:\n\ndef count_unique(col):\n    return len(set(col))\n\nelections.apply(count_unique, axis=\"index\") # function is passed an individual column",
    "crumbs": [
      "Home",
      "Applying custom functions"
    ]
  },
  {
    "objectID": "pandas/apply.html#applying-a-function-to-each-row-of-a-pd.dataframe-axis1",
    "href": "pandas/apply.html#applying-a-function-to-each-row-of-a-pd.dataframe-axis1",
    "title": "Applying custom functions",
    "section": "3. Applying a function to each row of a pd.DataFrame (axis=1)",
    "text": "3. Applying a function to each row of a pd.DataFrame (axis=1)\ndata.apply(f, axis=1) applies the function f to each row of the DataFrame data.\n\n\n\nFor instance, let’s say we wanted to count the total number of voters in an election.\nWe can use .apply to answer that question using the following formula:\n\\[ \\text{total} \\times \\frac{\\%}{100} = \\text{Popular vote} \\]\n\ndef compute_total(row):\n    return int(row['Popular vote']*100/row['%'])\n\nelections.apply(compute_total, axis=1)\n\n0         264413\n1         264412\n2        1143702\n3        1143703\n4        1287655\n         ...    \n177    135720167\n178    158383403\n179    158383403\n180    158383401\n181    158383402\nLength: 182, dtype: int64",
    "crumbs": [
      "Home",
      "Applying custom functions"
    ]
  },
  {
    "objectID": "pandas/apply.html#anonymous-functions",
    "href": "pandas/apply.html#anonymous-functions",
    "title": "Applying custom functions",
    "section": "Anonymous functions",
    "text": "Anonymous functions\nAnonymous functions are functions that are defined without a name.\nIn Python, we use the lambda keyword to create anonymous functions.\nThe syntax of a lambda function is:\nlambda arguments: expression\nLambda functions can have any number of arguments but only one expression, which is evaluated and returned, without using the return keyword.\nThey are useful when we need a simple function that we will only use once.\nFor example, let’s say we wanted to add 1 to each element in a DataFrame. We could use an anonymous function to do this:\ndouble = lambda x: x * 2 \n\ndouble(5) # returns 10\n\nmultiplyby2 = double \n\nmultiplyby2(5) # returns 10\nNote that in the above example, double is not a function name per se. It is a variable that refers to the anonymous function lambda x: x * 2.\n\nAnonymous functions with .apply\nWe can use anonymous functions with .apply to apply a function to each column or row of a DataFrame.\nFor example, let’s say we wanted to find the number of unique values in each column of a DataFrame data. We could use the following code:\ndata.apply(lambda x: x.nunique())\nLet’s say we wanted to count the total number of voters in an election. We can use .apply to answer that question using the following formula:\ndata['Popular vote'] = data.apply(lambda row: row['total'] * row['%'] / 100, axis=1)",
    "crumbs": [
      "Home",
      "Applying custom functions"
    ]
  },
  {
    "objectID": "pandas/select_filter.html",
    "href": "pandas/select_filter.html",
    "title": "Selection, Filtering and Dropping",
    "section": "",
    "text": "In this section, we will learn how to extract and remove a subset of rows and columns in pandas. The two primary operations of data extraction are:\n\nSelection: Extracting subset of columns.\nFiltering: Extracting subset of rows.\n\nLet’s start by loading the dataset.\n\nimport pandas as pd \n\nurl = \"https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv\"\n\nelections = pd.read_csv(url)"
  },
  {
    "objectID": "pandas/read_data.html",
    "href": "pandas/read_data.html",
    "title": "Reading Data",
    "section": "",
    "text": "To begin our studies in pandas, we must first import the library into our Python environment using import pandas as pd statement. pd is a common alias for pandas. The import statement will allow us to use pandas data structures and methods in our code.\nCSV files can be in pandas using read_csv. The following code cell imports pandas as pd, the conventional alias for Pandas and then reads the elections.csv file.\n# `pd` is the conventional alias for Pandas\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv\"\nelections = pd.read_csv(url)\nelections\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n177\n2016\nJill Stein\nGreen\n1457226\nloss\n1.073699\n\n\n178\n2020\nJoseph Biden\nDemocratic\n81268924\nwin\n51.311515\n\n\n179\n2020\nDonald Trump\nRepublican\n74216154\nloss\n46.858542\n\n\n180\n2020\nJo Jorgensen\nLibertarian\n1865724\nloss\n1.177979\n\n\n181\n2020\nHoward Hawkins\nGreen\n405035\nloss\n0.255731\n\n\n\n\n182 rows × 6 columns\nLet’s dissect the code above.\nThis code stores our DataFrame object in the elections variable. We see that our elections DataFrame has 182 rows and 6 columns (Year, Candidate, Party, Popular Vote, Result, %). Each row represents a single record – in our example, a presedential candidate from some particular year. Each column represents a single attribute, or feature of the record.\nIn the example above, we constructed a DataFrame object using data from a CSV file. As we’ll explore in the next section, we can also create a DataFrame with data of our own.\nIn the elections dataset, each row represents one instance of a candidate running for president in a particular year. For example, the first row represents Andrew Jackson running for president in the year 1824. Each column represents one characteristic piece of information about each presidential candidate. For example, the column named Result stores whether or not the candidate won the election.\nSome relevant arguments for read_csv are:",
    "crumbs": [
      "Home",
      "Reading Data"
    ]
  },
  {
    "objectID": "pandas/read_data.html#arguments",
    "href": "pandas/read_data.html#arguments",
    "title": "Reading Data",
    "section": "Arguments",
    "text": "Arguments\n\nSeparator\nThe sep argument specifies the character used to separate the values in the CSV file. By default, this is a comma ,. However, some CSV files use other characters, such as tabs or semicolons, to separate values. In such cases, we can specify the separator character using the sep argument.\n\n\n\nHeader row (column labels)\nThe header argument specifies the row number to use as the column names. By default, this is 0, which means the first row is used as the column names. If the CSV file does not have a header row, we can set header=None to use the default column names.\n\n\n\nIndex column (row labels)\nThe index_col argument specifies the column to use as the row labels of the DataFrame. By default, this is None, which means that the row labels are integers starting from 0. If we want to use one of the columns as the row labels, we can specify the column name or index using the index_col argument.\n\n\n\nIgnore erroneous lines\nThe error_bad_lines argument specifies whether the parser should skip lines with too many fields rather than raising an error. By default, this is False, which means that the parser will raise an error if it encounters a line with too many fields. If we want the parser to skip such lines, we can set error_bad_lines=True.\n\n\n\nSkip first k rows\nThe skiprows argument specifies the number of rows to skip at the beginning of the CSV file. By default, this is None, which means that no rows are skipped. If we want to skip a certain number of rows at the beginning of the file, we can specify the number of rows using the skiprows argument.\n\n\n\nRead only k rows\nThe nrows argument specifies the number of rows to read from the CSV file. By default, this is None, which means that all rows are read. If we want to read only a certain number of rows from the file, we can specify the number of rows using the nrows argument.\n\n\n\nRead only subset of columns\nThe usecols argument specifies the columns to read from the CSV file. By default, this is None, which means that all columns are read. If we want to read only certain columns from the file, we can specify the column names or indices using the usecols argument.\n\n\n\nCharacter Encoding\nThe encoding argument specifies the character encoding to use when reading the CSV file. By default, this is None, which means that the encoding is detected automatically. If the CSV file uses a different character encoding, we can specify the encoding using the encoding argument. Some common character encodings are:\n\nutf-8: Unicode Transformation Format 8-bit\nlatin1 or iso-8859-1: ISO 8859-1 supports many languages, including English, French, German, Spanish, and Portuguese.\ncp1252: The cp1252 encoding is similar to the latin1 encoding, but it includes additional characters that are not present in latin1. The cp1252 encoding is commonly used in Windows operating systems.\nascii: The ASCII encoding is a 7-bit encoding that supports English characters and some special characters, such as punctuation marks and symbols.",
    "crumbs": [
      "Home",
      "Reading Data"
    ]
  },
  {
    "objectID": "pandas/read_data.html#installation",
    "href": "pandas/read_data.html#installation",
    "title": "Reading Data",
    "section": "Installation",
    "text": "Installation\nYou can install tqdm using pip:\npip install tqdm",
    "crumbs": [
      "Home",
      "Reading Data"
    ]
  },
  {
    "objectID": "pandas/read_data.html#usage",
    "href": "pandas/read_data.html#usage",
    "title": "Reading Data",
    "section": "Usage",
    "text": "Usage\nTo use tqdm, you just need to wrap your iterable with the tqdm function. Here’s an example:\n\nfrom tqdm import tqdm\n\nfor i in tqdm(range(100)):\n    # do something\nThis will create a progress bar that shows the progress of the loop. You can customize the progress bar by passing additional arguments to the tqdm function. For example, you can set the total number of iterations, the width of the progress bar, and the format of the progress bar.\nHere’s an example that shows how to customize the progress bar:\nfrom tqdm import tqdm\n\nfor i in tqdm(range(100), total=100, desc=\"Processing\", ncols=100):\n    # do something\nThis will create a progress bar with a width of 100 characters and a description that says “Processing”.",
    "crumbs": [
      "Home",
      "Reading Data"
    ]
  },
  {
    "objectID": "pandas/read_data.html#examples",
    "href": "pandas/read_data.html#examples",
    "title": "Reading Data",
    "section": "Examples",
    "text": "Examples\nHere are a few examples that show how to use tqdm with different types of iterables:\n\nList\nfrom tqdm import tqdm\n\ndata = [1, 2, 3, 4, 5]\n\nfor i in tqdm(data, desc=\"Processing\", ncols=100):\n    # do something\n\n\nRange\n\nfrom tqdm import tqdm\n\nfor i in tqdm(range(100), desc=\"Processing\", ncols=100):\n    # do something\n\n\nFile\nfrom tqdm import tqdm\n\nwith open(\"data.txt\", \"r\") as f:\n    for line in tqdm(f, desc=\"Processing\", ncols=100):\n        # do something\n\n\nPandas DataFrame\nimport pandas as pd\nfrom tqdm import tqdm\n\ndf = pd.read_csv(\"data.csv\")\n\nfor index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\", ncols=100):\n    # do something",
    "crumbs": [
      "Home",
      "Reading Data"
    ]
  },
  {
    "objectID": "pandas/wrangling.html",
    "href": "pandas/wrangling.html",
    "title": "Vectorized Operations",
    "section": "",
    "text": "Vector based programming (also known as array based programming) is a programming paradigm that uses operations on arrays to execute tasks. This is in contrast to scalar based programming where operations are performed on individual elements of an array.\nVectorization operates at the level of individual instructions sent to a processor within each node. For instance, in the illustration shown here, the instruction is to add 5 to a column of numbers and copy the results to a new column B. With vectorization, all the data elements in that column are transformed simultaneously, i.e. the instruction to add 5 is applied to multiple pieces of data at the same time. This paradigm is sometimes referred to as Single Instruction Multiple Data (or SIMD).\nWe can think of vectorization as subdividing the work into smaller chunks that can be handled independently by different computational units at the same time.\nIn this course, we will be minimizing the use of loops.\nThis is orders of magnitude faster than the conventional sequential model where each piece of data is handled one after the other in sequence.\nVectorized operations are also known as SIMD (Single Instruction Multiple Data) operations in the context of computer architecture. In contrast, scalar operations are known as SISD (Single Instruction Single Data) operations.\nWith vectorization, performing the same operation on a modern intel CPU is 16 times faster than the sequential mode. The performance gains on GPUs with thousands of computational cores is even greater. However, despite these remarkable performance benefits, most analytical code out there is written in the slower sequential mode. This is not a surprise, since until about a decade ago, CPU and GPU hardware could not really support vectorization for data analysis. So most implementations had to be sequential.\nThe last 10 years, however, have seen the rise of new technologies like CUDA from NVidia and advanced vector extensions from Intel that have dramatically shifted our ability to apply vectorization. Because of the power of vectorization, some traditional vendors now make claims about including vectorization in their offerings. But shifting to this new vectorized paradigm is not easy, since all of your code needs to be written from scratch to utilize these capabilities.\nVectorization can only be applied in situations when operations at individual elements are independent of each other. For example, if we want to add two arrays, we can do so by adding each element of the first array to the corresponding element of the second array. This is a vectorized operation.\nHowever, for problems such as the Fibonacci sequence, where the value of an element depends on the values of the previous two elements, we cannot vectorize the operation. Similarly, finding minimum or maximum of an array cannot be vectorized.\nLet’s read in the same elections data from the previous exercise and do some data manipulation and wrangling using pandas.\nimport pandas as pd \nurl = \"https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv\"\nelections = pd.read_csv(url)",
    "crumbs": [
      "Home",
      "Vectorized Operations"
    ]
  },
  {
    "objectID": "pandas/wrangling.html#data-alignment",
    "href": "pandas/wrangling.html#data-alignment",
    "title": "Vectorized Operations",
    "section": "Data Alignment",
    "text": "Data Alignment\npandas can make it much simpler to work with objects that have different indexes. For example, when you add objects, if any index pairs are not the same, the respective index in the result will be the union of the index pairs. Let’s look at an example:\n\ns1 = pd.Series([7.3, -2.5, 3.4, 1.5], index=[\"a\", \"c\", \"d\", \"e\"])\n\ns2 = pd.Series([-2.1, 3.6, -1.5, 4, 3.1], index=[\"a\", \"c\", \"e\", \"f\", \"g\"])\n\ns1, s2, s1 + s2\n\n(a    7.3\n c   -2.5\n d    3.4\n e    1.5\n dtype: float64,\n a   -2.1\n c    3.6\n e   -1.5\n f    4.0\n g    3.1\n dtype: float64,\n a    5.2\n c    1.1\n d    NaN\n e    0.0\n f    NaN\n g    NaN\n dtype: float64)\n\n\nThe internal data alignment introduces missing values in the label locations that don’t overlap. Missing values will then propagate in further arithmetic computations.\nIn the case of DataFrame, alignment is performed on both rows and columns:\n\ndf1 = pd.DataFrame({\"A\": [1, 2], \"B\":[3, 4]})\ndf2 = pd.DataFrame({\"B\": [5, 6], \"D\":[7, 8]})\ndf1 + df2",
    "crumbs": [
      "Home",
      "Vectorized Operations"
    ]
  },
  {
    "objectID": "pandas/wrangling.html#math-operations",
    "href": "pandas/wrangling.html#math-operations",
    "title": "Vectorized Operations",
    "section": "Math Operations",
    "text": "Math Operations\nIn native Python, we have a number of operators that we can use to manipulate data. Most, if not all, of these operators can be used with Pandas Series and DataFrames and are applied element-wise in parallel. A summary of the operators supported by Pandas is shown below:\n\n\n\n\n\n\n\n\n\n\nCategory\nOperators\nSupported by Pandas\nComments\n\n\n\n\nArithmetic\n+, -, *, /, %, //, **\n✅\nAssuming comparable shapes (equal length)\n\n\nAssignment\n=, +=, -=, *=, /=, %=, //=, **=\n✅\nAssuming comparable shapes\n\n\nComparison\n==, !=, &gt;, &lt;, &gt;=, &lt;=\n✅\nAssuming comparable shapes\n\n\nLogical\nand, or, not\n❌\nUse &, \\|, ~ instead\n\n\nIdentity\nis, is not\n✅\nAssuming comparable data type/structure\n\n\nMembership\nin, not in\n❌\nUse isin() method instead\n\n\nBitwise\n&, \\|, ^, ~, &lt;&lt;, &gt;&gt;\n❌\n\n\n\n\n\nThe most significant difference is that logical operators and, or, and not are NOT used with Pandas Series and DataFrames. Instead, we use &, |, and ~ respectively.\nMembership operators in and not in are also not used with Pandas Series and DataFrames. Instead, we use the isin() method.",
    "crumbs": [
      "Home",
      "Vectorized Operations"
    ]
  },
  {
    "objectID": "pandas/selection.html",
    "href": "pandas/selection.html",
    "title": "Selection: subset of columns",
    "section": "",
    "text": "To select a column in a DataFrame, we can use the bracket notation. That is, name of the DataFrame followed by the column name in square brackets: df['column_name'].\n\n\n\nFor example, to select a column named Candidate from the election DataFrame, we can use the following code:\n\nimport pandas as pd \n\nurl = \"https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv\"\n\nelections = pd.read_csv(url)\n\n\ncandidates = elections['Candidate']\nprint(candidates)\n\n0         Andrew Jackson\n1      John Quincy Adams\n2         Andrew Jackson\n3      John Quincy Adams\n4         Andrew Jackson\n             ...        \n177           Jill Stein\n178         Joseph Biden\n179         Donald Trump\n180         Jo Jorgensen\n181       Howard Hawkins\nName: Candidate, Length: 182, dtype: object\n\n\nThis extracts a single column as a Series. We can confirm this by checking the type of the output.\n\ntype(candidates)\n\npandas.core.series.Series\n\n\nTo select multiple columns, we can pass a list of column names. For example, to select both Candidate and Votes columns from the election DataFrame, we can use the following line of code:\n\nelections[['Candidate', 'Party']]\n\n\n\n\n\n\n\n\nCandidate\nParty\n\n\n\n\n0\nAndrew Jackson\nDemocratic-Republican\n\n\n1\nJohn Quincy Adams\nDemocratic-Republican\n\n\n2\nAndrew Jackson\nDemocratic\n\n\n3\nJohn Quincy Adams\nNational Republican\n\n\n4\nAndrew Jackson\nDemocratic\n\n\n...\n...\n...\n\n\n177\nJill Stein\nGreen\n\n\n178\nJoseph Biden\nDemocratic\n\n\n179\nDonald Trump\nRepublican\n\n\n180\nJo Jorgensen\nLibertarian\n\n\n181\nHoward Hawkins\nGreen\n\n\n\n\n182 rows × 2 columns\n\n\n\nThis extracts multiple columns as a DataFrame. We can confirm as well this by checking the type of the output.\n\ntype(elections[['Candidate', 'Party']])\n\nThis is how we can select columns in a DataFrame. Next, let’s learn how to filter rows.\n\n[]\nThe [] selection operator is the most baffling of all, yet the most commonly used. It only takes a single argument, which may be one of the following:\n\nA list of column labels\nA single column label\n\nSay we wanted the first four rows of our elections DataFrame.",
    "crumbs": [
      "Home",
      "Selection: subset of columns"
    ]
  },
  {
    "objectID": "calendar.html",
    "href": "calendar.html",
    "title": "Calendar",
    "section": "",
    "text": "Calendar\n\n\n\n\n\n\nCaution\n\n\n\nPlease note that this is a tentative plan and is subject to change.\n\n\n\n\n\n#\nTOPIC\nREADING\nMEETING 1\nMEETING 2\nWRITTEN ASSIGNMENT\nPROGRAMMING ASSIGNMENT\n\n\n\n\n1\nIntroduction\nMcKinney Ch.5, 8, 10\nIntro\nPandas Intro\nWA1\nPA 1\n\n\n2\nDropping/Retaining relevant rows/columns\nMcKinney Ch.5, 8, 10\nSelection, Filtering and Dropping\nData Manipulation and Wrangling\nWA2\nPA 2\n\n\n3\nWorking with multiple data files\nMcKinney Ch.5, 8, 10\nConcatenation\nMerging\n—\nPA3\n\n\n4\nCustom Transformations\nMcKinney Ch.5, 8, 10\nApply\nApply\nWA3\nPA4\n\n\n5\nData Visualization\nMcKinney Ch.9\nLine plots\nScatter\n—\nPA5\n\n\n6\nEncoding and Representation\nMcKinney Ch.7\nEncoding Feature Types\nImages + Sound + Text\nWA4\nPA6\n\n\n\n\n\nMIDTERM\n\n\n\n\n\n#\nTOPIC\nREADING\nMEETING 1\nMEETING 2\nWRITTEN ASSIGNMENT\nPROGRAMMING ASSIGNMENT\n\n\n\n\n7\nNearest Neighbor\nSkiena Ch. 8 and Ch. 10\nGeometric Interpretation\nDot Product\n—\nPA7\n\n\n8\nProbability + Stats\nSkiena Ch.2\n\n\n\nPA8\n\n\n9\nNaive Bayes\nSLP 3rd edition Ch.4\n\n\n\nPA9\n\n\n10\nExpectation Maximization\nSkiena Ch.10 (10.5)\n\n\n\nPA10\n\n\n11\nGradient Descent\nSkiena Ch.9 (9.1-9.5)\n\n\n\n\n\n\n\n\n\nFINAL EXAM"
  },
  {
    "objectID": "stats/distributions.html",
    "href": "stats/distributions.html",
    "title": "Statistical Distributions",
    "section": "",
    "text": "Every variable we observe in the data has a particular frequency distribution and in turn a probability distribution. These exact distribtions are unique to the variable under consideration. However, the shapes of these distributions are not unique. There are a few common shapes that we see over and over again. In other words, the world’s rich variety of data appears only in a small number of classical shapes. Once abstracted from specific data observations, they become probability distributions, worthy of independent study.\nThese classical distributions have two nice properties:\nAs indicated in the previous section, probability provides a way to express and reason about uncertainty. In other words, once you have probability distributions, you can use them to reason about the world. However, in the real world, we don’t know the probability distributions. We have to estimate them from data.\nThis is where statistics comes in. Statistics allows us to look at data and intelligently guess what the underlying probability distributions might be in the following two steps:\nBelow we will look at some of the most common distributions for categorical variables, focusing specifically on the parameters that define them. Once we have estimated parameters, we have a complete description of the probability distribution that can be used to reason about the world."
  },
  {
    "objectID": "stats/distributions.html#categorical-statistical-distributions",
    "href": "stats/distributions.html#categorical-statistical-distributions",
    "title": "Statistical Distributions",
    "section": "Categorical Statistical Distributions",
    "text": "Categorical Statistical Distributions\n\nBernoulli 🪙\n\nNumber of possible outcomes \\(k = 2\\)\nNumber of trials \\(n = 1\\)\nExample: Coin toss (Heads/Tails), yes/no, true/false, success/failure\nNumber of parameters: \\(1\\)\nParameter: Probability of success \\(p\\)\n\nBernoulli Distribution is a discrete probability distribution used to model a single trial \\(n=1\\) of a binary random that can have two possible outcomes \\(k=2\\).\nFor instance, if we were interested in probability of observing a head in a single coin toss, we would use the Bernoulli distribution, where “1” is defined to mean “heads” and “0” is defined to mean “tails”.\nThe Bernoulli distribution has a single parameter, the probability of success, which we will call \\(p\\). The probability of observing a head is \\(p\\) and the probability of observing a tail is \\(q=1-p\\). The probability function of the Bernoulli distribution is:\n\\[\n\\begin{aligned}\nP(x) &= p^x(1-p)^{1-x} \\\\\n&= \\begin{cases}\np & \\text{if } x = 1 \\\\\n1-p & \\text{if } x = 0\n\\end{cases}\n\\end{aligned}\n\\]\nwhere \\(x\\) is the outcome of the coin toss.\nIf we observe a head, then \\(x=1\\) and the probability function is \\(P(1) = p\\). If we observe a tail, then \\(x=0\\) and the likelihood function is \\(P(0) = 1-p\\).\nIf a variable \\(X\\) follows Bernoulli distribtion with \\(p=0.5\\), it is denoted as \\(X \\sim \\text{Bernoulli}(p)\\).\n\nimport seaborn as sns\nimport pandas as pd\n\ndata = pd.DataFrame()\ndata[\"X\"] = [\"Heads\", \"Tails\"]\ndata[\"P(X)\"] = [0.5, 0.5]\naxs = sns.barplot(data=data, x=\"X\", y=\"P(X)\", color=\"lightblue\");\naxs.set(title=\"Bernoulli Distribution of a Fair Coin Flip \\n $X \\\\sim Bernoulli(0.5)$\");\n\n\n\n\n\n\n\n\n\n\n\nCategorical 🎲\n\nNumber of possible outcomes \\(k &gt; 2\\)\nNumber of trials \\(n = 1\\)\nExample: Rolling a die, choosing a color, choosing a letter\nNumber of parameters: \\(k\\)\nParameter: Probability of each outcome \\(p_1, p_2, \\ldots, p_k\\)\n\nThe categorical distribution is a generalization of the Bernoulli distribution. It models the probability of observing a particular outcome from a set of \\(k &gt; 2\\) outcomes in a single trials.\nFor example, it models the probability of observing a particular face when rolling a k-sided die once.\nThe probability mass function of the categorical distribution is:\n\\[\nf(x) = \\begin{cases}\np_1 & \\text{if } x = 1 \\\\\np_2 & \\text{if } x = 2 \\\\\n\\vdots \\\\\np_k & \\text{if } x = k\n\\end{cases}\n\\]\nwhere \\(p_1, p_2, \\ldots, p_k\\) are the probabilities of observing each of the \\(k\\) outcomes. The probabilities must sum to 1.\n\ncat_data = pd.DataFrame()\ncat_data['X'] = range(1, 7)\ncat_data['Y'] = 1/6\naxs = sns.barplot(data=cat_data, x=\"X\", y=\"Y\", color=\"lightblue\");\naxs.set(title=\"Categorical Distribution of a Fair Die Roll\");\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import categorical\n\nn = 50\np = 0.5\n\nbinom_data = pd.DataFrame()\nbinom_data['X'] = range(51)\nbinom_data['Y'] = binom.pmf(binom_data['X'], n, p)\n\naxs = sns.lineplot(data=binom_data, x=binom_data['X'], y=binom_data['Y']);\naxs.set(title=\"Binomial Distribution of 50 Flips of a Fair Coin \\n $X \\\\sim Binomial(n=50, p=0.5)$\");\n\n\n\n\nBinomial 🪙 🪙 🪙\n\nNumber of possible outcomes \\(k = 2\\)\nNumber of trials \\(n &gt; 1\\)\nExample: Count of Heads in \\(n\\) coin tosses\nNumber of parameters: \\(2\\)\nParameters: Probability of success \\(p\\) and number of trials \\(n\\) \n\nThe binomial distribution is the discrete probability distribution of the number of successes in a sequence of \\(n\\) independent trials, each asking a yes–no question, and each with its own boolean-valued outcome: success/yes/true/one (with probability \\(p\\)) or failure/no/false/zero (with probability \\(q = 1 − p\\)).\nFor example, if we were interested in probability of observing a head in \\(n\\) coin tosses, we would use the binomial distribution, where “1” is defined to mean “heads” and “0” is defined to mean “tails”.\nThe probability function of the binomial distribution is:\n\\[\nP(X=x) = \\binom{n}{x} p^x(1-p)^{n-x}\n\\]\nwhere \\(n\\) is the number of trials and \\(p\\) is the probability of success. The binomial coefficient \\(\\binom{n}{x}\\) is the number of ways to choose \\(x\\) items from a set of \\(n\\) items. The binomial coefficient is defined as:\n\\[\n\\binom{n}{x} = \\frac{n!}{x!(n-x)!}\n\\]\nwhere \\(n!\\) is the factorial of \\(n\\).\n\nfrom scipy.stats import binom\n\nn = 50\np = 0.5\n\nbinom_data = pd.DataFrame()\nbinom_data['X'] = range(51)\nbinom_data['Y'] = binom.pmf(binom_data['X'], n, p)\n\naxs = sns.lineplot(data=binom_data, x=binom_data['X'], y=binom_data['Y']);\naxs.set(title=\"Binomial Distribution of 50 Flips of a Fair Coin \\n $X \\\\sim Binomial(n=50, p=0.5)$\");\n\n\n\n\n\n\n\n\n\n\n\nMultinomial 🎲 🎲 🎲\n\n\nNumber of possible outcomes \\(k &gt; 2\\)\nNumber of trials \\(n &gt; 1\\)\nExample: As a result of \\(n=9\\) rolls of a die,  Count of 1-face ⚀ = 2 \\(\\wedge\\)  Count of 2-face ⚁ = 1 \\(\\wedge\\)  Count of 3-face ⚂ = 2 \\(\\wedge\\)  Count of 4-face ⚃ = 1 \\(\\wedge\\)  Count of 5-face ⚄ = 2 \\(\\wedge\\)  Count of 6-face ⚅ = 1\nNumber of parameters: \\(k + 2\\)\nParameters: \\(n\\), \\(k\\) and probability of each outcome \\(p_1, p_2, \\ldots, p_k\\) such that \\(\\sum_{i=1}^k p_i = 1\\) \n\nMultinomial distribution is a multivariate generalization of the binomial distribution. It models the probability of observing a particular count for each of \\(k &gt; 2\\) outcomes in \\(n &gt; 1\\) trials.\nFor example, it models the probability of counts for rolling a k-sided die n times.\nThe probability function of the multinomial distribution is:\n\\[\nP(X=\\{x_1, x_2, \\ldots x_k \\}) = \\frac{n!}{x_1!x_2!\\ldots x_k!} p_1^{x_1}p_2^{x_2}\\ldots p_k^{x_k}\n\\]\nwhere \\(n\\) is the number of trials, \\(k\\) is the number of outcomes, \\(n_1, n_2, \\ldots, n_k\\) are the counts for each outcome, and \\(p_1, p_2, \\ldots, p_k\\) are the probabilities of each outcome. The probabilities must sum to 1 and the counts must sum to \\(n\\) i.e. \\(\\sum_{i=1}^k p_i = 1\\) and \\(\\sum_{i=1}^k x_i = n\\).\nFor instance, using Multinomial distribution, as a result of rolling a die 9 times, we can calculate the probability of observing each even-sided face 2 times and each odd-sided face 1 times as follows :\n\\[P(\\#⚀ = 1, \\#⚁ = 2, \\#⚂ = 1, \\#⚃ = 2, \\#⚄ = 1, \\#⚅ = 2) = \\frac{9!}{1!2!1!2!1!2!} \\left(\\frac{1}{6}\\right)^9\\]\n\\[ = \\frac{362880}{8} \\times \\left(\\frac{1}{6}\\right)^9\\]\n\\[ = 0.0045 \\]\nVisualizing the probability distribution of the Multinomial distribution is hard. The following figure shows the probability distribution of the Multinomial distribution for \\(n=5\\), \\(k=3\\), \\(p_1=0.5\\), \\(p_2=0.3\\) and \\(p_3=0.2\\).\n\n\n\n\nSummary\nThe following table presents a summary and comparison of the categorical statistical distributions:\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nNumber of Trials \\(n\\)\nNumber of Outomes \\(k\\)\nNumber of parameters\nParameters\nExample\n\n\n\n\nBernoulli\n\\(1\\)\n\\(2\\)\n\\(1\\)\n\\(p\\)\nA single coin toss\n\n\nCategorical\n\\(1\\)\n\\(&gt;2\\)\n\\(k\\)\n\\(p_1, p_2, \\ldots, p_k\\)\nA single roll of a die\n\n\nBinomial\n\\(&gt;1\\)\n\\(2\\)\n\\(2\\)\n\\(n, p\\)\nCount of heads in \\(n\\) coin tosses\n\n\nMultinomial\n\\(&gt;1\\)\n\\(&gt;2\\)\n\\(k+2\\)\n\\(n, k, p_1, p_2, \\ldots, p_k\\)\nCount of each face in \\(n\\) rolls of a die"
  },
  {
    "objectID": "stats/distributions.html#maximum-likelihood-estimation",
    "href": "stats/distributions.html#maximum-likelihood-estimation",
    "title": "Statistical Distributions",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\nMaximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate.\nLikelihood function is the probability of observing the data given the parameters.\nMaximum likelihood estimate of Bernoulli distribution’s only parameter \\(p\\), given data, is:\n\\[ \\hat{p} = \\frac{\\text{Number of successes}}{\\text{Number of observations}} \\]\nSimilarly, Maximum likelihood estimate of \\(k\\) parameters of Categorical distribution is:\n\\[\n\\begin{aligned}\n\\hat{p}_i &= \\frac{\\text{Number of observations of outcome i}}{\\text{Number of observations}} \\\\\n\\\\\n\\hat{n} &= \\text{Number of observations}\n\\end{aligned}\n\\]\nNote that the maximum likelihood estimate of the parameters of the Binomial and Multinomial distributions are not as simple as the Bernoulli and Categorical distributions and require multiple samples. However, since there is a Bernoulli distribution for each trial of the Binomial distribution and a Categorical distribution for each trial of the Multinomial distribution, we can use the maximum likelihood estimates of the Bernoulli and Categorical distributions to estimate the parameters of the Binomial and Multinomial distributions."
  },
  {
    "objectID": "stats/distributions.html#law-of-large-numbers",
    "href": "stats/distributions.html#law-of-large-numbers",
    "title": "Statistical Distributions",
    "section": "Law of Large Numbers",
    "text": "Law of Large Numbers\nThe law of large numbers states that as the number of trials of a random experiment increases, the average of the observed outcomes approaches the expected value.\nFor example, if we toss a fair coin 10 times, we might expect to observe 5 heads but may or may not get 5. However, if we toss a fair coin 1,000,000 times, we can confidently expect to observe nearabouts 500,000 heads, if the coin is indeed fair.\nThe law of large numbers is the basis for the frequentist approach to statistics. The frequentist approach to statistics is based on the idea that the probability of an event is the long-run relative frequency of the event. In other words, the probability of an event is the proportion of times the event occurs in a large number of trials.\nIn the context of MLE, the law of large numbers implies that as the number of observations increases, the maximum likelihood estimate of the parameters approaches the true parameters of the distribution.\n\nfrom numpy.random import choice \n\nn = 500\n\ndata = pd.DataFrame()\ndata['X'] = choice([0, 1], size=n)\ndata['trials'] = range(1, n+1)\ndata['p_hat'] = data['X'].cumsum() / data['trials']\nsns.lineplot(data=data, x='trials', y='p_hat', label='estimated p');\naxs = sns.lineplot(data=data, x='trials', y=0.5, color='red', label='true p');\n\naxs.set(title=\"Estimating the Probability of Heads $\\\\hat{p} = \\\\frac{Count(x=1)}{n}$ \\n \" + \\\n              \"$X \\\\sim Bernoulli(p)$ \\n Note \", xlabel='Number of Trials', ylabel='Estimated Probability of Heads');"
  },
  {
    "objectID": "stats/distributions.html#central-dogma-of-statistics",
    "href": "stats/distributions.html#central-dogma-of-statistics",
    "title": "Statistical Distributions",
    "section": "Central Dogma of Statistics",
    "text": "Central Dogma of Statistics\nThe central dogma of data science, simply put, is that general claims about a population can be made from a sample of data.\n\nThis raises concerns about the sampling process such as the representativeness of the sample, the size of the sample, the sampling bias, etc. Which in turn raises concerns about potential negative effects of the claims made based on questionable data.\n\n\nIssues with sampling and underrepresentative data are not new. In 1936, the Literary Digest magazine conducted a poll to predict the outcome of the presidential election. The poll was based on a sample of 2.4 million people. The poll predicted that Alf Landon would win the election with 57% of the vote. However, the actual election results were the opposite. Franklin D. Roosevelt won the election with 62% of the vote.\nThe reason for the failure of the poll was that the sample was not representative of the population. The sample was biased towards the wealthy and the educated. The poll was also conducted by sending out postcards to people who subscribed to the Literary Digest magazine. In 1936, only the wealthy and the educated subscribed to magazines.\nThe Literary Digest magazine went bankrupt soon after the election. George Gallup, who correctly predicted the outcome of the election, went on to found the American Institute of Public Opinion, which later became the Gallup Poll.\nThe big takeaway from this story is that size alone is not enough. In other words, large sample sizes is a necessary but not sufficient condition for making general claims about a population."
  },
  {
    "objectID": "stats/stats.html",
    "href": "stats/stats.html",
    "title": "PROBABILITY & STATISTICS",
    "section": "",
    "text": "Probability and statistics are related areas of mathematics which concern themselves with analyzing the relative frequency of events. Both subjects are important, relevant, and useful. But they are different, and understanding the distinction is crucial in properly interpreting the relevance of mathematical evidence.\nStill, there are fundamental differences in the way they see the world:\n\n\n\n\n\n\n\n\n\nProbability\nStatistics\n\n\n\n\nGoal:\nPredicting the Future\nSummarizing the Past\n\n\nBranch of Mathematics:\nTheoretical (pure math)\nApplication-driven (applied math)\n\n\nReasoning / Inference:\nDeduction (Rules -&gt; Data)\nInduction (Data -&gt; Rules)\n\n\nWorldview:\nIdeal\nReal / Messy\n\n\nLevel of Confidence:\nCertainty\nEstimation\n\n\nResearchers / Practitioners\nHappy go-lucky\nTormented\n\n\n\n\nIn summary, probability theory enables us to find the consequences of a given ideal world, while statistical theory enables us to measure the extent to which our world is ideal. This constant tension between theory and practice is why statisticians prove to be a tortured group of individuals compared with the happy-go-lucky probabilists.\nThis distinction will perhaps become clearer if we trace the thought process of a mathematician encountering their first gambling game:\n\n\nModern probability theory first emerged from gambling tables of France in mid 1600s. Blaise Pascal and Pierre de Fermat wondered whether the player or the house had the advantage in a particular betting game. \n\nIf a gambler were a probabilist, they would see the dice and think “Six-sided dice? Each side of the dice is presumably equally likely to land face up. Now assuming that each face comes up with probability 1/6, I can figure out what my chances are of winning.”\nIf instead a gambler were statistician, they would see the dice and think “How do I know that they are not loaded?” They’ll watch the game a while, and keep track of how often each number comes up. Then they can decide if my observations are consistent with the assumption of equal-probability faces. Once they have the probabilities figured out, they can call a probabilist to tell them how to bet.\n\nIn this course, we will use Statistics to go from Data to Probability-based models, as shown in the figure below:\n\nHaving learned or estimated the model, we can then use Probability to make predictions about the future."
  },
  {
    "objectID": "calculus/gradientdescent.html",
    "href": "calculus/gradientdescent.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "Conditional probability is the probability of one event occurring given that another event has occurred.\nThe conditional probability is usually denoted by \\(P(A | B)\\) and is defined as:\n\\[ P(A | B) = \\frac{P(A, B)}{P(B)} \\]\nThe denominator is the marginal probability of \\(B\\).\n\n\nFor example, if we are flipping two coins, the conditional probability of flipping heads in the second toss, knowing the first toss was tails is:\n\n\n\nPossible world\n\\(\\text{Coin}_1\\)\n\\(\\text{Coin}_2\\)\n\\(P(\\omega)\\)\n\n\n\n\n\\(\\omega_1\\)\nH\nH\n0.25\n\n\n\\(\\omega_2\\)\nH\nT\n0.25\n\n\n\\(\\omega_3\\)\nT\nH\n0.25\n\n\n\\(\\omega_4\\)\nT\nT\n0.25\n\n\n\n\\[ P(\\text{Coin}_2 = H | \\text{Coin}_1 = T) = \\frac{P(\\text{Coin}_2 = H, \\text{Coin}_1 = T)}{P(\\text{Coin}_1 = T)} = \\frac{0.25}{0.5} = 0.5 \\]\n\nmerged\n\n\n\n\n\n\n\n\nC\nD\nP(C, D)\nP(C)\nP(D)\nP(C) P(D)\n\n\n\n\n0\nH\n1\n0.24\n0.51\n0.43\n0.2193\n\n\n1\nT\n1\n0.19\n0.49\n0.43\n0.2107\n\n\n2\nH\n2\n0.13\n0.51\n0.22\n0.1122\n\n\n3\nT\n2\n0.09\n0.49\n0.22\n0.1078\n\n\n4\nH\n3\n0.09\n0.51\n0.22\n0.1122\n\n\n5\nT\n3\n0.13\n0.49\n0.22\n0.1078\n\n\n6\nH\n4\n0.01\n0.51\n0.05\n0.0255\n\n\n7\nT\n4\n0.04\n0.49\n0.05\n0.0245\n\n\n8\nH\n5\n0.03\n0.51\n0.03\n0.0153\n\n\n9\nT\n5\n0.00\n0.49\n0.03\n0.0147\n\n\n10\nH\n6\n0.01\n0.51\n0.05\n0.0255\n\n\n11\nT\n6\n0.04\n0.49\n0.05\n0.0245\n\n\n\n\n\n\n\n\nmerged['P(D | C)'] = merged['P(C, D)'] / merged['P(C)']\nmerged['P(D | C)']\n\n0     0.470588\n1     0.387755\n2     0.254902\n3     0.183673\n4     0.176471\n5     0.265306\n6     0.019608\n7     0.081633\n8     0.058824\n9     0.000000\n10    0.019608\n11    0.081633\nName: P(D | C), dtype: float64\n\n\n\nmerged[['C', 'D', 'P(D | C)']]\n\naxs = sns.catplot(data=merged, x=\"D\", y=\"P(D | C)\", hue=\"C\", kind=\"bar\");\naxs.set(title=\"Conditional probability distribution of D given C\\nNote that the blue bars add up to 1\");\n\n\n\n\n\n\n\n\nNote that the sum of conditional probabilites, unlike joint probability, is not 1.\n\nmerged[\"P(D | C)\"].sum()\n\n2.0\n\n\nThis is because\n\\[ \\sum_C \\sum_D P(D|C) = \\sum_D P(D|C=\\text{Heads}) + \\sum_D P(D|C=\\text{Tails}) \\]\nAnd \\(\\sum_D P(D|C=\\text{Heads})\\) and \\(\\sum_D P(D|C=\\text{Tails})\\) are individually probability distributions that each sum to 1, over different values of \\(D\\).\nIn other words, in the plot above, the blue bars add up to 1 and the orange bars add up to 1.\n\nheads = merged[merged[\"C\"] == \"H\"]\ntails = merged[merged[\"C\"] == \"T\"]\n\nheads[\"P(D | C)\"].sum(), tails[\"P(D | C)\"].sum()\n\n(1.0, 1.0)"
  },
  {
    "objectID": "calculus/gradientdescent.html#conditional-probability-pa-b",
    "href": "calculus/gradientdescent.html#conditional-probability-pa-b",
    "title": "Gradient Descent",
    "section": "",
    "text": "Conditional probability is the probability of one event occurring given that another event has occurred.\nThe conditional probability is usually denoted by \\(P(A | B)\\) and is defined as:\n\\[ P(A | B) = \\frac{P(A, B)}{P(B)} \\]\nThe denominator is the marginal probability of \\(B\\).\n\n\nFor example, if we are flipping two coins, the conditional probability of flipping heads in the second toss, knowing the first toss was tails is:\n\n\n\nPossible world\n\\(\\text{Coin}_1\\)\n\\(\\text{Coin}_2\\)\n\\(P(\\omega)\\)\n\n\n\n\n\\(\\omega_1\\)\nH\nH\n0.25\n\n\n\\(\\omega_2\\)\nH\nT\n0.25\n\n\n\\(\\omega_3\\)\nT\nH\n0.25\n\n\n\\(\\omega_4\\)\nT\nT\n0.25\n\n\n\n\\[ P(\\text{Coin}_2 = H | \\text{Coin}_1 = T) = \\frac{P(\\text{Coin}_2 = H, \\text{Coin}_1 = T)}{P(\\text{Coin}_1 = T)} = \\frac{0.25}{0.5} = 0.5 \\]\n\nmerged\n\n\n\n\n\n\n\n\nC\nD\nP(C, D)\nP(C)\nP(D)\nP(C) P(D)\n\n\n\n\n0\nH\n1\n0.24\n0.51\n0.43\n0.2193\n\n\n1\nT\n1\n0.19\n0.49\n0.43\n0.2107\n\n\n2\nH\n2\n0.13\n0.51\n0.22\n0.1122\n\n\n3\nT\n2\n0.09\n0.49\n0.22\n0.1078\n\n\n4\nH\n3\n0.09\n0.51\n0.22\n0.1122\n\n\n5\nT\n3\n0.13\n0.49\n0.22\n0.1078\n\n\n6\nH\n4\n0.01\n0.51\n0.05\n0.0255\n\n\n7\nT\n4\n0.04\n0.49\n0.05\n0.0245\n\n\n8\nH\n5\n0.03\n0.51\n0.03\n0.0153\n\n\n9\nT\n5\n0.00\n0.49\n0.03\n0.0147\n\n\n10\nH\n6\n0.01\n0.51\n0.05\n0.0255\n\n\n11\nT\n6\n0.04\n0.49\n0.05\n0.0245\n\n\n\n\n\n\n\n\nmerged['P(D | C)'] = merged['P(C, D)'] / merged['P(C)']\nmerged['P(D | C)']\n\n0     0.470588\n1     0.387755\n2     0.254902\n3     0.183673\n4     0.176471\n5     0.265306\n6     0.019608\n7     0.081633\n8     0.058824\n9     0.000000\n10    0.019608\n11    0.081633\nName: P(D | C), dtype: float64\n\n\n\nmerged[['C', 'D', 'P(D | C)']]\n\naxs = sns.catplot(data=merged, x=\"D\", y=\"P(D | C)\", hue=\"C\", kind=\"bar\");\naxs.set(title=\"Conditional probability distribution of D given C\\nNote that the blue bars add up to 1\");\n\n\n\n\n\n\n\n\nNote that the sum of conditional probabilites, unlike joint probability, is not 1.\n\nmerged[\"P(D | C)\"].sum()\n\n2.0\n\n\nThis is because\n\\[ \\sum_C \\sum_D P(D|C) = \\sum_D P(D|C=\\text{Heads}) + \\sum_D P(D|C=\\text{Tails}) \\]\nAnd \\(\\sum_D P(D|C=\\text{Heads})\\) and \\(\\sum_D P(D|C=\\text{Tails})\\) are individually probability distributions that each sum to 1, over different values of \\(D\\).\nIn other words, in the plot above, the blue bars add up to 1 and the orange bars add up to 1.\n\nheads = merged[merged[\"C\"] == \"H\"]\ntails = merged[merged[\"C\"] == \"T\"]\n\nheads[\"P(D | C)\"].sum(), tails[\"P(D | C)\"].sum()\n\n(1.0, 1.0)"
  },
  {
    "objectID": "calculus/gradientdescent.html#product-rule-pa-b",
    "href": "calculus/gradientdescent.html#product-rule-pa-b",
    "title": "Gradient Descent",
    "section": "Product Rule \\(P(A, B)\\)",
    "text": "Product Rule \\(P(A, B)\\)\nRearranging the definition of conditional probability, we get the product rule:\n\\[ P(A, B) = P(A | B) \\cdot P(B) \\]\nSimilarly, we can also write:\n\\[ P(A, B) = P(B | A) \\cdot P(A)\\]\nIn summary,\n\\[ P(A, B) = P(A | B) \\cdot P(B) = P(B | A) \\cdot P(A)\\]"
  },
  {
    "objectID": "calculus/gradientdescent.html#chain-rule-pa-b-c",
    "href": "calculus/gradientdescent.html#chain-rule-pa-b-c",
    "title": "Gradient Descent",
    "section": "Chain Rule \\(P(A, B, C)\\)",
    "text": "Chain Rule \\(P(A, B, C)\\)\nThe chain rule is a generalization of the product rule to more than two events.\n$ P(A, B, C) = P(A | B, C) P(B, C) $\n\\(P(A, B, C) = P(A | B, C) \\cdot P(B | C) \\cdot P(C)\\)\nsince \\(P(B, C) = P(B | C) \\cdot P(C)\\) as per the product rule.\nChain rule essentially allows expressing the joint probability of multiple random variables as a product of conditional probabilities. This is useful because conditional probabilities are often easier to estimate from data than joint probabilities."
  },
  {
    "objectID": "calculus/gradientdescent.html#inclusion-exclusion-principle-pa-vee-b",
    "href": "calculus/gradientdescent.html#inclusion-exclusion-principle-pa-vee-b",
    "title": "Gradient Descent",
    "section": "Inclusion-Exclusion Principle \\(P(A \\vee B)\\)",
    "text": "Inclusion-Exclusion Principle \\(P(A \\vee B)\\)\nInclusion-Exclusion Principle is a way of calculating the probability of two events occurring i.e. $ P(A=a  B=b) $ denoted generally as \\(P(A = a \\vee B = b)\\).\nIt is defined as:\n\\[ P(A = a \\vee B = b) = P(A = a) + P(B = b) - P(A = a \\wedge B = b) \\]\n\nFor example, if we are rolling two dice, the Inclusion-Exclusion Principle can be used to calculate the probability of rolling a 1 on the first die or a 2 on the second die.\n$P(_1=H _2=T) $\n$ = P(_2=H) + P(_1=T) - P(_2=H ∧ _1=T)$\n$ = 0.5 + 0.5 - 0.25 $\n$ = 0.75$"
  },
  {
    "objectID": "calculus/gradientdescent.html#bayes-theorem-pab",
    "href": "calculus/gradientdescent.html#bayes-theorem-pab",
    "title": "Gradient Descent",
    "section": "Bayes Theorem \\(P(A|B)\\)",
    "text": "Bayes Theorem \\(P(A|B)\\)\nBayes theorem is a way of calculating conditional probability. For example, if we are rolling two dice, Bayes theorem can be used to calculate the probability of rolling a 1 on the first die given that we rolled a 2 on the second die.\n\\[ P(A | B) = \\frac{P(B | A) \\cdot P(A)}{P(B)} \\]\n\\(P(A|B)\\) in the context of Bayes theorem is called the Posterior probability.\n\\(P(B|A)\\) is called the Likelihood.\n\\(P(A)\\) is called the Prior probability.\n\\(P(B)\\) is called the Evidence, also known as Marginal Likelihood.\n\\[ P(\\text{Posterior}) = \\frac{P(\\text{Likelihood})\\cdot P(\\text{Prior})}{P(\\text{Evidence})}\\]\n\n\n\n\nBayes Theorem allows a formal method of updating prior beliefs with new evidence and is the foundation of Bayesian Statistics. We will talk more about this when we talk about Statistics.\nIn machine learning, the task is often to find \\(P(Y | X_1 = x_1, X_2 = x_2, \\ldots X_D = x_D)\\) i.e. the probability of an unknown Y, given some values for \\(D\\) features (\\(X_1, X_2 \\ldots X_D\\)). Bayes theorem allows us to calculate this probability from the data.\nLet’s assume we are interested in predicting if a person is a football player (\\(Y_F=1\\)) or not (\\(Y_F=0\\)), given their height (\\(X_H\\)) and weight (\\(X_W\\)).\nSay, we observe a person who is 7 feet tall and weighs 200 pounds. We can use Bayes theorem to calculate the probability of this person being a football player using the following equation:\n\\(P(Y | X_H = 7, X_W = 200) = \\frac{P(X_H = 7, X_W = 200 | Y_F) \\cdot P(Y_F)}{P(X_H = 7, X_W = 200)}\\)\nNote that here \\(P(X_H = 7, X_W = 200 | Y_F)\\) is the Likelihood probability of observing someone who is 7 feet tall and weighs 200 pounds, knowing if they are a football player.\n\\(P(Y_F)\\) is the Prior probability of a person being a football player out of the entire population.\n\\(P(X_H = 7, X_W = 200)\\) is the probability of the Evidence i.e. probability of observing anyone who is 7 feet tall and weighs 200 pounds in the entire population.\nRecall that all supervised learning is based on the assumption that there is a relationship between the input variables \\(X\\) and the output variable \\(y\\) i.e. \n\\[\\textbf{y} = f(\\textbf{X})\\]\nwhere \\(f\\) is some unknown function.\n\\(X\\) here simply is some data that we have as a pd.DataFrame whereas \\(y\\) here is the target variable, one value for each observation, that we want to predict, as of type pd.Series.\n\nThe form of supervised learning we have talked about so far is classification. As discussed previously, in classification, the output variable \\(y\\) is a discrete target variable e.g. sentiment \\(\\in\\) {positive, neutral or negative}, ring \\(\\in\\) {A, B} or diagnosis \\(\\in\\) {malignant, benign} etc.\nThe other type of supervised learning that we will talk about in this notebook is called Regression. In regression, the target variable is to predict a continuous target variable i.e. \\[\\mathbf{y} \\in \\mathbb{R^N}\\].\nFor example, predicting the stock price of a publicly listed company, predicting the price of a house in dollars, or predicting the average surface temperature on Earth next year are all examples of regression problems.\nNote that the splitting of the data into training and test sets is exactly the same as in classification. The only difference is that the target variable is continuous instead of discrete.\n\n\nLinear Regression\nLinear Regression is the simplest solution to any regression problem.\nIn linear regression, the relationship between the input variable(s) \\(X\\) and the output variable \\(y\\) is assumed to be a linear.\n\nUnivariate Linear Regression\nFor simplicity, let’s assume we only have one input variable \\(x\\) and one output variable \\(y\\).\nOur goal, then is to find a linear function \\(f\\) that maps \\(x\\) to \\(y\\):\n\\[\\mathbf{y} = f(\\mathbf{x})\\]\nRecall that the equation for a straight line is $ f(x) = mx + b $\nwhere \\(m\\) is the slope of the line and \\(b\\) is the y-intercept. Assuming that \\(f\\) is a linear function, we can write:\n\\[\\mathbf{y} = f(\\mathbf{x}) = \\mathbf{m} \\cdot \\mathbf{x} + b\\]\nExpanding this equation for each observation in our dataset, we get:\n\\[\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = m \\times \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} + b\\]\nwhere \\(n\\) is the number of observations in our dataset.\nThe goal of linear regression is to find the values of \\(m\\) and \\(b\\) that best fit the data.\nThe cell code below plots y vs x where \\(y=mx + b\\) for a given value of \\(m\\) and \\(b\\).\n\nfrom matplotlib import pyplot as plt \nimport pandas as pd \nimport seaborn as sns\n\nm = 5\nb = 10\n\ndf = pd.DataFrame()\ndf['x'] = range(-5, 6)\ndf['f (x)'] = m * df['x'] + b\n\nsns.lineplot(x='x', y='f (x)', data=df);\n\nplt.axhline(0, color='black');\nplt.axvline(0, color='black');\n\nplt.title(\"f(x) = mx + b\\n\\nslope (m) = %s\\ny-intercept (b) = %s \" % (m, b));\nplt.grid()\n\n\n\n\n\n\n\n\nNow let’s load a dataset and try to find the best values of \\(m\\) and \\(b\\) that fit the data.\nThe code below loads a dataset of Average Land Temperature on each country for each year from 1750 to 2015.\nThe temperature is in degrees Celsius. Below these temperatures are aggregated and global averages are calculated for each year.\n\nimport pandas as pd\n\ndata         = pd.read_csv('../data/GlobalLandTemperaturesByCountry.csv')\ndata['dt']   = pd.to_datetime(data['dt'])\ndata['year'] = data['dt'].apply(lambda x: x.year)\ndata['month']= data['dt'].apply(lambda x: x.month)\ndata         = data.dropna()\ndata         = data[(data['year'] &gt;= 1900) & (data['month'] == 1)]\navgs         = data.groupby('year').mean()['AverageTemperature']\navgs.name    = 'Average Temperature (C)'\n\nsns.scatterplot(x=avgs.index, y=avgs);\n\n\n\n\n\n\n\n\nIn the code below, the data is split into training and test sets, similar to what we did in the classification examples for Naive Bayes and Nearest Neighbor models.\n\ntrain_pct = 0.8\ntrain_size = int(train_pct * len(avgs))\ntrain_set = avgs.sample(train_size, random_state=42)\ntest_set  = avgs[~avgs.index.isin(train_set.index)]\n\nX_train = train_set.index\ny_train = train_set\n\nX_test = test_set.index\ny_test = test_set\n\nsns.scatterplot(x=X_train, y=y_train, label='Train set');\nsns.scatterplot(x=X_test,  y=y_test,  label='Test set');\n\n\n\n\n\n\n\n\nIn the code cell below, two linear functions are plotted against the training data. Both implement the same linear function \\(f(x) = mx + b\\) but with different values of \\(m\\) and \\(b\\).\n\nsns.scatterplot(x=X_train, y=y_train,  label='Train Set');\n\nb = -5.5\nm = 0.0101\nmodel1 = m * X_train + b\nsns.scatterplot(x=X_train, y=model1.values,  label='Model 1');\n\n\nb = 14.5\nm = 0\nmodel2 = m * X_train + b\nsns.scatterplot(x=X_train, y=model2.values,  label='Model 2');\n\n\n\n\n\n\n\n\n\n\nErrors in Regression\nThe evaluation of regression models is done similar to classification models. The model is trained on the training set and then evaluated on the test set.\n\nThe difference is that the training has an internal evaluation metric that is minimized to find the values of coefficients such as \\(m\\) and \\(b\\) that best fit the training data.\nFor example, model-1 and model-2 above would yield different scores for how well they fit the training data. The model with the lowest score is the one that best fits the training data.\nOnce the model is trained, the test set is used to evaluate the model.\nThe internal evaluation metrics for linear regression during training are similar to the ones used in extrinsic evaluation on the test set.\nThe most common evaluation metrics for linear regression are as follows:\n\nResiduals\nResiduals are the difference between the true values of y and the predicted values of y.\n\\[\\text{Residual}_i = y_i - \\hat{y}_i\\]\nwhere \\(y_i\\) is the \\(i^{th}\\) true value of the target variable and \\(\\hat{y_i}\\) is the \\(i^{th}\\) predicted value of the target variable i.e. \\[\\hat{y_i} = m x_i + b\\]\n\ndf = pd.DataFrame()\ndf['y'] = y_train\ndf['model1'] = model1\ndf['model2'] = model2\n\nsns.scatterplot(x=X_train, y=y_train);\nsns.scatterplot(x=X_train, y=model1.values);\ndf.apply(lambda x: plt.plot((x.name, x.name), (x['y'], x['model1']), color='red', linewidth=1), axis=1);\nplt.legend(['Train set', 'Model 1', 'Residuals']);\n\n\n\n\n\n\n\n\n\nsns.scatterplot(x=X_train, y=y_train);\nsns.scatterplot(x=X_train, y=model2.values, color='green');\ndf.apply(lambda x: plt.plot((x.name, x.name), (x['y'], x['model2']), color='red', linewidth=1), axis=1);\nplt.legend(['Train set', 'Model 2', 'Residuals']);\n\n\n\n\n\n\n\n\n\n\nMean Absolute Error\nThe Mean Absolute Error (or MAE) is the average of the absolute differences between predictions and actual values. It gives an idea of how wrong the predictions were. The measure gives an idea of the magnitude of the error, but no idea of the direction (e.g. over or under predicting).\n\\[MAE = \\frac{1}{n}\\sum_{i=1}^{n}|\\text{residual}_i|\\]\nor\n\\[MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|\\]\nIn the code cell below, two linear functions are plotted against the training data. Both implement the same linear function \\(f(x) = mx + b\\) but with different values of \\(m\\) and \\(b\\).\n\nsns.scatterplot(x=X_train, y=y_train,  label='Train Set');\n\ndef mae(y, y_hat):\n    return sum(abs(y - y_hat)) / len(y)\n\nb = -5.5\nm = 0.0101\nmodel1 = m * X_train + b\nmae_model1 = mae(y_train, model1)\nsns.scatterplot(x=X_train, y=model1.values,  label=r'Model 1 (MAE$_{~train}$ = %s)' % round(mae_model1, 2));\n\n\nb = 14.5\nm = 0\nmodel2 = m * X_train + b\nmae_model2 = mae(y_train, model2)\nsns.scatterplot(x=X_train, y=model2.values,  label=r'Model 2 (MAE$_{~train}$ = %s)' % round(mae_model2, 2));\n\n\n\n\n\n\n\n\nIn order to find the best values of \\(m\\) and \\(b\\), we need to define an evaluation metric that we want to minimize.\nThe code cell below plots model 1 and model 2 against the test set and also calculates the MAE for each model.\n\nsns.scatterplot(x=X_train, y=y_train,  label='Test Set');\n\ndef mae(y, y_hat):\n    return sum(abs(y - y_hat)) / len(y)\n\nb = -5.5\nm = 0.0101\nmodel1 = m * X_test + b\nmae_model1 = mae(y_test, model1)\nsns.lineplot(x=X_test, y=model1.values, linewidth=3,\\\n             label=r'Model 1 (MAE$_{~test}$ = %s)' % round(mae_model1, 2), color='orange');\n\n\nb = 14.5\nm = 0\nmodel2 = m * X_test + b\nmae_model2 = mae(y_test, model2)\nsns.lineplot(x=X_test, y=model2.values, linewidth=3,  \\\n             label=r'Model 2 (MAE$_{~test}$ = %s)' % round(mae_model2, 2), color='green');\n\n\n\n\n\n\n\n\n\n\nMean Squared Error\nThe Mean Squared Error (or MSE) is much like the mean absolute error in that it provides a gross idea of the magnitude of error.\n\\[MSE = \\frac{1}{n}\\sum_{i=1}^{n}(\\text{Residual}_i)^2\\]\nor\n\\[MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\\]\n\ndef mse(y, y_hat):\n    return sum((y - y_hat)**2) / len(y)\n\nmse1 = mse(df['y'], df['model1'])\nmse2 = mse(df['y'], df['model2'])\n\nprint(\" MSE model 1: \", round(mse1, 2), \"\\n\", \"MSE model 2: \", round(mse2, 2))\n\n MSE model 1:  0.17 \n MSE model 2:  0.37\n\n\n\n\nRoot Mean Squared Error\nTaking the square root of the mean squared error converts the units back to the original units of the output variable and can be meaningful for description and presentation. This is called the Root Mean Squared Error (or RMSE).\n\\[RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(\\text{Residual}_i)^2}\\]\nor\n\\[RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}\\]\n\ndef rmse(y, y_hat):\n    return (sum((y - y_hat)**2) / len(y))**(1/2)\n\nmse1 = rmse(df['y'], df['model1'])\nmse2 = rmse(df['y'], df['model2'])\n\nprint(\" RMSE model 1: \", round(mse1, 2), \"\\n\", \"RMSE model 2: \", round(mse2, 2))\n\n RMSE model 1:  0.41 \n RMSE model 2:  0.61\n\n\n\n\n\\(\\text{R}^2\\)\nThe \\(\\text{R}^2\\) (or R Squared) metric provides an indication of the goodness of fit of a set of predictions to the actual values. In statistical literature, this measure is called the coefficient of determination. This is a value between 0 and 1 for no-fit and perfect fit respectively.\n\\[\\text{R}^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\\]\nwhere \\(\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n}y_i\\) is the mean of the observed data.\n\ndef r2(y, y_hat):\n    return 1 - sum((y - y_hat)**2) / sum((y - y.mean())**2)\n\nr2_1 = r2(df['y'], df['model1'])\nr2_2 = r2(df['y'], df['model2'])\n\nprint(\" R2 model 1: \", round(r2_1, 2), \"\\n\", \"R2 model 2: \", round(r2_2, 2))\n\n R2 model 1:  0.36 \n R2 model 2:  -0.4\n\n\n\n\n\n\nMultivariate Regression\nMultiple linear regression (MLR), also known simply as multiple regression, uses multiple (&gt; 1) input variables (\\(X\\)) to predict the outcome of a target variable (\\(y \\in \\mathbb{R}\\)) by fitting a linear equation to observed data.\n\\[\\mathbf{y} = f(\\mathbf{X})\\]\nHere \\(f\\) is a linear function of the form:\n\\[f(\\mathbf{X}) = \\mathbf{X}\\mathbf{m} + b\\]\nwhere \\(X\\) is a matrix of \\(N\\) observations and \\(D\\) features and \\(y\\) is a vector of \\(N\\) observations.\n\\[X = \\begin{bmatrix}\nx_{11} & x_{12} & \\dots & x_{1D} \\\\\nx_{21} & x_{22} & \\dots & x_{2D} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{N1} & x_{N2} & \\dots & x_{ND} \\\\\n\\end{bmatrix}\\]\n\\[y = \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_N \\\\\n\\end{bmatrix}\\]\nwhere \\(\\mathbf{m}\\) is a vector of \\(D\\) slopes and \\(b\\) is the y-intercept. i.e. \n\\[\\mathbf{m} = \\begin{bmatrix}\nm_1 \\\\\nm_2 \\\\\n\\vdots \\\\\nm_D \\\\\n\\end{bmatrix}\\]\nPutting it all together, we get:\n\\[\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_N \\\\\n\\end{bmatrix} = \\begin{bmatrix}\nx_{11} & x_{12} & \\dots & x_{1D} \\\\\nx_{21} & x_{22} & \\dots & x_{2D} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{N1} & x_{N2} & \\dots & x_{ND} \\\\\n\\end{bmatrix} \\begin{bmatrix}\nm_1 \\\\\nm_2 \\\\\n\\vdots \\\\\nm_D \\\\\n\\end{bmatrix} + b\\]\nThe goal of multiple linear regression is to find the values of \\(m_1, m_2, \\dots, m_D\\) and \\(b\\) that best fit the data.\n\nNow let’s load a dataset and try to find the best values of \\(m_1\\) and \\(b\\) that fit the data.\n\nThe code below uses data from California Housing Dataset to predict the median house value in California districts given the following input variables:\n\nMedInc: Median income in block.\nHouseAge: Median house age within a block (measured in years).\nAveRooms: Average number of rooms within a block of houses.\nAveBedrms: Average number of bedrooms within a block of houses.\nPopulation: Total number of people residing within a block.\nAveOccup: Average number of people occupying each house within a block.\nLongitude: A measure of how far west a house is; a higher value is farther west.\nLatitude: A measure of how far north a house is; a higher value is farther north.\n\nThe target variable is:\n\nMedian house value for households within a block (measured in US Dollars).\n\n\nimport pandas as pd \nfrom sklearn import  datasets\n\ncalifornia = datasets.fetch_california_housing()\nX = pd.DataFrame(california.data, columns=california.feature_names)\ny = pd.Series(california.target, name='Price')\n\n\nX.shape\n\n(20640, 8)\n\n\n\nX.head()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n\n\n\n\n\n\n\n\ny.head()\n\n0    4.526\n1    3.585\n2    3.521\n3    3.413\n4    3.422\nName: Price, dtype: float64\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nmodel = LinearRegression()\n\nmodel.fit(X_train, y_train)\n\ny_hat = model.predict(X_test)\n\nprint(\"MAE: \",  round(mean_absolute_error(y_test, y_hat), 2))\nprint(\"MSE: \",  round(mean_squared_error(y_test, y_hat), 2))\nprint(\"RMSE:\",  round((mean_squared_error(y_test, y_hat))**(1/2), 2))\nprint(\"R2:  \",  round(r2_score(y_test, y_hat), 2))\n\nMAE:  0.53\nMSE:  0.51\nRMSE: 0.71\nR2:   0.62\n\n\nChoosing between these metrics depends on the specific context of the problem:\n\nUse MAE if you want a metric that’s easy to understand and not influenced much by outliers.\nUse RMSE when larger errors should be penalized more heavily and a metric in the same unit as the target variable is desired.\nUse MSE when optimizing models since it emphasizes larger errors, making it useful in minimizing those errors during training\n\n\nresults = pd.DataFrame()\nresults['Actual'] = y_test\nresults['Prediction'] = y_hat\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nplt.figure(figsize=(10, 3))\nsns.scatterplot(x='Actual', y='Prediction', data=results);"
  },
  {
    "objectID": "calculus/gradientdescent.html#interpreting-the-model",
    "href": "calculus/gradientdescent.html#interpreting-the-model",
    "title": "Gradient Descent",
    "section": "Interpreting the Model",
    "text": "Interpreting the Model\nThe model we have trained is a linear function of the form:\n\\[\\text{MedianHouseValue} = (m_1 \\times \\text{MedInc}) + (m_2 \\times \\text{HouseAge}) + \\\\(m_3 \\times \\text{AveRooms}) + (m_4 \\times \\text{AveBedrms}) + \\\\(m_5 \\times \\text{Population}) + (m_6 \\times \\text{AveOccup}) + \\\\(m_7 \\times \\text{Longitude}) + (m_8 \\times \\text{Latitude}) + b\\]\nwhere \\(m_1, m_2, \\dots, m_8\\) are the slopes and \\(b\\) is the y-intercept.\nThe slopes \\(m_1, m_2, \\dots, m_8\\) tell us how much the target variable changes when the corresponding input variable changes by 1 unit.\nThe code cell below plots the slopes in decreasing order of magnitude.\n\nweights = pd.Series(model.coef_, index=X.columns)\nweights = weights.sort_values(ascending=False)\nsns.barplot(x=weights.index, y=weights.values, palette='bwr');\nplt.xticks(rotation=90);\n\n\n\n\n\n\n\n\nThe plot indicates that AveBedrms, MedInc and AveRooms have the highest positive relationship with the Price of the house whereas AveRooms, Latitude, Longitude have the strongest negative relationship with the target variable Price of the house."
  },
  {
    "objectID": "calculus/gradientdescent.html#polynomial-regression",
    "href": "calculus/gradientdescent.html#polynomial-regression",
    "title": "Gradient Descent",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\nPolynomial functions are functions that have the form:\n\\[f(x) = b + m_1 x + m_2 x^2 + m_3 x^3 + ... + m_n x^n\\]\nwhere \\(b, m_1, m_2, m_3, ..., w_n\\) are the coefficients of the polynomial function and \\(n\\) is called the degree of the polynomial. In other words, the degree of a polynomial function is the highest power of the variable in the polynomial function.\nNote that the linear function \\(f(x) = mx + b\\) is a special case of the polynomial function. More specifically, a linear function is a polynomial function of degree 1.\nPolynomial functions of degree 2 or higher are called non-linear functions. As the degree of the polynomial function increases, the function becomes more flexible and can fit more complex patterns in the data.\nIf we have only one input variable \\(x\\) to predict the output variable \\(y\\), then the polynomial function becomes:\n\\[y = f(x) = b + m_1 x + m_2 x^2 + m_3 x^3 + ... + m_n x^n\\]\nIn matrix notation, polynomial regression can be written as:\n\\[f(\\mathbf{x}) = \\mathbf{X}\\mathbf{m} + b\\]\nwhere \\(\\mathbf{X}\\) is a matrix of \\(N\\) observations and each feature is raised to a power from 1 to \\(D\\).\n\\[\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_N \\\\\n\\end{bmatrix}  = \\begin{bmatrix}\nx_1 & x_1^2 & x_1^3 & \\dots & x_1^D \\\\\nx_2 & x_2^2 & x_2^3 & \\dots & x_2^D \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_N & x_N^2 & x_N^3 & \\dots & x_N^D \\\\\n\\end{bmatrix} \\cdot \\begin{bmatrix}\nm_1 \\\\\nm_2 \\\\\n\\vdots \\\\\nm_D \\\\\n\\end{bmatrix} + b\\]\n\nCode Example\nLet’s implement polynomial regression on the Average Land Temperature dataset.\n\nimport pandas as pd\n\ndata         = pd.read_csv('../data/GlobalLandTemperaturesByCountry.csv')\ndata['dt']   = pd.to_datetime(data['dt'])\ndata['year'] = data['dt'].apply(lambda x: x.year)\ndata['month']= data['dt'].apply(lambda x: x.month)\ndata         = data.dropna()\ndata         = data[(data['year'] &gt;= 1900) & (data['month'] == 1)]\navgs         = data.groupby('year').mean()['AverageTemperature']\navgs.name    = 'Average Temperature (C)'\n\nsns.scatterplot(x=avgs.index, y=avgs);\n\n\n\n\n\n\n\n\n\nX = avgs.index.values.reshape(-1, 1)\ny = avgs\n\n\nannual_means = data.groupby('year').mean()['AverageTemperature'][::10]\n\nX = annual_means.index.values.reshape(-1, 1)\ny = annual_means.values\n\n\ntrain_set = avgs.sample(int(0.8*len(avgs)), random_state=42).sort_index()\ntest_set  = avgs.drop(train_set.index).sort_index()\n\nX_train   = train_set.index.values.reshape(-1, 1)\ny_train   = train_set.values.reshape(-1, 1)\n\nX_test    = test_set.index.values.reshape(-1, 1)\ny_test    = test_set.values.reshape(-1, 1)\n\nImplementing polynomial regression in sklearn is similar to linear regression but with one additional step. We need to transform the input data into a polynomial matrix before fitting the model. In sklearn, this is done using the PolynomialFeatures class.\nThe code cell below implements polynomial regression of degrees 1, 2 and 5 on the Average Land Temperature dataset.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\nplt.figure(figsize=(5, 5))\nplt.scatter(X_train, y_train, label='X', alpha=0.7);\n\ncolors = ['orange', 'green', 'red']\n\nfor i, degree in enumerate([1, 2, 5]):\n\n    # Create polynomial features for X_train and X_test\n    poly         = PolynomialFeatures(degree=degree)\n    X_train_poly = poly.fit_transform(X_train)\n    X_test_poly  = poly.fit_transform(X_test)\n\n    # Fit a linear regression model to the training data\n    model        = LinearRegression()\n    model.fit(X_train_poly, y_train)\n\n    # Predict y values for X_test\n    y_pred       = model.predict(X_test_poly)\n    \n    # Plot the predictions\n    plt.plot(X_test, y_pred, linewidth=3, label='Degree = %s' % degree, alpha=0.7, color=colors[i]);\n\nplt.legend();\n\n\n\n\n\n\n\n\nNote that with increasing degree, the polynomial function can fit more complex patterns non-linear trends in the data."
  },
  {
    "objectID": "calculus/gradientdescent.html#underfitting-vs.-overfitting",
    "href": "calculus/gradientdescent.html#underfitting-vs.-overfitting",
    "title": "Gradient Descent",
    "section": "Underfitting vs. Overfitting",
    "text": "Underfitting vs. Overfitting\nThis example demonstrates the problems of underfitting and overfitting and how we can use linear regression with polynomial features to approximate nonlinear functions.\nThe plot shows the function that we want to approximate, which is a part of the cosine function. In addition, the samples from the real function and the approximations of different models are displayed. The models have polynomial features of different degrees. We can see that a linear function (polynomial with degree 1) is not sufficient to fit the training samples. This is called underfitting.\nA polynomial of degree 4 approximates the true function almost perfectly. However, for higher degrees the model will overfit the training data, i.e. it learns the noise of the training data. This is called overfitting. We evaluate quantitatively overfitting / underfitting by using cross-validation.\nWe calculate the mean squared error (MSE) on the validation set, the higher, the less likely the model generalizes correctly from the training data.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\ndef true_fun(X):\n    return np.cos(1.5 * np.pi * X)\n\nnp.random.seed(0)\n\nn_samples = 30\ndegrees = [1, 4, 15]\n\nX = np.sort(np.random.rand(n_samples))\ny = true_fun(X) + np.random.randn(n_samples) * 0.1\n\n\nax = sns.scatterplot(x=X, y=y, label='Samples');\nax.set(xlabel='X', ylabel='y', title = r'$y = cos(1.5 \\pi  x) + \\epsilon$');\n\nsns.lineplot(x=X, y=true_fun(X), label='True function', color='blue');\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(14, 5))\nfor i in range(len(degrees)):\n    ax = plt.subplot(1, len(degrees), i + 1)\n    plt.setp(ax, xticks=(), yticks=())\n\n    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)\n    linear_regression = LinearRegression()\n    pipeline = Pipeline(\n        [\n            (\"polynomial_features\", polynomial_features),\n            (\"linear_regression\", linear_regression),\n        ]\n    )\n    pipeline.fit(X[:, np.newaxis], y)\n\n    # Evaluate the models using crossvalidation\n    scores = cross_val_score(\n        pipeline, X[:, np.newaxis], y, scoring=\"neg_mean_squared_error\", cv=10\n    )\n\n    X_test = np.linspace(0, 1, 100)\n    plt.scatter(X, y, edgecolor=\"b\", s=20, label=\"Samples\")\n    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model %s (degree = %s)\" % (i+1, degrees[i]))\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.xlim((0, 1))\n    plt.ylim((-2, 2))\n    plt.legend(loc=\"best\")\n    plt.title(\n        \"MSE = {:.2} +/- {:.2}\".format(\n            round(-scores.mean(), 2), round(scores.std(), 2)\n        )\n    )\n\nplt.suptitle(\"Polynomial Regression with increasing degrees, leading to overfitting\", fontsize=14);\nplt.show()\n\n\n\n\n\n\n\n\nIdeally, you want to strike a balance between underfitting (high training error, high testing error) and overfitting (low training error, high testing error) by picking a model complexity (number of parameters) that generalizes well to unseen data.\n\nNote that model complexity here refers to the number of parameters in the model. For example, univariate linear regression model has 2 parameters (slope and y-intercept) whereas a polynomial regression model of degree 2 has 3 parameters (slope, y-intercept and coefficient of \\(x^2\\))."
  },
  {
    "objectID": "calculus/41_classification.html",
    "href": "calculus/41_classification.html",
    "title": "Classification",
    "section": "",
    "text": "Classification lies at the heart of both human and machine intelligence. Deciding what letter, word, or image has been presented to our senses, recognizing faces or voices, sorting mail, assigning grades to homeworks; these are all examples of assigning a category to an input.\nOne method for classification is to use handwritten rules. There are many areas of data mining where handwritten rule-based classifiers constitute a state-of-the-art system, or at least part of it. Rules can be fragile, however, as situations or data change over time, and for some tasks humans aren’t necessarily good at coming up with the rules. Most cases of classification therefore are instead done via supervised machine learning.\nClassification is the type of supervised learning where \\(\\mathcal{y}\\) is a discrete categorical variable.\nThe discrete output variable \\(\\mathcal{y}\\) is often also called the label or target or class.\nFor example, we might want to predict whether a patient has a disease or not, based on their symptoms. In this case, \\(\\mathcal{y}\\) is a binary variable, taking the value 1 if the patient has the disease, and 0 otherwise. Other examples of classification problems include predicting the sentiment of a movie review: positive, negative, or neutral.\nFor example,\nIn other words, the classification problem is to learn a function \\(f\\) that maps the input \\(\\mathcal{X}\\) to the discrete output \\(\\mathcal{Y}\\)."
  },
  {
    "objectID": "calculus/41_classification.html#evaluation-metrics",
    "href": "calculus/41_classification.html#evaluation-metrics",
    "title": "Classification",
    "section": "Evaluation Metrics",
    "text": "Evaluation Metrics\nThe most common metric for evaluating a classifier is accuracy. Accuracy is the proportion of correct predictions. It is the number of correct predictions divided by the total number of predictions.\n\\[Accuracy = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\\]\nFor example, if we have a test set of 100 documents, and our classifier correctly predicts the class of 80 of them, then the accuracy is 80%.\nAccuracy is a good metric when the classes are balanced \\(N_{class1} \\approx N_{class2}\\). However, when the classes are imbalanced, accuracy can be misleading. For example, if we have a test set of 100 documents, and 95 of them are positive and 5 of them are negative, then a classifier that always predicts positive will have an accuracy of 95%. However, this classifier is not useful, because it never predicts negative.\n\nMulti-class classification as multiple Binary classifications\nEvery multi-class classification problem can be decomposed into multiple binary classification problems. For example, if we have a multi-class classification problem with 3 classes, we can decompose it into 3 binary classification problems.\n\n\n \nAssuming the categorical variable that we are trying to predict is binary, we can define the accuracy in terms of the four possible outcomes of a binary classifier:\n\nTrue Positive (TP): The classifier correctly predicted the positive class.\nFalse Positive (FP): The classifier incorrectly predicted the negative class as positive.\nTrue Negative (TN): The classifier correctly predicted the negative class.\nFalse Negative (FN): The classifier incorrectly predicted the positive class as negative.\n\nTrue positive means that the classifier correctly predicted the positive class. False positive means that the classifier incorrectly predicted the positive class. True negative means that the classifier correctly predicted the negative class. False negative means that the classifier incorrectly predicted the negative class.\nThese definitions are summarized in the table below:\n\n\n\n\nPrediction \\(\\hat{y} = f'(x)\\)\nTruth \\(y = f(x)\\)\n\n\n\n\nTrue Negative (TN)\n0\n0\n\n\nFalse Negative (FN)\n0\n1\n\n\nFalse Positive (FP)\n1\n0\n\n\nTrue Positive (TP)\n1\n1\n\n\n\nIn terms of the four outcomes above, the accuracy is:\n\\[ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\\]\nAccuracy is a useful metric, but it can be misleading.\nOther metrics that are often used to evaluate classifiers are:\n\nPrecision: The proportion of positive predictions that are correct. Mathematically, it is defined as:\n\n\\[\\text{Precision} = \\frac{TP}{TP + FP}\\]\n\nRecall: The proportion of positive instances that are correctly predicted. Mathematically, it is defined as:\n\n\\[\\text{Recall} = \\frac{TP}{TP + FN}\\]\nThe precision and recall are often combined into a single metric called the F1 score. The F1 score is the harmonic mean of precision and recall. The harmonic mean of two numbers is given by:\n\nF1 Score: The harmonic mean of precision and recall.\n\n\\[\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\]\n\n\nMany kinds of machine learning algorithms are used to build classifiers. Two common classifiers are Naive Bayes and Logistic Regression.\nThese exemplify two primary category of models for doing classification:\n\nGenerative models like naive Bayes build a model of how a class could generate some input data. Given an observation, they return the class most likely to have generated the observation.\nDiscriminative models like logistic regression instead learn what features from the input are most useful to discriminate between the different possible classes.\n\nWhile discriminative systems are often more accurate and hence more commonly used, generative classifiers still have a role. They can be more robust to missing data, and can be used to generate synthetic data.\n\n\nCode Example\nThe code below shows how to train a Nearest Neighbor classifier on the Iris dataset. The Iris dataset is a dataset of 150 observations of iris flowers. There are 3 classes of iris flowers: setosa, versicolor, and virginica. For each observation, there are 4 features: sepal length, sepal width, petal length, and petal width. The goal is to predict the class of iris flower given the 4 features.\nThe code below uses the scikit-learn library to train a Nearest Neighbor classifier on the Iris dataset. The Nearest Neighbor classifier is a simple classifier that works by finding the training observation that is closest to the test observation, and predicting the class of the closest training observation. The Nearest Neighbor classifier is a discriminative classifier.\nThere are 5 steps shown in the code below:\n\nImport the dataset: The Iris dataset is included in scikit-learn. We import it using the load_iris function.\nSplit the dataset into training and test sets: We split the dataset into a training set and a test set. The training set is used to train the classifier, and the test set is used to evaluate the classifier.\nInstantiate the classifier: We instantiate the classifier using the KNeighborsClassifier class.\nTrain the classifier: We train the classifier using the fit method.\nMake predictions: We make predictions on the test set using the predict method.\nPrint the classification report: We evaluate the classifier using the classification_report function.\n\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\n\n# 1. Load the data\ndata = load_iris(as_frame=True)\nX    = data['data']\ny    = data['target']\n\n# 2. Create a train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# 3. Instantiate a model\nmodel = KNeighborsClassifier()\n\n# 4. Fit a model\nmodel.fit(X_train, y_train)\n\n# 5. Predict on the test set\npreds = model.predict(X_test)\n\n# 6. Print classification report\nprint(classification_report(y_test, preds))\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00         8\n           1       0.91      1.00      0.95        10\n           2       1.00      0.92      0.96        12\n\n    accuracy                           0.97        30\n   macro avg       0.97      0.97      0.97        30\nweighted avg       0.97      0.97      0.97        30"
  },
  {
    "objectID": "linearalgebra/vectors.html",
    "href": "linearalgebra/vectors.html",
    "title": "Geometric Interpretation",
    "section": "",
    "text": "Vectors have two common geometric interpretations:\n\nVectors as Points in Feature Space: In this interpretation, we consider vectors as points in a space with a fixed reference point called the origin.\nVectors as Displacement: In this interpretation, we consider vectors as displacements between points in space.\n\n\n\nGiven a vector, the first interpretation that we should give it is as a point in space.\nIn two or three dimensions, we can visualize these points by using the components of the vectors to define the location of the points in space compared to a fixed reference called the origin. This can be seen in the figure below.\n\n\n\n\nThis geometric point of view allows us to consider the problem on a more abstract level. No longer faced with some insurmountable seeming problem like classifying pictures as either cats or dogs, we can start considering tasks abstractly as collections of points in space and picturing the task as discovering how to separate two distinct clusters of points.\n\nimport pandas as pd \nfrom matplotlib import pyplot as plt\n\nplt.style.use('dark_background')\n\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\n\nvector1 = [1, 2]\nvector2 = [2, -1]\n\ndisplacement = 0.1\n\n# Plotting vector 1\nplt.scatter(x=vector1[0], y=vector1[1], color='blue');\nplt.text(x=vector1[0]+displacement, y=vector1[1], \\\n             s=f\"(%s, %s)\" % (vector1[0], vector1[1]), size=15);\n\n# Plotting vector 2\nplt.scatter(x=vector2[0], y=vector2[1], color='magenta');\nplt.text(x=vector2[0]+displacement, y=vector2[1], \\\n             s=f\"(%s, %s)\" % (vector2[0], vector2[1]), size=15);\n\n# Plotting the x and y axes\nplt.axhline(0, color='white');\nplt.axvline(0, color='white');\n\n# Plotting the legend\nplt.legend(['vector1', 'vector2'], loc='upper left');\n\n\n\n\n\n\n\n\n\n\n\nIn parallel, there is a second point of view that people often take of vectors: as directions in space. Not only can we think of the vector \\(\\textbf{v} = [3, 2]^{T}\\) as the location \\(3\\) units to the right and \\(2\\) units up from the origin, we can also think of it as the direction itself to take \\(3\\) steps to the right and \\(2\\) steps up. In this way, we consider all the vectors in figure below the same.\n\n\n\n\n\n\n\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\n\n# Plotting vector 1\nplt.quiver(0, 0, vector1[0], vector1[1], scale=1, scale_units='xy', angles='xy', color='blue')\nplt.text(x=vector1[0]+displacement, y=vector1[1], \\\n             s=f\"(%s, %s)\" % (vector1[0], vector1[1]), size=20);\n\n# Plotting vector 2\nplt.quiver(0, 0, vector2[0], vector2[1], scale=1, scale_units='xy', angles='xy', color='violet')\nplt.text(x=vector2[0]+displacement, y=vector2[1], \\\n             s=f\"(%s, %s)\" % (vector2[0], vector2[1]), size=20);\n\nplt.legend(['vector1', 'vector2'], loc='upper left');\n\n# Plotting the x and y axes\nplt.axhline(0, color='white');\nplt.axvline(0, color='white');\n\n\n\n\n\n\n\n\nOne of the benefits of this shift is that we can make visual sense of the act of vector addition. In particular, we follow the directions given by one vector, and then follow the directions given by the other, as seen below:\n\n\n\n\n\nVector subtraction has a similar interpretation. By considering the identity that \\(\\mathbf{u} = \\mathbf{v} + (\\mathbf{u} - \\mathbf{v})\\), we see that the vector \\(\\mathbf{u} - \\mathbf{v}\\) is the direction that takes us from the point \\(\\mathbf{v}\\) to the point \\(\\mathbf{u}\\).\n\nvector1 = pd.Series([1, 2])\nvector2 = pd.Series([2, -1])\n\nsum_vector = vector1 + vector2\n\nsum_vector\n\n0    3\n1    1\ndtype: int64\n\n\n\nvector1 = pd.Series([1, 2])\nvector2 = pd.Series([2, -1])\nsum = vector1 + vector2\n\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\n\n# Plotting vector 1\nplt.quiver(0, 0, vector1[0], vector1[1], scale=1, scale_units='xy', angles='xy', color='blue')\nplt.text(x=vector1[0]+displacement, y=vector1[1], \\\n             s=f\"(%s, %s)\" % (vector1[0], vector1[1]), size=20);\n\n# Plotting vector 2\nplt.quiver(vector1[0], vector1[1], vector2[0], vector2[1], scale=1, scale_units='xy', angles='xy', color='magenta')\nplt.text(x=vector2[0]+displacement, y=vector2[1], \\\n             s=f\"(%s, %s)\" % (vector2[0], vector2[1]), size=20);\n\n\nplt.quiver(0, 0, sum[0], sum[1], scale=1, scale_units='xy', angles='xy', color='lime')\nplt.text(x=sum[0]+displacement, y=sum[1], \\\n             s=f\"(%s, %s)\" % (sum[0], sum[1]), size=20);\n\nplt.legend(['vector1', 'vector2', 'sum'], loc='upper left');\n\n# Plotting the x and y axes\nplt.axhline(0, color='white');\nplt.axvline(0, color='white');",
    "crumbs": [
      "Home",
      "Geometric Interpretation"
    ]
  },
  {
    "objectID": "linearalgebra/vectors.html#geometry-of-vectors",
    "href": "linearalgebra/vectors.html#geometry-of-vectors",
    "title": "Geometric Interpretation",
    "section": "",
    "text": "Vectors have two common geometric interpretations:\n\nVectors as Points in Feature Space: In this interpretation, we consider vectors as points in a space with a fixed reference point called the origin.\nVectors as Displacement: In this interpretation, we consider vectors as displacements between points in space.\n\n\n\nGiven a vector, the first interpretation that we should give it is as a point in space.\nIn two or three dimensions, we can visualize these points by using the components of the vectors to define the location of the points in space compared to a fixed reference called the origin. This can be seen in the figure below.\n\n\n\n\nThis geometric point of view allows us to consider the problem on a more abstract level. No longer faced with some insurmountable seeming problem like classifying pictures as either cats or dogs, we can start considering tasks abstractly as collections of points in space and picturing the task as discovering how to separate two distinct clusters of points.\n\nimport pandas as pd \nfrom matplotlib import pyplot as plt\n\nplt.style.use('dark_background')\n\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\n\nvector1 = [1, 2]\nvector2 = [2, -1]\n\ndisplacement = 0.1\n\n# Plotting vector 1\nplt.scatter(x=vector1[0], y=vector1[1], color='blue');\nplt.text(x=vector1[0]+displacement, y=vector1[1], \\\n             s=f\"(%s, %s)\" % (vector1[0], vector1[1]), size=15);\n\n# Plotting vector 2\nplt.scatter(x=vector2[0], y=vector2[1], color='magenta');\nplt.text(x=vector2[0]+displacement, y=vector2[1], \\\n             s=f\"(%s, %s)\" % (vector2[0], vector2[1]), size=15);\n\n# Plotting the x and y axes\nplt.axhline(0, color='white');\nplt.axvline(0, color='white');\n\n# Plotting the legend\nplt.legend(['vector1', 'vector2'], loc='upper left');\n\n\n\n\n\n\n\n\n\n\n\nIn parallel, there is a second point of view that people often take of vectors: as directions in space. Not only can we think of the vector \\(\\textbf{v} = [3, 2]^{T}\\) as the location \\(3\\) units to the right and \\(2\\) units up from the origin, we can also think of it as the direction itself to take \\(3\\) steps to the right and \\(2\\) steps up. In this way, we consider all the vectors in figure below the same.\n\n\n\n\n\n\n\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\n\n# Plotting vector 1\nplt.quiver(0, 0, vector1[0], vector1[1], scale=1, scale_units='xy', angles='xy', color='blue')\nplt.text(x=vector1[0]+displacement, y=vector1[1], \\\n             s=f\"(%s, %s)\" % (vector1[0], vector1[1]), size=20);\n\n# Plotting vector 2\nplt.quiver(0, 0, vector2[0], vector2[1], scale=1, scale_units='xy', angles='xy', color='violet')\nplt.text(x=vector2[0]+displacement, y=vector2[1], \\\n             s=f\"(%s, %s)\" % (vector2[0], vector2[1]), size=20);\n\nplt.legend(['vector1', 'vector2'], loc='upper left');\n\n# Plotting the x and y axes\nplt.axhline(0, color='white');\nplt.axvline(0, color='white');\n\n\n\n\n\n\n\n\nOne of the benefits of this shift is that we can make visual sense of the act of vector addition. In particular, we follow the directions given by one vector, and then follow the directions given by the other, as seen below:\n\n\n\n\n\nVector subtraction has a similar interpretation. By considering the identity that \\(\\mathbf{u} = \\mathbf{v} + (\\mathbf{u} - \\mathbf{v})\\), we see that the vector \\(\\mathbf{u} - \\mathbf{v}\\) is the direction that takes us from the point \\(\\mathbf{v}\\) to the point \\(\\mathbf{u}\\).\n\nvector1 = pd.Series([1, 2])\nvector2 = pd.Series([2, -1])\n\nsum_vector = vector1 + vector2\n\nsum_vector\n\n0    3\n1    1\ndtype: int64\n\n\n\nvector1 = pd.Series([1, 2])\nvector2 = pd.Series([2, -1])\nsum = vector1 + vector2\n\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\n\n# Plotting vector 1\nplt.quiver(0, 0, vector1[0], vector1[1], scale=1, scale_units='xy', angles='xy', color='blue')\nplt.text(x=vector1[0]+displacement, y=vector1[1], \\\n             s=f\"(%s, %s)\" % (vector1[0], vector1[1]), size=20);\n\n# Plotting vector 2\nplt.quiver(vector1[0], vector1[1], vector2[0], vector2[1], scale=1, scale_units='xy', angles='xy', color='magenta')\nplt.text(x=vector2[0]+displacement, y=vector2[1], \\\n             s=f\"(%s, %s)\" % (vector2[0], vector2[1]), size=20);\n\n\nplt.quiver(0, 0, sum[0], sum[1], scale=1, scale_units='xy', angles='xy', color='lime')\nplt.text(x=sum[0]+displacement, y=sum[1], \\\n             s=f\"(%s, %s)\" % (sum[0], sum[1]), size=20);\n\nplt.legend(['vector1', 'vector2', 'sum'], loc='upper left');\n\n# Plotting the x and y axes\nplt.axhline(0, color='white');\nplt.axvline(0, color='white');",
    "crumbs": [
      "Home",
      "Geometric Interpretation"
    ]
  },
  {
    "objectID": "linearalgebra/vectors.html#norms",
    "href": "linearalgebra/vectors.html#norms",
    "title": "Geometric Interpretation",
    "section": "Norms",
    "text": "Norms\nSome of the most useful operators in linear algebra are norms. A norm is a function \\(\\| \\cdot \\|\\) that maps a vector to a scalar.\nInformally, the norm of a vector tells us magnitude or length of the vector.\nFor instance, the \\(l_2\\) norm measures the euclidean length of a vector. That is, \\(l_2\\) norm measures the euclidean distance of a vector from the origin \\((0, 0)\\).\n\n \\[ \\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2} \\]\n\nx = pd.Series(vector1)\nl2_norm = (x**2).sum()**(1/2)\nl2_norm\n\n2.23606797749979\n\n\nThe \\(l_1\\) norm is also common and the associated measure is called the Manhattan distance. By definition, the \\(l_1\\) norm sums the absolute values of a vector’s elements:\n\\[ \\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right| \\]\nCompared to the \\(l_2\\) norm, it is less sensitive to outliers. To compute the \\(l_1\\) norm, we compose the absolute value with the sum operation.\n\nl1_norm = x.abs().sum()\nl1_norm\n\n6\n\n\nBoth the \\(l_1\\) and \\(l_2\\) norms are special cases of the more general norms:\n\\[ \\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}. \\]\n\nvec = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9])\n\np = 3\n\nlp_norm = ((abs(vec))**p).sum()**(1/p)\n\nlp_norm\n\n12.651489979526238",
    "crumbs": [
      "Home",
      "Geometric Interpretation"
    ]
  },
  {
    "objectID": "linearalgebra/vectors.html#dot-product",
    "href": "linearalgebra/vectors.html#dot-product",
    "title": "Geometric Interpretation",
    "section": "Dot Product",
    "text": "Dot Product\nOne of the most fundamental operations in linear algebra (and all of data science and machine learning) is the dot product.\nGiven two vectors \\(\\textbf{x}, \\textbf{y} \\in \\mathbb{R}^d\\), their dot product \\(\\textbf{x}^{\\top} \\textbf{y}\\) (also known as inner product \\(\\langle \\textbf{x}, \\textbf{y} \\rangle\\)) is a sum over the products of the elements at the same position:\n\\[\\textbf{x}^\\top \\textbf{y} = \\sum_{i=1}^{d} x_i y_i\\]\n\nimport pandas as pd\n\nx = pd.Series([1, 2, 3])\ny = pd.Series([4, 5, 6])\n\nx.dot(y) # 1*4 + 2*5 + 3*6 \n\n32\n\n\nEquivalently, we can calculate the dot product of two vectors by performing an elementwise multiplication followed by a sum:\n\nsum(x * y)\n\n32\n\n\nDot products are useful in a wide range of contexts. For example, given some set of values, denoted by a vector $ ^{n} $ , and a set of weights, denoted by \\(\\mathbf{x} \\in \\mathbb{R}^{n}\\), the weighted sum of the values in \\(\\mathbf{x}\\) according to the weights \\(\\mathbf{w}\\) could be expressed as the dot product \\(\\mathbf{x}^\\top \\mathbf{w}\\). When the weights are nonnegative and sum to \\(1\\), i.e., \\((\\sum_{i=1}^n w_i = 1)\\), the dot product expresses a weighted average. After normalizing two vectors to have unit length, the dot products express the cosine of the angle between them. Later in this section, we will formally introduce this notion of length.",
    "crumbs": [
      "Home",
      "Geometric Interpretation"
    ]
  },
  {
    "objectID": "linearalgebra/vectors.html#dot-products-and-angles",
    "href": "linearalgebra/vectors.html#dot-products-and-angles",
    "title": "Geometric Interpretation",
    "section": "Dot Products and Angles",
    "text": "Dot Products and Angles\nIf we take two column vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\), we can form their dot product by computing:\n\\[ \\mathbf{u}^\\top\\mathbf{v} = \\sum_i u_i\\cdot v_i \\]\nBecause the equation above is symmetric, we will mirror the notation of classical multiplication and write\n\\[ \\mathbf{u}\\cdot\\mathbf{v} = \\mathbf{u}^\\top\\mathbf{v} = \\mathbf{v}^\\top\\mathbf{u}, \\]\nto highlight the fact that exchanging the order of the vectors will yield the same answer.\nThe dot product also admits a geometric interpretation: dot product it is closely related to the angle between two vectors.\n\n\n\nTo start, let’s consider two specific vectors:\n\\[ \\mathbf{v} = (r,0) \\; \\textrm{and} \\; \\mathbf{w} = (s\\cos(\\theta), s \\sin(\\theta)) \\]\nThe vector \\(\\mathbf{v}\\) is length \\(r\\) and runs parallel to the \\(x\\)-axis, and the vector \\(\\mathbf{w}\\) is of length \\(s\\) and at angle \\(\\theta\\) with the \\(x\\)-axis.\nIf we compute the dot product of these two vectors, we see that\n\\[ \\mathbf{v}\\cdot\\mathbf{w} = rs\\cos(\\theta) = \\|\\mathbf{v}\\|\\|\\mathbf{w}\\|\\cos(\\theta) \\]\nWith some simple algebraic manipulation, we can rearrange terms to obtain the equation for any two vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\):\n\\[ \\theta = \\arccos\\left(\\frac{\\mathbf{v}\\cdot\\mathbf{w}}{\\|\\mathbf{v}\\|\\|\\mathbf{w}\\|}\\right) \\]\nWe will not use it right now, but it is useful to know that we will refer to vectors for which the angle is \\(\\pi/2\\)(or equivalently \\(90^{\\circ}\\)) as being orthogonal.\nBy examining the equation above, we see that this happens when \\(\\theta = \\pi/2\\), which is the same thing as \\(cos(\\theta) = 0\\).\nThe only way this can happen is if the dot product itself is zero, and two vectors are orthogonal if and only if \\(\\mathbf{v}\\cdot\\mathbf{w} = 0\\).\nThis will prove to be a helpful formula when understanding objects geometrically.\nIt is reasonable to ask: why is computing the angle useful? Consider the problem of classifying text data. We might want the topic or sentiment in the text to not change if we write twice as long of document that says the same thing.\nFor some encoding (such as counting the number of occurrences of words in some vocabulary), this corresponds to a doubling of the vector encoding the document, so again we can use the angle.\n\nv = pd.Series([0, 2])\nw = pd.Series([2, 0])\n\nv.dot(w)\n\n0\n\n\n\nfrom math import acos\n\ndef l2_norm(vec):\n    return (vec**2).sum()**(1/2)\n\nv = pd.Series([0, 2])\nw = pd.Series([2, 0])\n\nv.dot(w) / (l2_norm(v) * l2_norm(w))\n\n0.0\n\n\n\nfrom math import acos, pi\n\ntheta = acos(v.dot(w) / (l2_norm(v) * l2_norm(w)))\n\ntheta == pi / 2\n\nTrue",
    "crumbs": [
      "Home",
      "Geometric Interpretation"
    ]
  },
  {
    "objectID": "linearalgebra/vectors.html#cosine-similaritydistance",
    "href": "linearalgebra/vectors.html#cosine-similaritydistance",
    "title": "Geometric Interpretation",
    "section": "Cosine Similarity/Distance",
    "text": "Cosine Similarity/Distance\nIn ML contexts where the angle is employed to measure the closeness of two vectors, practitioners adopt the term cosine similarity to refer to the portion\n\\[ \\cos(\\theta) = \\frac{\\mathbf{v}\\cdot\\mathbf{w}}{\\|\\mathbf{v}\\|\\|\\mathbf{w}\\|}. \\]\nThe cosine takes a maximum value of \\(1\\) when the two vectors point in the same direction, a minimum value of \\(-1\\) when they point in opposite directions, and a value of \\(0\\) when the two vectors are orthogonal. \nNote that cosine similarity can be converted to cosine distance by subtracting it from \\(1\\) and dividing by 2.\n\\[ \\text{Cosine Distance} = \\frac{1 - \\text{Cosine Similarity}}{2}\\]\nwhere \\(\\text{Cosine Similarity} = \\frac{\\mathbf{v}\\cdot\\mathbf{w}}{\\|\\mathbf{v}\\|\\|\\mathbf{w}\\|}\\)\nCosine distance is a very useful alternative to Euclidean distance for data where the absolute magnitude of the features is not particularly meaningful, which is a very common scenario in practice.\n\nfrom random import uniform\nimport pandas as pd\nimport seaborn as sns\n\ndf = pd.DataFrame()\ndf['cosine similarity'] = pd.Series([uniform(-1, 1) for i in range(100)])\ndf['cosine distance']   = (1 - df['cosine similarity'])/2\nax = sns.scatterplot(data=df, x='cosine similarity', y='cosine distance');\nax.set(title='Cosine Similarity vs. Cosine Distance')\nplt.grid()\n\n\n\n\n\n\n\n\n\ndef l2_norm(vec):\n    return (vec**2).sum()**(1/2)\n\nplt.axhline(0, color='black');\nplt.axvline(0, color='black');\n\nv = pd.Series([1.2, 1.2])\nw = pd.Series([2, 2.5])\n\nplt.quiver(0, 0, v[0], v[1], scale=1, scale_units='xy', angles='xy', color='navy')\nplt.quiver(0, 0, w[0], w[1], scale=1, scale_units='xy', angles='xy', color='magenta')\n\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\n\ncosine_similarity = v.dot(w) / (l2_norm(v) * l2_norm(w))\ncosine_similarity = round(cosine_similarity, 2)\n\ncosine_distance = (1 - cosine_similarity) / 2\ncosine_distance = round(cosine_distance, 2)\n\nplt.title(\"θ ≈ 0° (or 0 radians)\\n\"+\\\n          \"Cosine Similarity: %s \\nCosine Distance: %s\" % \\\n          (cosine_similarity, cosine_distance), size=15);\n\n\n\n\n\n\n\n\n\ncosine_similarity\n\n0.0\n\n\n\n\nplt.axhline(0, color='black');\nplt.axvline(0, color='black');\n\nv = pd.Series([2, 2])\nw = pd.Series([1, -1])\n\nplt.quiver(0, 0, v[0], v[1], scale=1, scale_units='xy', angles='xy', color='navy')\nplt.quiver(0, 0, w[0], w[1], scale=1, scale_units='xy', angles='xy', color='magenta')\n\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\n\ncosine_similarity = v.dot(w) / (l2_norm(v) * l2_norm(w))\ncosine_similarity = round(cosine_similarity, 2)\n\n\ncosine_distance = (1 - cosine_similarity) / 2\ncosine_distance = round(cosine_distance, 2)\n\nplt.title(\"θ = 90° (or π / 2 radians) \\nCosine Similarity: %s \\nCosine Distance: %s\" % (cosine_similarity, cosine_distance));\n\n\n\n\n\n\n\n\nNote that cosine similarity can be negative, which means that the angle is greater than \\(90^{\\circ}\\), i.e., the vectors point in opposite directions.\n\nv = pd.Series([2, 2])\nw = pd.Series([-1, -1])\n\nplt.quiver(0, 0, v[0], v[1], scale=1, scale_units='xy', angles='xy', color='navy')\nplt.quiver(0, 0, w[0], w[1], scale=1, scale_units='xy', angles='xy', color='magenta')\n\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\n\nplt.axhline(0, color='black');\nplt.axvline(0, color='black');\n\ncosine_similarity = v.dot(w) / (l2_norm(v) * l2_norm(w))\n\ncosine_similarity = round(cosine_similarity, 2)\n\ncosine_distance = (1 - cosine_similarity) / 2\ncosine_distance = round(cosine_distance, 2)\n\nplt.title(\"θ = 180° (or π radians)\\n\"+\\\n          \"Cosine Similarity: %s \\nCosine Distance: %s\" % \\\n          (cosine_similarity, cosine_distance), size=15);",
    "crumbs": [
      "Home",
      "Geometric Interpretation"
    ]
  },
  {
    "objectID": "plotting/anatomy.html",
    "href": "plotting/anatomy.html",
    "title": "Anatomy of a Plot",
    "section": "",
    "text": "Before we start creating plots, let’s take a look at the basic components of a plot in matplotlib:",
    "crumbs": [
      "Home",
      "Anatomy of a Plot"
    ]
  },
  {
    "objectID": "plotting/anatomy.html#figure-and-axes",
    "href": "plotting/anatomy.html#figure-and-axes",
    "title": "Anatomy of a Plot",
    "section": "Figure and Axes",
    "text": "Figure and Axes\nFigure in matplotlib is essentially the entire plot or image. This is the top-level container that holds all the other elements of the plot.\nAxes is the region of the plot that contains the data. This is where the data is actually plotted. A figure can have multiple axes.\n\nfig = plt.figure()  # an empty figure with no Axes\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n# a figure with a single Axes\n# by default, only one subplot (Axes) is created\n# figsize is a tuple of the width and height of the figure in inches\n\nfig, ax = plt.subplots(figsize=(3,3))  \n\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(2, 2)  # a figure with a 2x2 grid of Axes",
    "crumbs": [
      "Home",
      "Anatomy of a Plot"
    ]
  },
  {
    "objectID": "plotting/anatomy.html#axis-and-axis-labels",
    "href": "plotting/anatomy.html#axis-and-axis-labels",
    "title": "Anatomy of a Plot",
    "section": "Axis and Axis labels",
    "text": "Axis and Axis labels\nAxis (not to be confused with Axes) is the number line that represents the scale of the plot (e.g., x-axis, y-axis).\nAxis labels are the labels that describe the axis. They are usually placed at the end of the axis.\n\nfig, ax = plt.subplots(figsize=(3,3))  # a figure with a single Axes\n\nax.set_xlabel('x label');  # Add an x-label to the axes.\nax.set_ylabel('y label');  # Add a y-label to the axes.",
    "crumbs": [
      "Home",
      "Anatomy of a Plot"
    ]
  },
  {
    "objectID": "plotting/anatomy.html#ticks-and-tick-labels",
    "href": "plotting/anatomy.html#ticks-and-tick-labels",
    "title": "Anatomy of a Plot",
    "section": "Ticks and Tick Labels",
    "text": "Ticks and Tick Labels\nAn axis can have multiple ticks and labels.\nTicks are the marks on the axis that represent the scale of the plot.\nset_xticks() and set_yticks() are used to set the ticks on the x-axis and y-axis, respectively.\n\nfig, ax = plt.subplots(figsize=(3,3))  # a figure with a single Axes\n\n# ticks are the values used to show specific points on the coordinate axis\n# tick labels are the values shown at the ticks\n# set_xticks() and set_yticks() are used to set the tick locations\n\nax.set_xticks([0, 1, 2, 3, 4]);  # Set the x-ticks\n\n\n\n\n\n\n\n\n\nTick labels are the labels that describe the ticks. They are usually placed below the ticks.\nset_xticklabels() and set_yticklabels() are used to set the tick labels\n\n\nfig, ax = plt.subplots(figsize=(3,3))  # a figure with a single Axes\n\nax.set_xticks([0, 1, 2, 3, 4]);  # Set the x-ticks\nax.set_xticklabels(['zero', 'one', 'two', 'three', 'four']);  # Set the x-tick labels",
    "crumbs": [
      "Home",
      "Anatomy of a Plot"
    ]
  },
  {
    "objectID": "plotting/anatomy.html#title",
    "href": "plotting/anatomy.html#title",
    "title": "Anatomy of a Plot",
    "section": "Title",
    "text": "Title\nThe title of the plot.\n\n# a figure with a single Axes\nfig, ax = plt.subplots(figsize=(3,3))  \n\n# Add a title to the figure\nfig.suptitle('Title', fontsize=16);  \n\n\n\n\n\n\n\n\nFor a\n\n# a figure with a 2x2 grid of Axes\nfig, ax = plt.subplots(2, 2, figsize=(6,6)) \n\n# Add a title to the entire figure\nfig.suptitle('Super Title');  \n\n# a figure with a single Axes\nax[0][0].set_title('Top left');  \nax[0][1].set_title('Top right');  \nax[1][0].set_title('Bottom left');  \nax[1][1].set_title('Bottom right');",
    "crumbs": [
      "Home",
      "Anatomy of a Plot"
    ]
  },
  {
    "objectID": "plotting/anatomy.html#legend",
    "href": "plotting/anatomy.html#legend",
    "title": "Anatomy of a Plot",
    "section": "Legend",
    "text": "Legend\nA box that explains the meaning of the different colors or line styles in the plot.\n\n# a figure with a single Axes\nfig, ax = plt.subplots(figsize=(3,3))  \n\n# Legend is a box that labels the lines in a plot\n\n# Plot some data on the Axes\nax.plot([1, 2, 3, 4], [1, 4, 2, 3], label='line 1')\nax.plot([1, 2, 3, 4], [2, 3, 3, 2], label='line 2')\n\n# Add a legend\nax.legend(title='Legend', loc='upper right');",
    "crumbs": [
      "Home",
      "Anatomy of a Plot"
    ]
  },
  {
    "objectID": "plotting/anatomy.html#grid",
    "href": "plotting/anatomy.html#grid",
    "title": "Anatomy of a Plot",
    "section": "Grid",
    "text": "Grid\nThe lines that help to visually separate the different parts of the plot.\n\n# a figure with a single Axes\nfig, ax = plt.subplots(figsize=(3,3))  \n\nax.grid(True)  # Add a grid to the axes",
    "crumbs": [
      "Home",
      "Anatomy of a Plot"
    ]
  },
  {
    "objectID": "plotting/anatomy.html#styles",
    "href": "plotting/anatomy.html#styles",
    "title": "Anatomy of a Plot",
    "section": "Styles",
    "text": "Styles\nThe style of the plot can be customized using the plt.style parameter.\nSome of the available styles are:\n\ndark_background\nseaborn\nggplot\nfivethirtyeight\nbmh\ngrayscale\n\n\nplt.style.use('fivethirtyeight')  # Use a different style\n\n# a figure with a single Axes\nfig, ax = plt.subplots(figsize=(3,3))\n\n# Plot some data on the Axes\nax.plot([1, 2, 3, 4], [1, 4, 2, 3])\nax.plot([1, 2, 3, 4], [2, 2, 5, 5])\n\n# Display the figure\nplt.show()",
    "crumbs": [
      "Home",
      "Anatomy of a Plot"
    ]
  },
  {
    "objectID": "plotting/anatomy.html#saving-plots",
    "href": "plotting/anatomy.html#saving-plots",
    "title": "Anatomy of a Plot",
    "section": "Saving plots",
    "text": "Saving plots\nYou can save the plot as an image file using the plt.savefig() function.\n\nplt.style.use('fivethirtyeight')  # Use a different style\n\n# a figure with a single Axes\nfig, ax = plt.subplots(figsize=(3,3))\n\n# Plot some data on the Axes\nax.plot([1, 2, 3, 4], [1, 4, 2, 3])\nax.plot([1, 2, 3, 4], [2, 2, 5, 5])\n\n# Save the figure\nplt.savefig('plot.png')",
    "crumbs": [
      "Home",
      "Anatomy of a Plot"
    ]
  },
  {
    "objectID": "plotting/21_univariate.html",
    "href": "plotting/21_univariate.html",
    "title": "Univariate Visualizations",
    "section": "",
    "text": "Uni-variate (single variable) visualizations are used to better understand one-dimensional data (think pandas Series) in the dataset.\nIn this section, we will look at techniques to understand the distribution of each feature. Distribution of a feature, simply put, is the frequency of each value of the variable.\nWe know from previous discussion that .value_counts() method on a pandas Series gives us the frequency distribution of each value in the Series.\nWe will use the World Bank dataset, which contains information about countries and social statistics.\nimport pandas as pd \nimport seaborn as sns \nfrom matplotlib import pyplot as plt\n\nsns.set_style('whitegrid')\n\ndata = pd.read_csv('https://raw.githubusercontent.com/fahadsultan/csc272ta/world_bank.csv', index_col=0)\ndata.head()\n\n\n\n\n\n\n\n\nContinent\nCountry\nPrimary completion rate: Male: % of relevant age group: 2015\nPrimary completion rate: Female: % of relevant age group: 2015\nLower secondary completion rate: Male: % of relevant age group: 2015\nLower secondary completion rate: Female: % of relevant age group: 2015\nYouth literacy rate: Male: % of ages 15-24: 2005-14\nYouth literacy rate: Female: % of ages 15-24: 2005-14\nAdult literacy rate: Male: % ages 15 and older: 2005-14\nAdult literacy rate: Female: % ages 15 and older: 2005-14\n...\nAccess to improved sanitation facilities: % of population: 1990\nAccess to improved sanitation facilities: % of population: 2015\nChild immunization rate: Measles: % of children ages 12-23 months: 2015\nChild immunization rate: DTP3: % of children ages 12-23 months: 2015\nChildren with acute respiratory infection taken to health provider: % of children under age 5 with ARI: 2009-2016\nChildren with diarrhea who received oral rehydration and continuous feeding: % of children under age 5 with diarrhea: 2009-2016\nChildren sleeping under treated bed nets: % of children under age 5: 2009-2016\nChildren with fever receiving antimalarial drugs: % of children under age 5 with fever: 2009-2016\nTuberculosis: Treatment success rate: % of new cases: 2014\nTuberculosis: Cases detection rate: % of new estimated cases: 2015\n\n\n\n\n0\nAfrica\nAlgeria\n106.0\n105.0\n68.0\n85.0\n96.0\n92.0\n83.0\n68.0\n...\n80.0\n88.0\n95.0\n95.0\n66.0\n42.0\nNaN\nNaN\n88.0\n80.0\n\n\n1\nAfrica\nAngola\nNaN\nNaN\nNaN\nNaN\n79.0\n67.0\n82.0\n60.0\n...\n22.0\n52.0\n55.0\n64.0\nNaN\nNaN\n25.9\n28.3\n34.0\n64.0\n\n\n2\nAfrica\nBenin\n83.0\n73.0\n50.0\n37.0\n55.0\n31.0\n41.0\n18.0\n...\n7.0\n20.0\n75.0\n79.0\n23.0\n33.0\n72.7\n25.9\n89.0\n61.0\n\n\n3\nAfrica\nBotswana\n98.0\n101.0\n86.0\n87.0\n96.0\n99.0\n87.0\n89.0\n...\n39.0\n63.0\n97.0\n95.0\nNaN\nNaN\nNaN\nNaN\n77.0\n62.0\n\n\n5\nAfrica\nBurundi\n58.0\n66.0\n35.0\n30.0\n90.0\n88.0\n89.0\n85.0\n...\n42.0\n48.0\n93.0\n94.0\n55.0\n43.0\n53.8\n25.4\n91.0\n51.0\n\n\n\n\n5 rows × 47 columns"
  },
  {
    "objectID": "plotting/21_univariate.html#bar-plots",
    "href": "plotting/21_univariate.html#bar-plots",
    "title": "Univariate Visualizations",
    "section": "Bar Plots",
    "text": "Bar Plots\nBar plots are used to visualize the frequency distribution of features that take on a small set of unique values. Such features are more often categorical features as opposed to numerical features.\nVisualizing categorical features is easy. We can use .value_counts() method to get the frequency distribution of each value in the feature. This frequency distribution can be plotted using a bar chart.\n\nval_counts = data['Continent'].value_counts()\nprint(val_counts)\n\nAfrica        47\nEurope        43\nAsia          34\nN. America    18\nOceania       13\nS. America    11\nName: Continent, dtype: int64\n\n\nIn seaborn the method to make a bar chart is barplot(). The method takes in two arguments: x and y.\n\nsns.barplot(x=val_counts.index, y=val_counts.values);\n\n\n\n\n\n\n\n\nNote that the plots above are incomplete. If you show this plot to someone, they will not be able to understand what the plot is about.\nTo complete the plot, so it is understandable, we need to add atleast a title, axis labels and, depending on context, a legend."
  },
  {
    "objectID": "plotting/21_univariate.html#axis-labels-and-title",
    "href": "plotting/21_univariate.html#axis-labels-and-title",
    "title": "Univariate Visualizations",
    "section": "Axis labels and Title",
    "text": "Axis labels and Title\nTo add axis labels and title, we need to assign the plot object returned by sns.barplot to a variable and then use .set() method on the plot object.\n\nax = sns.barplot(x=val_counts.index, y=val_counts.values);\n\nax.set(xlabel='Continent', ylabel='Count of countries (rows)', title='Count of countries in each continent');\n\n\n\n\n\n\n\n\nNote that we have used .set() method on the plot object, returned by sns.barplot() method, and passed the arguments xlabel, ylabel and title to the method.\nNow, the plot is “complete” and can be understood by someone who has not seen the data.\n\nBar plots don’t scale. If you try to plot a feature (categorical or numerical) with a lot of unique values, the plot will be unreadable. Yes, even with a title and axis labels!\nHere is an example:\n\nval_counts = data['Country'].value_counts();\n\nax = sns.barplot(x=val_counts.index, y=val_counts.values);\n\nax.set(xlabel='Country', ylabel='Count', title='Frequency of countries in the World Bank data');\n\n\n\n\n\n\n\n\nThe plot above is unreadable. We can’t make out the labels on the x-axis. This is because there are too many unique values in the country feature.\nThere is no easy way to fix this for categorical features. For numerical features, however, there is a way."
  },
  {
    "objectID": "plotting/21_univariate.html#histograms",
    "href": "plotting/21_univariate.html#histograms",
    "title": "Univariate Visualizations",
    "section": "Histograms",
    "text": "Histograms\nHistograms look very similar to bar plots. However, they are different in a subtle but critical way.\nHistograms solve the problem of plotting a feature with a lot of unique values by binning the values.\nBinning is the process of dividing the range of values of a feature into bins. Each bin represents a range of values. The height of the bar in a histogram represents the number of values in the bin. In most cases, the bins are of equal width.\nFor example, if we have a feature with values ranging from 0 to 100, we can divide the range into 10 bins of width 10 each. The first bin will contain values from 0 to 10, the second bin will contain values from 10 to 20 and so on.\n\ncol = \"Gross domestic product: % growth : 2016\"\n\nax = sns.histplot(data=data, x=col);\n\nax.set(title=\"Distribution of GDP growth in 2016\");\n\n\n\n\n\n\n\n\nBy default, sns.histplot will try to infer the bin edges from the data. However, it is possible to set the bin edges explicitly. This can be useful when comparing multiple distributions."
  },
  {
    "objectID": "plotting/21_univariate.html#box-plots",
    "href": "plotting/21_univariate.html#box-plots",
    "title": "Univariate Visualizations",
    "section": "Box Plots",
    "text": "Box Plots\n\nBox plots display distributions using information about quartiles.\nA quartile represents a 25% portion of the data. We say that:\n\nThe first quartile (Q1) repesents the 25th percentile – 25% of the data lies below the first quartile\nThe second quartile (Q2) represents the 50th percentile, also known as the median – 50% of the data lies below the second quartile\nThe third quartile (Q3) represents the 75th percentile – 75% of the data lies below the third quartile.\n\nIn a box plot, the lower extent of the box lies at Q1, while the upper extent of the box lies at Q3. The horizontal line in the middle of the box corresponds to Q2 (equivalently, the median).\nThe Inter-Quartile Range (IQR) measures the spread of the middle % of the distribution, calculated as the (\\(3^{rd}\\) Quartile \\(-\\) \\(1^{st}\\) Quartile).\nThe whiskers of a box-plot are the two points that lie at the [\\(1^{st}\\) Quartile \\(-\\)(\\(1.5 \\times\\) IQR)], and the [\\(3^{rd}\\) Quartile \\(+\\) (\\(1.5 \\times\\) IQR)]. They are the lower and upper ranges of “normal” data (the points excluding outliers). Subsequently, the outliers are the data points that fall beyond the whiskers, or further than ( \\(1.5 \\times\\) IQR) from the extreme quartiles.\n\ncol = \"Gross domestic product: % growth : 2016\"\n\nprint(data[col].describe())\nax = sns.boxplot(data=data, y=col, width=0.5);\n\ncount    159.000000\nmean       2.780503\nstd        3.106862\nmin      -10.400000\n25%        1.450000\n50%        2.900000\n75%        4.500000\nmax       11.000000\nName: Gross domestic product: % growth : 2016, dtype: float64\n\n\n\n\n\n\n\n\n\nIf it helps, you can think of the box plot as a birds-eye-view of histogram. Histogram as seeing a hill from the side, while box plot is seeing the hill from above through a drone or through a bird’s eye.\nAlso note that boxplot is to .describe() as barplot is to .value_counts()."
  },
  {
    "objectID": "plotting/scatter.html",
    "href": "plotting/scatter.html",
    "title": "Scatter Plots (\\(\\geq\\) 2 numeric variables)",
    "section": "",
    "text": "Scatter plots are a great way to give you a sense of trends, concentrations, and outliers. This notebook will show you how to create scatter plots using Matplotlib.\nA scatter plot uses dots to represent values for two (or more) different numeric variables. The position of each dot on the horizontal and vertical axis indicates values for an individual data point. Scatter plots are used to observe relationships between variables.\nScatter plots are used when you want to show the relationship between two variables. Scatter plots are sometimes called correlation plots because they show how two variables are correlated.",
    "crumbs": [
      "Home",
      "Scatter Plots ($\\geq$ 2 numeric variables)"
    ]
  },
  {
    "objectID": "plotting/scatter.html#marker-size",
    "href": "plotting/scatter.html#marker-size",
    "title": "Scatter Plots (\\(\\geq\\) 2 numeric variables)",
    "section": "Marker Size",
    "text": "Marker Size\nThe size of the markers can be adjusted using the s parameter. This parameter controls the size of the markers. The default value is s=20.\n\n# creating figure and axis\nfig, ax = plt.subplots(1, 2, figsize=(10, 3))\n\n# scatter plot\nax[0].scatter(us_mainland['lng'], us_mainland['lat'], s=0.1);\nax[1].scatter(us_mainland['lng'], us_mainland['lat'], s=5);\n\n# setting labels and title\nax[0].set_title('s=0.1');\nax[1].set_title('s=5');\n\n\n\n\n\n\n\n\nThe s parameter accepts a scalar or an array of the same length as the number of data points.\n\n# creating figure and axis\nfig, ax = plt.subplots(figsize=(8, 5))\n\nscaling_factor = 1/50_000\n# scatter plot\nax.scatter(us_mainland['lng'], us_mainland['lat'], s=us_mainland['population']*scaling_factor);\n\n# setting labels and title\nax.set_title('Cities in the US Mainland, Size proportional to Population');",
    "crumbs": [
      "Home",
      "Scatter Plots ($\\geq$ 2 numeric variables)"
    ]
  },
  {
    "objectID": "plotting/scatter.html#marker-color",
    "href": "plotting/scatter.html#marker-color",
    "title": "Scatter Plots (\\(\\geq\\) 2 numeric variables)",
    "section": "Marker Color",
    "text": "Marker Color\nThe color of the markers can be adjusted using the c parameter. The c parameter accepts a scalar or an array of the same length as the number of data points. This parameter controls the color of the markers. The default value is c='b' (blue).\n\nsc = us_mainland[us_mainland['state_id'] == 'SC']\nga = us_mainland[us_mainland['state_id'] == 'GA']\n\n# creating figure and axis\nfig, ax = plt.subplots(figsize=(8, 5))\n\n# scatter plot\nax.scatter(sc['lng'], sc['lat'], c='red', label='South Carolina');\nax.scatter(ga['lng'], ga['lat'], c='blue', label='Georgia');\n\n# setting labels and title\nax.set_title('Cities in South Carolina and Georgia');\n\nax.legend(fontsize=12);\n\n\n\n\n\n\n\n\n\nsc = us_mainland[us_mainland['state_id'] == 'SC']\nga = us_mainland[us_mainland['state_id'] == 'GA']\n\n# creating figure and axis\nfig, ax = plt.subplots(figsize=(8, 5))\n\n# scatter plot\nsc_plt = ax.scatter(sc['lng'], sc['lat'], c=sc['density'], cmap='Reds'); \n\n# setting labels and title\nax.set_title('Cities in South Carolina, color proportional to Density');\n\nplt.colorbar(sc_plt, label='Density');",
    "crumbs": [
      "Home",
      "Scatter Plots ($\\geq$ 2 numeric variables)"
    ]
  },
  {
    "objectID": "plotting/scatter.html#marker-shape",
    "href": "plotting/scatter.html#marker-shape",
    "title": "Scatter Plots (\\(\\geq\\) 2 numeric variables)",
    "section": "Marker Shape",
    "text": "Marker Shape\nThe shape of the markers can be adjusted using the marker parameter. The marker parameter accepts a string that specifies the shape of the markers. The default value is marker=‘o’ (circle).\nHere are some of the marker shapes that you can use:\n\n‘o’ - Circle\n‘s’ - Square\n‘^’ - Triangle\n‘v’ - Inverted Triangle\n‘x’ - X\n‘+’ - Plus\n’*’ - Star\n‘D’ - Diamond\n‘d’ - Thin Diamond\n‘p’ - Pentagon\n‘h’ - Hexagon\n‘H’ - Rotated Hexagon\n‘&lt;’ - Left Triangle\n‘&gt;’ - Right Triangle\n\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\nax.scatter(sc['lng'], sc['lat'], label='South Carolina', color='red',  marker='x');\nax.scatter(ga['lng'], ga['lat'], label='Georgia',        color='blue', marker='^');\n\nax.legend(fontsize=12);",
    "crumbs": [
      "Home",
      "Scatter Plots ($\\geq$ 2 numeric variables)"
    ]
  },
  {
    "objectID": "plotting/scatter.html#pandas-and-seaborn",
    "href": "plotting/scatter.html#pandas-and-seaborn",
    "title": "Scatter Plots (\\(\\geq\\) 2 numeric variables)",
    "section": "Pandas and Seaborn",
    "text": "Pandas and Seaborn\nPandas and Seaborn are great libraries for data manipulation and data visualization, respectively. Pandas is used to load and manipulate data, while Seaborn is used to visualize data. Seaborn is built on top of Matplotlib and provides a high-level interface for creating attractive and informative statistical graphics.\nCreating a scatter plot using seaborn and pandas requires fewer lines of code compared to using Matplotlib.\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\nsc.plot(kind='scatter', \\\n        x='lng', \\\n        y='lat', \\\n        s=sc['population']/1000, \\\n        ax=ax, \\\n        color='red', \\\n        label='South Carolina');\n\n\n\n\n\n\n\n\nScatter plots can be created using the scatterplot() function from the Seaborn library. The scatterplot() function takes two arguments: the x-axis values and the y-axis values.\nNote that when using seaborn and pandas, the axis labels are automatically set to the column names of the DataFrame.\n\nimport seaborn as sns\nfig, ax = plt.subplots(figsize=(8, 5))\n\nsns.scatterplot(data=us_mainland, \\\n                x='lng', \\\n                y='lat', \\\n                size='population', \\\n                sizes = (1, 1000), \\\n                hue='state_id', \\\n                ax=ax, \\\n                legend=False);",
    "crumbs": [
      "Home",
      "Scatter Plots ($\\geq$ 2 numeric variables)"
    ]
  },
  {
    "objectID": "plotting/scatter.html#please-dont",
    "href": "plotting/scatter.html#please-dont",
    "title": "Scatter Plots (\\(\\geq\\) 2 numeric variables)",
    "section": "Please don’t",
    "text": "Please don’t\n\nOverplot\nOverplotting occurs when two or more data points are plotted on top of each other. This can make it difficult to see the true distribution of the data. Overplotting can be avoided by adjusting the transparency of the markers using the alpha parameter. The alpha parameter accepts a scalar between 0 and 1. This parameter controls the transparency of the markers. The default value is alpha=1.\n\ndata = pd.read_csv('https://raw.githubusercontent.com/fahadsultan/csc272/refs/heads/main/data/loan_approval_dataset.csv')\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\nax.scatter(data['cibil_score'], data['income_annum']);\n\nax.set_xlabel('Credit Score');\nax.set_ylabel('Annual Income');\n\n\n\n\n\n\n\n\n\n\nUse Scatter for Categorical Data\nScatter plots are used to show the relationship between two numeric variables. If you have categorical data, you should use a different type of plot, such as a bar plot or a box plot.\n\nfig, ax = plt.subplots()\n\nax.scatter(data['loan_term'], data['loan_amount'], alpha=0.5)\n\nax.set_ylabel('Loan Amount');\nax.set_xlabel('Loan Term');",
    "crumbs": [
      "Home",
      "Scatter Plots ($\\geq$ 2 numeric variables)"
    ]
  },
  {
    "objectID": "plotting/basics.html",
    "href": "plotting/basics.html",
    "title": "Visualizing Data",
    "section": "",
    "text": "Visualizing data is a key part of data science. It is not only a way to communicate your findings to others but, more importantly, it is a way to understand your data, models and algorithms better.\nPython has a rich ecosystem of libraries for data science, including for data visualization. The two most commonly used libraries for data visualization in Python are matplotlib and seaborn.\nHere we will look at three ways to visualize data in Python:\nmatplotlib is the primary plotting library in Python. It is a very powerful and highly customizable library that can be used to create a wide variety of plots and graphs. However, despite its power, is can often be not very user-friendly, requiring a lot of code to create even simple plots.\n# importing matplotlib\n\n# `plt` is the standard alias for `matplotlib.pyplot`\n#  similar to `pd` for `pandas`\n\nfrom matplotlib import pyplot as plt\nseaborn is a data visualization library built on top of matplotlib that is easier to use and creates more visually appealing plots. It is designed to work well with pandas DataFrames and can be used to create a wide variety of plots with just a few lines of code. It is particularly useful for creating statistical plots, such as scatter plots, bar plots, and box plots.\nJust as pandas is conventionally imported as pd, matplotlib.pyplot is conventionally imported as plt and seaborn is conventionally imported as sns.\nimport seaborn as sns",
    "crumbs": [
      "Home",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "plotting/basics.html#anscombes-quartet",
    "href": "plotting/basics.html#anscombes-quartet",
    "title": "Visualizing Data",
    "section": "Anscombe’s Quartet",
    "text": "Anscombe’s Quartet\nThere has long been an impression amongst academics and practitioners that “numerical calculations are exact, but graphs are rough”. In 1973, Francis Anscombe set out to counter this common misconception by creating a set of four datasets that are today known as Anscombe’s quartet.\nThe code cell below downloads and loads it as pandas DataFrame this data set:\n\nimport pandas as pd \nanscombe = sns.load_dataset(\"anscombe\")\nanscombe.head()\n\n\n\n\n\n\n\n\ndataset\nx\ny\n\n\n\n\n0\nI\n10.0\n8.04\n\n\n1\nI\n8.0\n6.95\n\n\n2\nI\n13.0\n7.58\n\n\n3\nI\n9.0\n8.81\n\n\n4\nI\n11.0\n8.33\n\n\n\n\n\n\n\nNow let’s see what the summary statistics of x and y features look like, with respect to dataset feature:\n\nanscombe.groupby(\"dataset\").describe()\n\n\n\n\n\n\n\n\nx\ny\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\ndataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI\n11.0\n9.0\n3.316625\n4.0\n6.5\n9.0\n11.5\n14.0\n11.0\n7.500909\n2.031568\n4.26\n6.315\n7.58\n8.57\n10.84\n\n\nII\n11.0\n9.0\n3.316625\n4.0\n6.5\n9.0\n11.5\n14.0\n11.0\n7.500909\n2.031657\n3.10\n6.695\n8.14\n8.95\n9.26\n\n\nIII\n11.0\n9.0\n3.316625\n4.0\n6.5\n9.0\n11.5\n14.0\n11.0\n7.500000\n2.030424\n5.39\n6.250\n7.11\n7.98\n12.74\n\n\nIV\n11.0\n9.0\n3.316625\n8.0\n8.0\n8.0\n8.0\n19.0\n11.0\n7.500909\n2.030579\n5.25\n6.170\n7.04\n8.19\n12.50\n\n\n\n\n\n\n\nNote that for all four unique values of dataset, we have eleven (x, y) values, as seen in count.\nFor each value of dataset, x and y have nearly identical simple descriptive statistics.\nFor all four datasets:\n\n\n\n\n\n\n\n\nProperty\nValue\nAccuracy\n\n\n\n\nMean of x\n9\nexact\n\n\nSample variance of x: s2\n11\nexact\n\n\nMean of y\n7.50\nto 2 decimal places\n\n\nSample variance of y: s2\n4.125\n±0.003\n\n\nCorrelation between x and y\n0.816\nto 3 decimal places\n\n\nLinear regression line\ny = 3.00 + 0.500x\nto 2 and 3 decimal places, respectively\n\n\nCoefficient of determination of the linear regression: \\(R^{2}\\)\n0.67\nto 2 decimal places\n\n\n\n\nNow let’s create a scatter plot of the data using seaborn:\n\nplt.style.use('dark_background')\n\ng = sns.FacetGrid(anscombe, col=\"dataset\");\ng.map(sns.scatterplot, \"x\", \"y\", s=100, color=\"orange\", linewidth=.5, edgecolor=\"black\");\n\n\n\n\n\n\n\n\nAnscombe’s quartet demonstrates both the importance of graphing data before analyzing it and the effect of outliers and other influential observations on statistical properties.",
    "crumbs": [
      "Home",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "plotting/basics.html#choosing-the-right-visualization",
    "href": "plotting/basics.html#choosing-the-right-visualization",
    "title": "Visualizing Data",
    "section": "Choosing the Right Visualization",
    "text": "Choosing the Right Visualization\nData Visualization is arguably the most mistake-prone part of the data science process. It is very easy to create misleading visualizations that lead to incorrect conclusions. It is therefore important to be aware of the common pitfalls and to avoid them.\nThe following is a useful taxonomy for choosing the right visualization depending on your goals for your data:",
    "crumbs": [
      "Home",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "project/project.html",
    "href": "project/project.html",
    "title": "Course Project",
    "section": "",
    "text": "In this course, you are expected to plan, execute, and present the results of a semester-long group project of your choosing.\nTake the time to investigate multiple options during the proposal phase. You should have gotten some preliminary results by then, enough to provide confidence you will be able to successfully complete the project.\n\n\nWorking alone on the project is strongly discouraged. Group sizes should range from two to three students. It is possible for multiple groups working on the same data set. However, such groups must work independently and are not allowed to share code or results. Each dataset leaves enough room to pursue different directions so I expect to see variety among the submissions from each group.\n\n\n\n\nThe steps of your project are to mirror the general data science lifecycle pipeline.\nThe data science lifecycle is a high-level overview of the data science workflow. It’s a cycle of stages that a data scientist should explore as they conduct a thorough analysis of a data-driven problem.\nThere are many variations of the key ideas present in the data science lifecycle. Here, we visualize the stages of the lifecycle using a flow diagram.\n\n\n\n\n\n\nWhether by curiosity or necessity, data scientists will constantly ask questions. For example, in the business world, data scientists may be interested in predicting the profit generated by a certain investment. In the field of medicine, they may ask whether some patients are more likely than others to benefit from a treatment.\nPosing questions is one of the primary ways the data science lifecycle begins. It helps to fully define the question. Here are some things you should ask yourself before framing a question.\n\nWhat do we want to know?\n\nA question that is too ambiguous may lead to confusion.\n\nWhat problems are we trying to solve?\n\nThe goal of asking a question should be clear in order to justify your efforts to stakeholders.\n\nWhat are the hypotheses we want to test?\n\nThis gives a clear perspective from which to analyze final results.\n\nWhat are the metrics for our success?\n\nThis gives a clear point to know when to finish the project.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n| 1. Ask a Question | 2. Obtain Data |\nThe second entry point to the lifecycle is by obtaining data. A careful analysis of any problem requires the use of data. Data may be readily available to us, or we may have to embark on a process to collect it. When doing so, its crucial to ask the following:\n\nWhat data do we have and what data do we need?\n\nDefine the units of the data (people, cities, points in time, etc.) and what features to measure.\n\nHow will we sample more data?\n\nScrape the web, collect manually, etc.\n\nIs our data representative of the population we want to study?\n\nIf our data is not representative of our population of interest, then we can come to incorrect conclusions.\n\n\n\n\n\n\n\n\nProject Phase 1\n\n\n\nIdentify or collect a publicly available data set that you will use for your project. A list of possible datasets and project ideas are provided here but you are allowed to propose your own project and data set. Just be mindful of the scale of the data: it should be large enough to be appropriate for usage in Machine Learning, but not so large that it is unwieldy.\nKey procedures: Data Acquisition, Data Cleaning\n\n\n\n\n\nRaw data itself is not inherently useful. It’s impossible to discern all the patterns and relationships between variables without carefully investigating them. Therefore, translating pure data to actionable insights is a key job of a data scientist. For example, we may choose to ask:\n\nHow is our data organized and what does it contain?\n\nKnowing what the data says about the world helps us better understand the world.\n\nDo we have relevant data?\n\nIf the data we have collected is not useful to the question at hand, then we must collected more data.\n\nWhat are the biases, anomalies, or other issues with the data?\n\nThese can lead to many false conclusions if ignored, so data scientists must always be aware of these issues.\n\nHow do we transform the data to enable effective analysis?\n\nData is not always easy to interpret at first glance, so a data scientist should reveal these hidden insights.\n\n\n\n\n\n\n\n\nProject Phase 2\n\n\n\nPerform some exploratory analysis to help you get acquainted with the data. This may include data visualization and basic preliminary analysis. Identify interesting aspects of the data set that would be useful for downstream prediction: correlations, outliers, missing values, etc.\nKey procedures: Exploratory data analysis, Data visualization.\n\n\n\n\n\nAfter observing the patterns in our data, we can begin answering our question. This may require that we predict a quantity (machine learning), or measure the effect of some treatment (inference).\nFrom here, we may choose to report our results, or possibly conduct more analysis. We may not be satisfied by our findings, or our initial exploration may have brought up new questions that require a new data.\n\nWhat does the data say about the world?\n\nGiven our models, the data will lead us to certain conclusions about the real world.\n\n\nDoes it answer our questions or accurately solve the problem?\n\nIf our model and data can not accomplish our goals, then we must reform our question, model, or both.\n\n\nHow robust are our conclusions and can we trust the predictions?\n\nInaccurate models can lead to untrue conclusions.\n\n\n\n\n\n\n\n\nProject Phase 3\n\n\n\nPreprocess data to change raw feature vectors into a representation that is more suitable for the downstream analysis. This may include data cleaning, calculating derivative or second-order variables, feature extraction, and feature selection.\nImplement baseline models covered in class and report their performance.\nIdentify, implement and apply as many models as relevant from class to predict some aspect of the data. This must be a supervised learning model. This may include, model selection, and model evaluation.\nYou must compare your results to a number of baselines, including random predictor, major class classifier and/or autocorrelation model. An example table is shown below:\n\n\n\nModel\nAccuracy\nPrecision\nRecall\nF1-score\n\n\n\n\nRandom baseline\n0.5\n0.52\n0.55\n0.53\n\n\nMajority class / Autocorrelation\n0.75\n0.5\n1\n0.66\n\n\nModel 1\n0.77\n0.72\n0.74\n0.73\n\n\nModel 2\n0.85\n0.79\n0.89\n0.88\n\n\n\nDiscuss the results of your analysis. This must include stating the assumptions of the model, the limitations of the model, a thorough error analysis and future directions.\nKey procedures: Model Creation, Prediction, Inference, Model Selection, Error Analysis."
  },
  {
    "objectID": "project/project.html#groups",
    "href": "project/project.html#groups",
    "title": "Course Project",
    "section": "",
    "text": "Working alone on the project is strongly discouraged. Group sizes should range from two to three students. It is possible for multiple groups working on the same data set. However, such groups must work independently and are not allowed to share code or results. Each dataset leaves enough room to pursue different directions so I expect to see variety among the submissions from each group."
  },
  {
    "objectID": "project/project.html#data-science-lifecycle",
    "href": "project/project.html#data-science-lifecycle",
    "title": "Course Project",
    "section": "",
    "text": "The steps of your project are to mirror the general data science lifecycle pipeline.\nThe data science lifecycle is a high-level overview of the data science workflow. It’s a cycle of stages that a data scientist should explore as they conduct a thorough analysis of a data-driven problem.\nThere are many variations of the key ideas present in the data science lifecycle. Here, we visualize the stages of the lifecycle using a flow diagram."
  },
  {
    "objectID": "project/project.html#ask-a-question-and-obtain-data",
    "href": "project/project.html#ask-a-question-and-obtain-data",
    "title": "Course Project",
    "section": "",
    "text": "Whether by curiosity or necessity, data scientists will constantly ask questions. For example, in the business world, data scientists may be interested in predicting the profit generated by a certain investment. In the field of medicine, they may ask whether some patients are more likely than others to benefit from a treatment.\nPosing questions is one of the primary ways the data science lifecycle begins. It helps to fully define the question. Here are some things you should ask yourself before framing a question.\n\nWhat do we want to know?\n\nA question that is too ambiguous may lead to confusion.\n\nWhat problems are we trying to solve?\n\nThe goal of asking a question should be clear in order to justify your efforts to stakeholders.\n\nWhat are the hypotheses we want to test?\n\nThis gives a clear perspective from which to analyze final results.\n\nWhat are the metrics for our success?\n\nThis gives a clear point to know when to finish the project.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n| 1. Ask a Question | 2. Obtain Data |\nThe second entry point to the lifecycle is by obtaining data. A careful analysis of any problem requires the use of data. Data may be readily available to us, or we may have to embark on a process to collect it. When doing so, its crucial to ask the following:\n\nWhat data do we have and what data do we need?\n\nDefine the units of the data (people, cities, points in time, etc.) and what features to measure.\n\nHow will we sample more data?\n\nScrape the web, collect manually, etc.\n\nIs our data representative of the population we want to study?\n\nIf our data is not representative of our population of interest, then we can come to incorrect conclusions.\n\n\n\n\n\n\n\n\nProject Phase 1\n\n\n\nIdentify or collect a publicly available data set that you will use for your project. A list of possible datasets and project ideas are provided here but you are allowed to propose your own project and data set. Just be mindful of the scale of the data: it should be large enough to be appropriate for usage in Machine Learning, but not so large that it is unwieldy.\nKey procedures: Data Acquisition, Data Cleaning"
  },
  {
    "objectID": "project/project.html#understand-the-data",
    "href": "project/project.html#understand-the-data",
    "title": "Course Project",
    "section": "",
    "text": "Raw data itself is not inherently useful. It’s impossible to discern all the patterns and relationships between variables without carefully investigating them. Therefore, translating pure data to actionable insights is a key job of a data scientist. For example, we may choose to ask:\n\nHow is our data organized and what does it contain?\n\nKnowing what the data says about the world helps us better understand the world.\n\nDo we have relevant data?\n\nIf the data we have collected is not useful to the question at hand, then we must collected more data.\n\nWhat are the biases, anomalies, or other issues with the data?\n\nThese can lead to many false conclusions if ignored, so data scientists must always be aware of these issues.\n\nHow do we transform the data to enable effective analysis?\n\nData is not always easy to interpret at first glance, so a data scientist should reveal these hidden insights.\n\n\n\n\n\n\n\n\nProject Phase 2\n\n\n\nPerform some exploratory analysis to help you get acquainted with the data. This may include data visualization and basic preliminary analysis. Identify interesting aspects of the data set that would be useful for downstream prediction: correlations, outliers, missing values, etc.\nKey procedures: Exploratory data analysis, Data visualization."
  },
  {
    "objectID": "project/project.html#understand-the-world",
    "href": "project/project.html#understand-the-world",
    "title": "Course Project",
    "section": "",
    "text": "After observing the patterns in our data, we can begin answering our question. This may require that we predict a quantity (machine learning), or measure the effect of some treatment (inference).\nFrom here, we may choose to report our results, or possibly conduct more analysis. We may not be satisfied by our findings, or our initial exploration may have brought up new questions that require a new data.\n\nWhat does the data say about the world?\n\nGiven our models, the data will lead us to certain conclusions about the real world.\n\n\nDoes it answer our questions or accurately solve the problem?\n\nIf our model and data can not accomplish our goals, then we must reform our question, model, or both.\n\n\nHow robust are our conclusions and can we trust the predictions?\n\nInaccurate models can lead to untrue conclusions.\n\n\n\n\n\n\n\n\nProject Phase 3\n\n\n\nPreprocess data to change raw feature vectors into a representation that is more suitable for the downstream analysis. This may include data cleaning, calculating derivative or second-order variables, feature extraction, and feature selection.\nImplement baseline models covered in class and report their performance.\nIdentify, implement and apply as many models as relevant from class to predict some aspect of the data. This must be a supervised learning model. This may include, model selection, and model evaluation.\nYou must compare your results to a number of baselines, including random predictor, major class classifier and/or autocorrelation model. An example table is shown below:\n\n\n\nModel\nAccuracy\nPrecision\nRecall\nF1-score\n\n\n\n\nRandom baseline\n0.5\n0.52\n0.55\n0.53\n\n\nMajority class / Autocorrelation\n0.75\n0.5\n1\n0.66\n\n\nModel 1\n0.77\n0.72\n0.74\n0.73\n\n\nModel 2\n0.85\n0.79\n0.89\n0.88\n\n\n\nDiscuss the results of your analysis. This must include stating the assumptions of the model, the limitations of the model, a thorough error analysis and future directions.\nKey procedures: Model Creation, Prediction, Inference, Model Selection, Error Analysis."
  },
  {
    "objectID": "syllabus/textbook.html",
    "href": "syllabus/textbook.html",
    "title": "Textbooks & Other Resources",
    "section": "",
    "text": "Caution\n\n\n\nPlease note that the following textbooks are NOT strictly required for this course, but they are strongly recommended for those who prefer to have a physical reference.",
    "crumbs": [
      "Textbooks & Other Resources"
    ]
  },
  {
    "objectID": "syllabus/textbook.html#other-resources",
    "href": "syllabus/textbook.html#other-resources",
    "title": "Textbooks & Other Resources",
    "section": "Other Resources",
    "text": "Other Resources\n\nMining of Massive Datasets by Jure Leskovec, Anand Rajaraman, Jeffrey D. Ullman\nProbabilistic Programming & Bayesian Methods for Hackers by Cameron Davidson-Pilon\nIntroduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani\n\n\n\n\nJupyter Notebook User Guide\nPython Data Science Handbook by Jake VanderPlas\nPython: How to Think Like a Computer Scientist (Swarthmore Edition) by Jeffrey Elkner, Allen B. Downey, and Chris Meyers (free, open textbook)",
    "crumbs": [
      "Textbooks & Other Resources"
    ]
  },
  {
    "objectID": "syllabus/mental_health.html",
    "href": "syllabus/mental_health.html",
    "title": "Mental Health Resources",
    "section": "",
    "text": "Empowering and equipping students to manage their mental health and academic success, the Counseling Center’s stepped care model offers an array of evidence based services.\nThe resources listed below are free, confidential and accessible to all enrolled students. Go to the Counseling Center Website for details.\n\n\nFurman University Counseling Center Mental Health and Crisis Support Line – Call the Counseling Center at 864-294-3031, press #3 (confidential, available 24/7/365 from anywhere).\n\n\n\n\nHeadspace – a mindfulness app that helps decrease stress and improve focus and mind-wandering, sponsored by SGA and PHOKUS. Students may enroll using their Furman email.\nTAO Connect – a self-help platform (anonymous and confidential, 24/7) sponsored by the Counseling Center and accessible to students, faculty and staff. Enroll with a Furman email.\n\n\n\n\n\nPaladin Peer Support is a student peer mentoring organization focused on wellness and self-efficacy. Follow them on Instagram and connect for support in reaching personal well-being goals. ### Skill Building Groups and Workshops\nRotating evidence-based psycho-education and skill building groups for anxiety and emotional regulation ### Consultation and Treatment Services\nStart Strong and Finish Strong Walk-in Clinics (first and last two weeks of every semester)\nBrief individual counseling (in person and online), which may include psychiatric and nutrition consults where clinically indicated.\nSingle Session Consultations\nGroup Counseling and Skill Building Workshops\n\n\n\n\n\n\nThe Office for Spiritual Life provides individual confidential counseling for students, faculty and staff in person and online\nGroups and workshops that are theme-focused and interpersonal\nContact OSL@furman.edu, 864-294-2133, or contact a chaplain directly: vaughn.crowetipton@furman.edu, kate.taber@furman.edu.",
    "crumbs": [
      "Mental Health Resources"
    ]
  },
  {
    "objectID": "syllabus/mental_health.html#the-counseling-center",
    "href": "syllabus/mental_health.html#the-counseling-center",
    "title": "Mental Health Resources",
    "section": "",
    "text": "Empowering and equipping students to manage their mental health and academic success, the Counseling Center’s stepped care model offers an array of evidence based services.\nThe resources listed below are free, confidential and accessible to all enrolled students. Go to the Counseling Center Website for details.\n\n\nFurman University Counseling Center Mental Health and Crisis Support Line – Call the Counseling Center at 864-294-3031, press #3 (confidential, available 24/7/365 from anywhere).\n\n\n\n\nHeadspace – a mindfulness app that helps decrease stress and improve focus and mind-wandering, sponsored by SGA and PHOKUS. Students may enroll using their Furman email.\nTAO Connect – a self-help platform (anonymous and confidential, 24/7) sponsored by the Counseling Center and accessible to students, faculty and staff. Enroll with a Furman email.\n\n\n\n\n\nPaladin Peer Support is a student peer mentoring organization focused on wellness and self-efficacy. Follow them on Instagram and connect for support in reaching personal well-being goals. ### Skill Building Groups and Workshops\nRotating evidence-based psycho-education and skill building groups for anxiety and emotional regulation ### Consultation and Treatment Services\nStart Strong and Finish Strong Walk-in Clinics (first and last two weeks of every semester)\nBrief individual counseling (in person and online), which may include psychiatric and nutrition consults where clinically indicated.\nSingle Session Consultations\nGroup Counseling and Skill Building Workshops",
    "crumbs": [
      "Mental Health Resources"
    ]
  },
  {
    "objectID": "syllabus/mental_health.html#spiritual-life",
    "href": "syllabus/mental_health.html#spiritual-life",
    "title": "Mental Health Resources",
    "section": "",
    "text": "The Office for Spiritual Life provides individual confidential counseling for students, faculty and staff in person and online\nGroups and workshops that are theme-focused and interpersonal\nContact OSL@furman.edu, 864-294-2133, or contact a chaplain directly: vaughn.crowetipton@furman.edu, kate.taber@furman.edu.",
    "crumbs": [
      "Mental Health Resources"
    ]
  },
  {
    "objectID": "syllabus/appointment.html",
    "href": "syllabus/appointment.html",
    "title": "Appointment",
    "section": "",
    "text": "Appointment\nI don’t have fixed office hours this semester. Instead, I am using an appointment scheduling system Calendly to make it easier for you to find a time that works for you.\nYou can schedule an appointment with me using using this link\n\nI am also ofcourse available via email."
  },
  {
    "objectID": "syllabus/github.html",
    "href": "syllabus/github.html",
    "title": "Github Classroom",
    "section": "",
    "text": "Link to CSC-223 Github Classroom"
  },
  {
    "objectID": "encoding/encoding.html",
    "href": "encoding/encoding.html",
    "title": "Encoding",
    "section": "",
    "text": "Data exists in many forms, formats and modalities. Even missing data is a form of data.\nRegardless of its form, format or modality, all data ultimately needs to be transformed into a matrix (or matrices) of numbers for it to be used for any machine learning or data mining algorithm.\nThe process of transforming raw data into numeric matrices is often referred to as Encoding and the resulting numeric matrices are referred to as Representations. These representations may focus on certain aspects (structure, content, semantics etc.) of the data more than others and may be more or less suitable for certain tasks.\n\n\n\n\nModalities of Data\nIn this section, we will discuss the most common modalities of data and how to work with them in Python.\nThere are too many data modalities to eumerate an exhaustive list. Each data modality has its own unique characteristics and often require specialized methods to process and analyze. Study and analysis of many data modalities is an area of research within itself. For example, Computer Vision is the area of research that deals with images and videos. Speech processing is the area of research that deals with sounds. Natural Language Processing (NLP) is the area of research that deals with text.\nSome common modalities and their associated areas of research are:\n\nText: Natural Language Processing, Computational Linguistics\nImages: Computer Vision, Digital Image Processing\nSounds: Digital Signal Processing (DSP)\nGraphs: Graph Theory, Network Theory\nTime Series: Time Series Analysis\nGeographic: Geographic Information Systems (GIS), Spatial Computing.",
    "crumbs": [
      "Home",
      "Encoding"
    ]
  },
  {
    "objectID": "encoding/time.html",
    "href": "encoding/time.html",
    "title": "Time Series Data",
    "section": "",
    "text": "Time series data is data that is collected over time. Time series data is often represented as a sequence of numbers. The numbers are usually collected at regular intervals. For example, the stock price of a company is collected every day at the close of the stock market.\n\nurl = \"https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv\"\ndata = pd.read_csv(url)\nwins = data[data['Result']=='win'].sort_values('Year')\nlosses = data[data['Result']=='loss'].sort_values('Year')\nplt.plot(wins['Year'], wins['Popular vote']);\nplt.plot(losses['Year'], losses['Popular vote']);\nplt.legend(['Wins', 'Losses']);\nplt.title('Popular vote in US presidential elections');",
    "crumbs": [
      "Home",
      "Time Series Data"
    ]
  },
  {
    "objectID": "encoding/text.html",
    "href": "encoding/text.html",
    "title": "Text Data",
    "section": "",
    "text": "Text is arguably the most ubiquitous non-numeric modality of data. Most of the data on the internet is text. Text data generally exists as a collection of documents called corpus, where each document is one or more sentences.\n\nBag of Words\n\n\n\n\nText data can be encoded into a number of different numeric representations. The most common is the Bag-of-Words (BoW) representation, which is closely related with One-Hot Encoding.\nIn the BoW representation, each row represents a document and each column represents a word in the vocabulary. The vocabulary is the list of all unique words in the corpus. The corpus is the collection of all the documents. A document is a sequence of words. The value in the cell at \\(i\\)th row and \\(j\\)th column represents the number of times the word \\(j\\) appears in document \\(i\\).\nCreating a Bag of Words representation for a corpus generally entails the following steps:\n\nTokenization: Split each document into a sequence of tokens (words).\nCreate Vocabulary: Create a list of all unique tokens (words) for all documents in the corpus. Often words are normalized by converting all words to lowercase and removing punctuation.\nCreate Document Vectors: Create a vector for each document in the corpus. The vector is the same length as the vocabulary. The value in each cell of the vector is the number of times the word in the corresponding column appears in the document.\nCreate Document-Term Matrix: Create a 2D array where each row represents a document and each column represents a word in the vocabulary. The value in the cell at \\(i\\)th row and \\(j\\)th column represents the number of times the word \\(j\\) appears in document \\(i\\).\n\nThe image below shows a bag-of-words representation of a corpus of two documents. The vocabulary is the list of words on the left. The corpus is the 2D array of numbers on the right.\n\nimport pandas as pd \n\ndata = pd.read_csv('https://raw.githubusercontent.com/fahadsultan/csc272/main/data/chat_dataset.csv')\n\ndata.head()\n\n\n\n\n\n\n\n\nmessage\nsentiment\n\n\n\n\n0\nI really enjoyed the movie\npositive\n\n\n1\nThe food was terrible\nnegative\n\n\n2\nI'm not sure how I feel about this\nneutral\n\n\n3\nThe service was excellent\npositive\n\n\n4\nI had a bad experience\nnegative\n\n\n\n\n\n\n\n\n# 1. Tokenization: Concatenate all messages into a single string and convert to lowercase and then split into words\ntokens = (' '.join(data['message'].values)).lower().split()\n\n# 2. Vocabulary: Create a list of UNIQUE words in the dataset\nvocab = list(set(tokens))\n\n# 4. Create an empty DataFrame with columns as the words in the vocab\nbow = pd.DataFrame(columns=vocab)\n\n# Go through each word in the vocab\nfor word in vocab: \n\n    # 3. For each message, count the number of times the word appears\n    bow[word] = data['message'].apply(lambda msg: msg.count(word))\n\nbow.head()\n\n\n\n\n\n\n\n\ni\nreally\nenjoyed\nthe\nmovie\nthe\nfood\nwas\nterrible\ni'm\n...\nex\nis\ndating\nsomeone\nnew.\ni\nfeel\nso\nheartbroken\n💔😢\n\n\n\n\n0\n1\n1\n1\n1\n1\n1\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n1\n1\n0\n0\n0\n0\n0\n1\n1\n1\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n2\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n\n\n3\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n...\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n4\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n\n\n5 rows × 4291 columns",
    "crumbs": [
      "Home",
      "Text Data"
    ]
  },
  {
    "objectID": "encoding/images.html",
    "href": "encoding/images.html",
    "title": "Image Data",
    "section": "",
    "text": "An image is a 2D array of pixels. Each pixel is a 3D vector of red, green, and blue (RGB) values. The RGB values are usually represented as integers between 0 and 255. The RGB values are used to represent the color of the pixel. For example, a pixel with RGB values of (255, 0, 0) is red, (0, 255, 0) is green, and (0, 0, 255) is blue. A pixel with RGB values of (0, 0, 0) is black and (255, 255, 255) is white.\nIn this notebook, we will learn how to read and write images, and how to manipulate them.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd \n\nplt.style.use('dark_background')\n\n# Read an image\n!wget https://raw.githubusercontent.com/fahadsultan/csc272/main/data/belltower.png\nimg = plt.imread('belltower.png');\nplt.imshow(img);\n\n--2024-10-17 08:17:16--  https://raw.githubusercontent.com/fahadsultan/csc272/main/data/belltower.png\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3604223 (3.4M) [image/png]\nSaving to: 'belltower.png'\n\nbelltower.png       100%[===================&gt;]   3.44M  --.-KB/s    in 0.1s    \n\n2024-10-17 08:17:17 (29.1 MB/s) - 'belltower.png' saved [3604223/3604223]\n\n\n\n\n\n\n\n\n\n\n\nimg.shape # (height, width, channels)\n\n(1600, 1267, 4)\n\n\n\nimg[0,0,:] # RGB values of the first pixel\n\narray([0.9411765 , 0.94509804, 0.9529412 , 1.        ], dtype=float32)\n\n\n\n# Convert to grayscale\nimg_gray = img.mean(axis=2)\nimg_df   = pd.DataFrame(img_gray)\n\n\nimg_df.shape\n\n(1600, 1267)\n\n\n\n# Convert to grayscale\nimg_gray = img.mean(axis=2)\nimg_df   = pd.DataFrame(img_gray)\n\nplt.imshow(img_df, cmap='gray');\nplt.colorbar();\n\n\n\n\n\n\n\n\n\nplt.imshow(256-img_df, cmap='gray'); \nplt.colorbar();\n\n\n\n\n\n\n\n\n\n# Threshold the image\nimg_thresh = img_gray &gt; 0.5 \nplt.imshow(img_thresh, cmap='gray');\n\n\n\n\n\n\n\n\n\n## Crop the image\nimg_crop = img_df.iloc[:700, :700]\nplt.imshow(img_crop, cmap='gray');\n\n\n\n\n\n\n\n\n\n# Rotate the image\nimg_rot = img_df.transpose()\nplt.imshow(img_rot, cmap='gray');\n\n\n\n\n\n\n\n\n\nVideos\nVideos are a sequence of images. Within the context of videos, each image is called a frame. Most videos are a sequence of 24-30 frames per second.\nMost modern videos are encoded using a variety of different codecs. A codec is a method of encoding and decoding a video. Some common codecs are H.264, MPEG-4, and VP9.",
    "crumbs": [
      "Home",
      "Image Data"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Syed Fahad Sultan سید فہد سلطان \nPronunciation: Saiyyudh Fahad Sool-tahn\nJust call me “Dr. Sultan” (click on the speaker for a short audio clip: 🔈)\n\n\n\n\n\nI am originally from Lahore, Pakistan and joined Furman University in Fall 2022 after earning my Ph.D. in Computer Science from State University of New York at Stony Brook.\n\n\n\n\n\nFresh out of college, I worked as a professional video game developer for a startup that later got acquired by the Japanese gaming giant DeNA. During this time, I was part of the team that built TapFish, the top-grossing game worldwide, for two weeks in 2011, on both the App Store and Google Play.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo games development\n\n\n\nUrban Sensing\n\n\n\nComputational Neuroscience\n\n\n\n\n\n\n\n\nI then went on to work at Technology for People Initiative, an applied research lab in my where I mined social media and cell phone data for proxies of socio-economic indicators that allowed more inclusive policy-making for marginalized communities. During these years, I also dabbled in data journalism and helped organize a boot camp on using data for journalists with the support of the International Center for Journalists (ICFJ) and the Knight Foundation.\n\nIn 2015, I moved to Mecca, Saudi Arabia to work for the GIS Innovation Center (now Wadi Makkah). There I worked on innovative urban sensing techniques for better crowd control during the annual pilgrimage to the city, the largest human gathering in the world every year.\nDuring my PhD, I worked at the intersection of computational neuroscience, bioinformatics and machine learning. My work focused on identifying neurological and genetic biomarkers linking type-2 diabetes with cognitive disorders such as Alzheimer’s and other dementias.\n\n\nOffice: Riley Hall 200-H\nEmail: fahad.sultan@furman.edu\n\nI don’t hold fixed office hours, but I’m available for on-demand meetings in real time. If you’d like to guarantee a time, you can schedule a meeting using this link. https://calendly.com/ssultan-dpq/15-minute-meeting.\n\n\n\n\n\n\n\nCourse website: https://fahadsultan.com/csc272\nThe Syllabus is available on the course website. In particular, please make sure to read the Grading, Academic Integrity and Textbook and other Resources sections carefully.\nAll of the course content will be posted on this website.\nImportant announcements will be made on both the course website homepage and in class.\nYou are to submit assignments and exams on the course Moodle page. I will also upload all of your grades there.\n\n\n\nDeclarative knowledge is knowledge about facts. It is knowledge that answers the “What is” questions. Most courses outside Computer Science are about declarative knowledge.\nIn contrast, Imperative knowledge is knowledge about how to do things. It is knowledge that answers the “How to” questions.\nWhile we will spend a non-trivial amount of time in this course on declarative knowledge, the overwhelming majority of this course will focus on imperative knowledge. Your grade in this course will be determined by your ability to apply declarative and more importantly imperative knowledge to solve problems.\n\nResearch shows that there is only one way to acquire imperative knowledge: Practice, Practice, Practice !. Practice combined with feedback is the only way to achieve mastery.\nIn this course, you will be given ample opportunities to practice along with regular feedback.\n\n\n\nApproach assignments purely as opportunities to learn, prepare for exams and to prepare for your career.\n\nIt is not worth cheating on assignments. Just come talk to me if you are struggling with an assignment. I will literally just tell you the answer.\nYou can schedule a time to get your assignments graded using this link.\nWritten Assignments:\nWritten assignments are to help you build a deeper understanding of algorithms and math covered in class.\nThese could simply be math problems or involve tracing algorithms and dry-runs.\nBoth handwritten or typed submissions are acceptable. Submissions, as always, on Moodle.\nProgramming Assignments:\nProgramming assignments are going to be posted at the start of the lab session each week and will be due in 10 days, unless otherwise specified.\nAll Programming assignments will be graded through an in-person code review. You are to give a walkthrough of your code and be able to answer questions about it.\nDuring these code review, you will be given feedback on how to improve your code and avoid common mistakes.\nYou should expect questions in the exams similar to assignments.\n\n\n\n\nThis course encourages you to explore the use of generative artificial intelligence (GAI) tools such as ChatGPT for all assignments and assessments. GAI is a subset of AI that utilizes machine learning models to create new, original content, such as images, text, or music, based on patterns and structures learned from existing data. As is indicated in the University academic integrity policy, use of GAI must be appropriately acknowledged and cited as you would any other source. By submitting assignments in this class, you pledge to affirm that they are your own work and you attribute use of any tools and sources. Violations of this policy will be considered academic misconduct.\nIt is your responsibility to assess the validity and applicability of any GAI output that is submitted; you bear the final responsibility. You should be aware that all GAI tools have a tendency to make up incorrect facts and fake citations or confidently reassert factual errors, that code generation models have a tendency to produce inaccurate outputs, and that image generation models can occasionally come up with highly offensive products. You will be responsible for any inaccurate, biased, offensive, or otherwise unethical content you submit regardless of whether it originally comes from you or a GAI tool. Please also note that different classes at Furman may implement different GAI policies, and it is your responsibility to conform to expectations for each course.\n\n\nGiven the glut of information accessible online and otherwise in this day and age, meaningful interactions with your peers and teachers is essentially why you are paying your college tuition.\nTo encourage this, I have allocated 5% of your course grade to class participation. Ways to earn class participation points include:\n\nComing to class and labs regularly\nAsking questions during class\nSharing your thoughts and comments during class discussions\n(Optional) Answering questions during class\n\nClass participation is somewhat subjective, but I will do my best to be as fair as possible. I will share your overall class participation points with you with each graded exam.\n\n\n\nThere will be three exams in the course, including the final. The final exam will be cumulative. Exams constitute 60% of your course grade.\nAll exams will be on paper and closed-book. Exam 3 will be on the last day of class.\nThe finals week will be used for project presentations.\n\n\n\nEverything is tentative and subject to change\n\nAny and all feedback is welcome!\nI will an anonymous feedback poll on Moodle. Please use this to anonymously share any feedback.\nShare any changes you want me to make in the course, at any point in the semester. You can submit multiple times over the span of the semester.\nThink of it as a Complaints Box for the course.\n\n\n\n\n“Data Mining” is a term from the 1990s, back when it was an exciting and popular new field. Around 2010, people instead started to speak of “big data”. Today, the popular term is “data science”. There are some who even regard data mining as synonymous with machine learning. There is no question that some data mining appropriately uses algorithms from machine learning. However, during all this time, the concept remained the same: use the most powerful hardware, the most powerful programming systems, and the most efficient algorithms to solve problems in science, commerce, healthcare, government, the humanities, and many other fields of human endeavor.\n\n\n\nFrom the Venn Diagram, the course content is going to cover ✅ Hacking Skills and ✅ Math & Statistics in detail but not ☐ Substantive Expertise. For that missing piece, I strongly encourage you to bring in knowledge from your GERs and other Non-CS department courses into this class and the term project in particular. Nothing would make me happier than to see projects that combines CS with your other interests.\n\n\n\n“But wait, I am not a Math Person!” you say!\nThere is no such thing as a “Math Person”. I do recognize, however, that Math Anxiety is a real thing and is very common. It is a feeling of fear based on a belief that one is not good at math or that math is inherently difficult.\nPlease use this course as an opportunity to overcome your Math anxiety!\nIn this course, the code you write will be mostly math. Most modern “AI” is just that: math, in code.\nThis presents a unique opportunity for you to overcome your Math anxiety. You will be able to see the math in action, be able to visualize the results and have a conversation with it.\nTrust me, there is a tremendous amount of beauty and joy to be found in mathematics. And if beauty and joy aren’t really your thing, then let me also assure you there is a lot of money to be made these days by being good at coding math. Either way, the rewards are well worth the effort!\n\n\n\n\nPandas is a powerful Python library that is widely used in data science and data analysis. It provides data structures and functions that make working with tabular data easy and intuitive.\nIt is generally accepted in the data science community as the industry- and academia-standard tool for manipulating tabular data.\n\n\n\n\nJupyter Notebooks are an open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text.\nThey are widely used in data science, scientific computing, and machine learning for data cleaning and transformation, numerical simulation, statistical modeling, data visualization, and much more.\n\n\n\n\n\nGoogle Colab is a free, cloud-based platform that allows you to write and execute Python code in your web browser. It provides a Jupyter notebook environment with access to powerful computing resources, including GPUs and TPUs.\nNote that code running on Google Colab is on a remote server, not on your local machine. This means that any files you want to use in your code must be uploaded to the Colab environment or accessed from a cloud storage service like Google Drive.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#about-the-course",
    "href": "intro.html#about-the-course",
    "title": "Introduction",
    "section": "",
    "text": "Course website: https://fahadsultan.com/csc272\nThe Syllabus is available on the course website. In particular, please make sure to read the Grading, Academic Integrity and Textbook and other Resources sections carefully.\nAll of the course content will be posted on this website.\nImportant announcements will be made on both the course website homepage and in class.\nYou are to submit assignments and exams on the course Moodle page. I will also upload all of your grades there.\n\n\n\nDeclarative knowledge is knowledge about facts. It is knowledge that answers the “What is” questions. Most courses outside Computer Science are about declarative knowledge.\nIn contrast, Imperative knowledge is knowledge about how to do things. It is knowledge that answers the “How to” questions.\nWhile we will spend a non-trivial amount of time in this course on declarative knowledge, the overwhelming majority of this course will focus on imperative knowledge. Your grade in this course will be determined by your ability to apply declarative and more importantly imperative knowledge to solve problems.\n\nResearch shows that there is only one way to acquire imperative knowledge: Practice, Practice, Practice !. Practice combined with feedback is the only way to achieve mastery.\nIn this course, you will be given ample opportunities to practice along with regular feedback.\n\n\n\nApproach assignments purely as opportunities to learn, prepare for exams and to prepare for your career.\n\nIt is not worth cheating on assignments. Just come talk to me if you are struggling with an assignment. I will literally just tell you the answer.\nYou can schedule a time to get your assignments graded using this link.\nWritten Assignments:\nWritten assignments are to help you build a deeper understanding of algorithms and math covered in class.\nThese could simply be math problems or involve tracing algorithms and dry-runs.\nBoth handwritten or typed submissions are acceptable. Submissions, as always, on Moodle.\nProgramming Assignments:\nProgramming assignments are going to be posted at the start of the lab session each week and will be due in 10 days, unless otherwise specified.\nAll Programming assignments will be graded through an in-person code review. You are to give a walkthrough of your code and be able to answer questions about it.\nDuring these code review, you will be given feedback on how to improve your code and avoid common mistakes.\nYou should expect questions in the exams similar to assignments.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#ai-policy",
    "href": "intro.html#ai-policy",
    "title": "Introduction",
    "section": "",
    "text": "This course encourages you to explore the use of generative artificial intelligence (GAI) tools such as ChatGPT for all assignments and assessments. GAI is a subset of AI that utilizes machine learning models to create new, original content, such as images, text, or music, based on patterns and structures learned from existing data. As is indicated in the University academic integrity policy, use of GAI must be appropriately acknowledged and cited as you would any other source. By submitting assignments in this class, you pledge to affirm that they are your own work and you attribute use of any tools and sources. Violations of this policy will be considered academic misconduct.\nIt is your responsibility to assess the validity and applicability of any GAI output that is submitted; you bear the final responsibility. You should be aware that all GAI tools have a tendency to make up incorrect facts and fake citations or confidently reassert factual errors, that code generation models have a tendency to produce inaccurate outputs, and that image generation models can occasionally come up with highly offensive products. You will be responsible for any inaccurate, biased, offensive, or otherwise unethical content you submit regardless of whether it originally comes from you or a GAI tool. Please also note that different classes at Furman may implement different GAI policies, and it is your responsibility to conform to expectations for each course.\n\n\nGiven the glut of information accessible online and otherwise in this day and age, meaningful interactions with your peers and teachers is essentially why you are paying your college tuition.\nTo encourage this, I have allocated 5% of your course grade to class participation. Ways to earn class participation points include:\n\nComing to class and labs regularly\nAsking questions during class\nSharing your thoughts and comments during class discussions\n(Optional) Answering questions during class\n\nClass participation is somewhat subjective, but I will do my best to be as fair as possible. I will share your overall class participation points with you with each graded exam.\n\n\n\nThere will be three exams in the course, including the final. The final exam will be cumulative. Exams constitute 60% of your course grade.\nAll exams will be on paper and closed-book. Exam 3 will be on the last day of class.\nThe finals week will be used for project presentations.\n\n\n\nEverything is tentative and subject to change\n\nAny and all feedback is welcome!\nI will an anonymous feedback poll on Moodle. Please use this to anonymously share any feedback.\nShare any changes you want me to make in the course, at any point in the semester. You can submit multiple times over the span of the semester.\nThink of it as a Complaints Box for the course.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#what-is-data-mining",
    "href": "intro.html#what-is-data-mining",
    "title": "Introduction",
    "section": "",
    "text": "“Data Mining” is a term from the 1990s, back when it was an exciting and popular new field. Around 2010, people instead started to speak of “big data”. Today, the popular term is “data science”. There are some who even regard data mining as synonymous with machine learning. There is no question that some data mining appropriately uses algorithms from machine learning. However, during all this time, the concept remained the same: use the most powerful hardware, the most powerful programming systems, and the most efficient algorithms to solve problems in science, commerce, healthcare, government, the humanities, and many other fields of human endeavor.\n\n\n\nFrom the Venn Diagram, the course content is going to cover ✅ Hacking Skills and ✅ Math & Statistics in detail but not ☐ Substantive Expertise. For that missing piece, I strongly encourage you to bring in knowledge from your GERs and other Non-CS department courses into this class and the term project in particular. Nothing would make me happier than to see projects that combines CS with your other interests.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#expect-lots-of-programming-and-lots-of-math",
    "href": "intro.html#expect-lots-of-programming-and-lots-of-math",
    "title": "Introduction",
    "section": "",
    "text": "“But wait, I am not a Math Person!” you say!\nThere is no such thing as a “Math Person”. I do recognize, however, that Math Anxiety is a real thing and is very common. It is a feeling of fear based on a belief that one is not good at math or that math is inherently difficult.\nPlease use this course as an opportunity to overcome your Math anxiety!\nIn this course, the code you write will be mostly math. Most modern “AI” is just that: math, in code.\nThis presents a unique opportunity for you to overcome your Math anxiety. You will be able to see the math in action, be able to visualize the results and have a conversation with it.\nTrust me, there is a tremendous amount of beauty and joy to be found in mathematics. And if beauty and joy aren’t really your thing, then let me also assure you there is a lot of money to be made these days by being good at coding math. Either way, the rewards are well worth the effort!",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#pandas",
    "href": "intro.html#pandas",
    "title": "Introduction",
    "section": "",
    "text": "Pandas is a powerful Python library that is widely used in data science and data analysis. It provides data structures and functions that make working with tabular data easy and intuitive.\nIt is generally accepted in the data science community as the industry- and academia-standard tool for manipulating tabular data.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#jupyter-notebooks",
    "href": "intro.html#jupyter-notebooks",
    "title": "Introduction",
    "section": "",
    "text": "Jupyter Notebooks are an open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text.\nThey are widely used in data science, scientific computing, and machine learning for data cleaning and transformation, numerical simulation, statistical modeling, data visualization, and much more.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#google-colab",
    "href": "intro.html#google-colab",
    "title": "Introduction",
    "section": "",
    "text": "Google Colab is a free, cloud-based platform that allows you to write and execute Python code in your web browser. It provides a Jupyter notebook environment with access to powerful computing resources, including GPUs and TPUs.\nNote that code running on Google Colab is on a remote server, not on your local machine. This means that any files you want to use in your code must be uploaded to the Colab environment or accessed from a cloud storage service like Google Drive.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "probability/naivebayes.html",
    "href": "probability/naivebayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes are a set of models for supervised learning based on applying Bayes’ theorem with a “naive” assumption.\nNaive Bayes models are generative and probabilistic.\nGenerative models are ones that make an assumption about how the data is generated. Probabilistic models are ones that make an assumption about the probability distribution of the data. A probabilistic models tell us the probability of the observation being in each class. This full distribution over the classes can be useful information for downstream decisions; avoiding making discrete decisions early on can be useful when combining system",
    "crumbs": [
      "Home",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "probability/naivebayes.html#bayes-theorem",
    "href": "probability/naivebayes.html#bayes-theorem",
    "title": "Naive Bayes",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\nRecall Bayes’ Theorem from probability theory:\n\\[ Posterior = \\frac{Likelihood \\cdot Prior}{Evidence} \\]\nor\n\\[ P(\\mathbf{y}|X) = \\frac{P(X|\\mathbf{y})P(\\mathbf{y})}{P(X)} \\]\nHere \\(X\\) is multi-dimensional input data (say Bag of Words representation of a set of documents) whereas \\(y\\) is the corresponding set of labels (say each document’s sentiment).",
    "crumbs": [
      "Home",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "probability/naivebayes.html#naive-assumption",
    "href": "probability/naivebayes.html#naive-assumption",
    "title": "Naive Bayes",
    "section": "Naive Assumption",
    "text": "Naive Assumption\nNote that the likelihood term \\(P(X|\\mathbf{y})\\) in the Bayes theorem can get very complicated to compute, for high dimensional data where X may be composed of many features.\nNaive Bayes’ makes a simplifying naive assumption that the features are independent of each other, given the class. In other words,\n\\[ P(X|y) = \\prod_{i=1}^{D} P(x_i|y) \\]\n\n\n\nThis is a naive assumption, since in most cases, features are not independent. This is especially not true for text data, where the presence of a word in a document is highly correlated with the presence of other words in the document. Hence the popular adage in NLP: “You shall know a word by the company it keeps” by Firth.\nHowever, the naive assumption makes the computation of the likelihood term tractable and still remarkably yields great results in practice.\nNaive Bayes model is a generative model since it makes an assumption about how the data is generated. It assumes that the data is generated by first sampling a class \\(y\\) from the prior distribution \\(P(y)\\) and then sampling each feature \\(x_i\\) from the likelihood distribution \\(P(x_i|y)\\).",
    "crumbs": [
      "Home",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "probability/naivebayes.html#bag-of-words-bow-as-multinomial-data",
    "href": "probability/naivebayes.html#bag-of-words-bow-as-multinomial-data",
    "title": "Naive Bayes",
    "section": "Bag of Words (BOW) as Multinomial Data",
    "text": "Bag of Words (BOW) as Multinomial Data\nRecall that multinomial distribution counts the number of times an outcome occurs when there are k-possible outcomes and N independent trials that can be used to capture probability of counts for rolling a k-sided die n times. Also recall that BOW representation of text is a count of how many times a word occurs in a document.\nBOW representation, therefore, can be modeled using the multinomial distibution. Here, instead of k-sided die, imagine a \\(|V|\\) sided die where \\(V\\) is the set of all unique words (vocabulary) and \\(|V|\\) is the size of the vocabulary. Each word in the vocabulary, in other words, is a possible outcome and count of a word in all documents constitutes a binomial distribution of that word.\n\n\n\n\n\n\nSo we can use the multinomial distribution to model the likelihood term \\(P(x_i|y)\\) in the prediction rule.\nHowever, the Naive Assumption in the Naive Bayes model alleviates the need to estimate the joint distribution of the features. Instead, we can estimate the parameters of the binomial distribution for each feature independently and simply multiply the probabilities of each feature to get the likelihood term.\nThe parameters of the binomial distribution are estimated using Maximum Likelihood Estimation (MLE) or Maximum A Posteriori (MAP) estimation.",
    "crumbs": [
      "Home",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "probability/naivebayes.html#multinomial-naive-bayes-classifier",
    "href": "probability/naivebayes.html#multinomial-naive-bayes-classifier",
    "title": "Naive Bayes",
    "section": "Multinomial Naive Bayes Classifier",
    "text": "Multinomial Naive Bayes Classifier\nNaive Bayes classifier is simply a probabilistic classifier where the prediction \\(\\hat{y}\\) is the class \\(y\\) that maximizes the posterior probability \\(P(\\mathbf{y}|X)\\) i.e. \n\\[ \\hat{y} = \\underset{y}{\\operatorname{argmax}} P(\\mathbf{y}|X) \\]\nIn other words, if y=0 maximizes P(y | X), then the predicted class is 0. Otherwise, if y=1 maximizes P(y | X), then the predicted class is 1.\nP(y | X) is proportional to P(X | y)P(y), as per the Bayes’ theorem. In other words, we can ignore the denominator P(X) since it is the same for all classes (values of \\(\\textbf{y}\\)).\nSo, we can also write the prediction rule as:\n\\[ \\hat{y} = \\underset{y}{\\operatorname{argmax}} P(X|\\mathbf{y})\\cdot P(\\mathbf{y}) \\]\nIf we make the naive assumption that the features are independent of each other, given the class, then we can write the prediction rule as:\n\\[ \\hat{y} = \\underset{y}{\\operatorname{argmax}} \\prod_{i=1}^{D} P(x_i|\\mathbf{y}) \\cdot P(\\mathbf{y})  \\]\nBased on our conversation on logarithms, when we apply log to the equation above, two things happen:\n\nDue to log being a monotonically increasing function, the outcome of argmax remains unchanged.\nAll multiplications in the equation become additions, making our lives easier.\n\nTherefore, we can write the prediction rule as:\n\\[ \\hat{y} = \\underset{y}{\\operatorname{argmax}} \\sum_{i=1}^{D} \\log P(x_i|\\mathbf{y}) + \\log P(\\mathbf{y})  \\]\nNow, let’s look at the two terms in the equation above:\n\n\\(P(\\mathbf{y})\\) is the prior probability of the class \\(y\\). It is estimated as the fraction of documents in the training set that belong to class \\(y\\).\n\\(P(x_i|\\mathbf{y})\\) is the likelihood term. It is the sum of the log probabilities of each \\(x_i\\) \\(X\\) given the class \\(y\\).\nIf, for example, we are trying to predict the sentiment of the document “Furman University” then the likelihood term would be \\(\\text{log}~ P(\\text{Furman}|y) + \\text{log}~ P(\\text{University}|y)\\).\nSimilarly, likelihood term for the document “Greeenville is in SC” would be \\(\\text{log}~P(\\text{Greenville}|y) + \\text{log}~ P(\\text{is}|y) + \\text{log}~ P(\\text{in}|y) + \\text{log}~ P(\\text{SC}|y)\\).",
    "crumbs": [
      "Home",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "probability/joint_prob.html",
    "href": "probability/joint_prob.html",
    "title": "Joint Probability",
    "section": "",
    "text": "import pandas as pd\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\ndata = pd.read_csv(\"../data/Shark Tank US dataset.csv\")\n\ndata = data[data.columns[:30]]\ndata['Got Deal'] = data['Got Deal'].astype(bool)\ndata.head(2)\n\n\n\n\n\n\n\n\nSeason Number\nStartup Name\nEpisode Number\nPitch Number\nSeason Start\nSeason End\nOriginal Air Date\nIndustry\nBusiness Description\nCompany Website\n...\nGot Deal\nTotal Deal Amount\nTotal Deal Equity\nDeal Valuation\nNumber of Sharks in Deal\nInvestment Amount Per Shark\nEquity Per Shark\nRoyalty Deal\nAdvisory Shares Equity\nLoan\n\n\n\n\n0\n1\nAvaTheElephant\n1\n1\n9-Aug-09\n5-Feb-10\n9-Aug-09\nHealth/Wellness\nAva The Elephant - Baby and Child Care\nhttp://www.avatheelephant.com/\n...\nTrue\n50000.0\n55.0\n90909.0\n1.0\n50000.0\n55.0\nNaN\nNaN\nNaN\n\n\n1\n1\nMrTod'sPieFactory\n1\n2\n9-Aug-09\n5-Feb-10\n9-Aug-09\nFood and Beverage\nMr. Tod's Pie Factory - Specialty Food\nhttp://whybake.com/\n...\nTrue\n460000.0\n50.0\n920000.0\n2.0\n230000.0\n25.0\nNaN\nNaN\nNaN\n\n\n\n\n2 rows × 30 columns",
    "crumbs": [
      "Home",
      "Joint Probability"
    ]
  },
  {
    "objectID": "probability/joint_prob.html#joint-frequency",
    "href": "probability/joint_prob.html#joint-frequency",
    "title": "Joint Probability",
    "section": "Joint Frequency",
    "text": "Joint Frequency\nThe joint frequency of two events is the number of times they both occur in a given number of trials.\nIn pandas, we can calculate the joint frequency of two events by using the crosstab function.\n\npd.crosstab(data['Industry'], data['Got Deal'])\n\n\n\n\n\n\n\nGot Deal\nFalse\nTrue\n\n\nIndustry\n\n\n\n\n\n\nAutomotive\n4\n13\n\n\nBusiness Services\n21\n19\n\n\nChildren/Education\n45\n78\n\n\nElectronics\n9\n7\n\n\nFashion/Beauty\n98\n128\n\n\nFitness/Sports/Outdoors\n48\n79\n\n\nFood and Beverage\n116\n180\n\n\nGreen/CleanTech\n5\n6\n\n\nHealth/Wellness\n27\n40\n\n\nLifestyle/Home\n83\n163\n\n\nLiquor/Alcohol\n5\n5\n\n\nMedia/Entertainment\n9\n17\n\n\nPet Products\n24\n33\n\n\nSoftware/Tech\n31\n38\n\n\nTravel\n6\n5\n\n\nUncertain/Other\n8\n15",
    "crumbs": [
      "Home",
      "Joint Probability"
    ]
  },
  {
    "objectID": "probability/joint_prob.html#joint-probability-pa-b",
    "href": "probability/joint_prob.html#joint-probability-pa-b",
    "title": "Joint Probability",
    "section": "Joint Probability \\(P(A, B)\\)",
    "text": "Joint Probability \\(P(A, B)\\)\nJoint probability is the probability of two events occurring together.The joint probability is usually denoted by \\(P(A, B)\\), which is shorthand for \\(P(A \\wedge B)\\) read as Probability of \\(A\\) AND \\(B\\).\nNote that \\(P(A, B) = P(B, A)\\) since \\(A \\wedge B = B \\wedge A\\).\nFor example, if we are rolling two dice, the joint probability is the probability of rolling a 1 on the first die and a 2 on the second die.\nIn Data Science, we rarely know the true joint probability. Instead, we estimate the joint probability from data. We will talk more about this when we talk about Statistics.\n\njoint_prob = pd.crosstab(data['Industry'], data['Got Deal'], normalize=True)\n\njoint_prob\n\n\n\n\n\n\n\nGot Deal\nFalse\nTrue\n\n\nIndustry\n\n\n\n\n\n\nAutomotive\n0.002930\n0.009524\n\n\nBusiness Services\n0.015385\n0.013919\n\n\nChildren/Education\n0.032967\n0.057143\n\n\nElectronics\n0.006593\n0.005128\n\n\nFashion/Beauty\n0.071795\n0.093773\n\n\nFitness/Sports/Outdoors\n0.035165\n0.057875\n\n\nFood and Beverage\n0.084982\n0.131868\n\n\nGreen/CleanTech\n0.003663\n0.004396\n\n\nHealth/Wellness\n0.019780\n0.029304\n\n\nLifestyle/Home\n0.060806\n0.119414\n\n\nLiquor/Alcohol\n0.003663\n0.003663\n\n\nMedia/Entertainment\n0.006593\n0.012454\n\n\nPet Products\n0.017582\n0.024176\n\n\nSoftware/Tech\n0.022711\n0.027839\n\n\nTravel\n0.004396\n0.003663\n\n\nUncertain/Other\n0.005861\n0.010989\n\n\n\n\n\n\n\n\nsum(joint_prob)\n\n1\n\n\nNote that sum of joint probabilities is 1 i.e. \\(\\sum P(C, D) = 1\\) at the end of the day, since the sum of all probabilities is 1.\nThe following three are all true at the same time:\n\n\\(\\sum_{C, D} P(C, D) = 1\\) where \\(P(C, D)\\) is a probability table with 12 rows and 3 columns: \\(C, D, P(C, D)\\).\n\\(\\sum_{C} P(C) = 1\\) where \\(P(C)\\) is a probability table with 2 rows (\\({H, T}\\)) and 2 columns: \\(C, P(C)\\).\n\\(\\sum_{D} P(D) = 1\\) where \\(P(D)\\) is a probability table with 6 rows (\\({1, 2, 3, 4, 5, 6}\\)) and 2 columns: \\(D, P(D)\\).\n\n\nfrom matplotlib import pyplot as plt\n\nplt.style.use('dark_background')\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nax.bar(joint_prob.index, joint_prob[True], label='Got Deal', color='blue', alpha=0.5)\nax.bar(joint_prob.index, joint_prob[False], bottom=joint_prob[True], label='No Deal', color='red', alpha=0.5)\n\nax.set_xticklabels(joint_prob.index, rotation=90)\nax.set_ylabel('Probability')\nax.set_title('Probability of Getting a Deal by Industry')\nax.legend();\n\n\n\n\n\n\n\n\n\n\n\nplt.style.use('dark_background')\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nx_values  = pd.Series(range(len(joint_prob.index)))\nx_values1 = x_values + 0.1\nx_values2 = x_values - 0.1\n\nax.bar(x_values1, joint_prob[True],  label='Got Deal', color='blue', alpha=0.5, width=0.2)\nax.bar(x_values2, joint_prob[False], label='No Deal', color='red', alpha=0.5, width=0.2)\n\nax.set_xticks(x_values)\nax.set_xticklabels(joint_prob.index, rotation=90)\n\nax.set_ylabel('Probability')\nax.set_title('Probability of Getting a Deal by Industry')\nax.legend();",
    "crumbs": [
      "Home",
      "Joint Probability"
    ]
  },
  {
    "objectID": "probability/joint_prob.html#marginal-probability-pa",
    "href": "probability/joint_prob.html#marginal-probability-pa",
    "title": "Joint Probability",
    "section": "Marginal Probability \\(P(A)\\)",
    "text": "Marginal Probability \\(P(A)\\)\nBecause most data sets are multi-dimensional i.e. involving multiple random variables, we can sometimes find ourselves in a situation where we want to know the joint probability \\(P(A, B)\\) of two random variables \\(A\\) and \\(B\\) but we don’t know \\(P(A)\\) or \\(P(B)\\). In such cases, we compute the marginal probability of one variable from joint probability over multiple random variables.\nMarginalizing is the process of summing over one or more variables (say B) to get the probability of another variable (say A). This summing takes place over the joint probability table.\n\\[ P(A) = \\sum_{b \\in \\Omega_B} P(A, B=b) \\]\n\njoint_prob\n\n\n\n\n\n\n\nGot Deal\nFalse\nTrue\n\n\nIndustry\n\n\n\n\n\n\nAutomotive\n0.002930\n0.009524\n\n\nBusiness Services\n0.015385\n0.013919\n\n\nChildren/Education\n0.032967\n0.057143\n\n\nElectronics\n0.006593\n0.005128\n\n\nFashion/Beauty\n0.071795\n0.093773\n\n\nFitness/Sports/Outdoors\n0.035165\n0.057875\n\n\nFood and Beverage\n0.084982\n0.131868\n\n\nGreen/CleanTech\n0.003663\n0.004396\n\n\nHealth/Wellness\n0.019780\n0.029304\n\n\nLifestyle/Home\n0.060806\n0.119414\n\n\nLiquor/Alcohol\n0.003663\n0.003663\n\n\nMedia/Entertainment\n0.006593\n0.012454\n\n\nPet Products\n0.017582\n0.024176\n\n\nSoftware/Tech\n0.022711\n0.027839\n\n\nTravel\n0.004396\n0.003663\n\n\nUncertain/Other\n0.005861\n0.010989\n\n\n\n\n\n\n\n\n# marginal probability of getting a deal\nmarginal_prob = joint_prob.sum(axis=0)\nmarginal_prob\n\nGot Deal\nFalse    0.394872\nTrue     0.605128\ndtype: float64\n\n\n\ndata['Got Deal'].value_counts(normalize=True)\n\nTrue     0.605128\nFalse    0.394872\nName: Got Deal, dtype: float64\n\n\n\n# marginal probability of industry\nmarginal_prob = joint_prob.sum(axis=1)\nmarginal_prob\n\nIndustry\nAutomotive                 0.012454\nBusiness Services          0.029304\nChildren/Education         0.090110\nElectronics                0.011722\nFashion/Beauty             0.165568\nFitness/Sports/Outdoors    0.093040\nFood and Beverage          0.216850\nGreen/CleanTech            0.008059\nHealth/Wellness            0.049084\nLifestyle/Home             0.180220\nLiquor/Alcohol             0.007326\nMedia/Entertainment        0.019048\nPet Products               0.041758\nSoftware/Tech              0.050549\nTravel                     0.008059\nUncertain/Other            0.016850\ndtype: float64\n\n\n\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\naxs[0].bar(joint_prob.index, joint_prob[True], label='Got Deal', color='blue', alpha=0.5)\naxs[0].bar(joint_prob.index, joint_prob[False], bottom=joint_prob[True], label='No Deal', color='red', alpha=0.5)\naxs[0].set_xticklabels(joint_prob.index, rotation=90)\naxs[0].set_ylabel('Probability')\naxs[0].set_title('Probability of Getting a Deal by Industry')\n\n# Marginal probability of getting a deal\nmarginal_prob_deal = joint_prob.sum(axis=0)\naxs[1].bar(marginal_prob_deal.index, marginal_prob_deal, color=['blue', 'red'], alpha=0.5)\naxs[1].set_xticks([0, 1])\naxs[1].set_xticklabels(['Got Deal', 'No Deal'])\naxs[1].set_ylabel('Probability')\naxs[1].set_title('Marginal Probability of Getting a Deal')\n\n# Marginal probability of industry\nmarginal_prob_industry = joint_prob.sum(axis=1)\naxs[2].bar(joint_prob.index, marginal_prob_industry, color='green', alpha=0.5)\naxs[2].set_xticklabels(joint_prob.index, rotation=90)\naxs[2].set_ylabel('Probability')\naxs[2].set_title('Marginal Probability of Industry')\n\nText(0.5, 1.0, 'Marginal Probability of Industry')\n\n\n\n\n\n\n\n\n\nAs we look at new concepts in probability, it is important to stay mindful of i) what the probability sums to ii) what are the dimensions of the table that represents the probability.\nYou can see from the cell below that the dimensions of marginal probability table is the length of the range of the variable.\nYou can see from the code below that both the computed marginal probabilities in add up to 1.",
    "crumbs": [
      "Home",
      "Joint Probability"
    ]
  },
  {
    "objectID": "probability/joint_prob.html#independent-random-variables",
    "href": "probability/joint_prob.html#independent-random-variables",
    "title": "Joint Probability",
    "section": "Independent Random Variables",
    "text": "Independent Random Variables\nRandom variables can be either independent or dependent. If two random variables are independent, then the value of one random variable does not affect the value of the other random variable.\nFor example, if we are rolling two dice, we can use two random variables to represent the numbers that we roll. The two random variables are independent because the value of one die does not affect the value of the other die. If two random variables are dependent, then the value of one random variable does affect the value of the other random variable. For example, if we are measuring the temperature and the humidity, we can use two random variables to represent the temperature and the humidity. The two random variables are dependent because the temperature affects the humidity and the humidity affects the temperature.\nMore formally, two random variables \\(X\\) and \\(Y\\) are independent if and only if \\(P(X, Y) = P(X) \\cdot P(Y)\\).\n\n# Test for independence of Industry and Got Deal\n\nprob_deal     = joint_prob.sum(axis=0)\nprob_industry = joint_prob.sum(axis=1)\n\nprob_deal\n\nGot Deal\nFalse    0.394872\nTrue     0.605128\ndtype: float64\n\n\n\njoint_prob = pd.crosstab(data['Industry'], data['Got Deal'], normalize=True)\n\n\njoint_prob['False2'] = joint_prob.apply(lambda x: (prob_deal[False] * prob_industry[x.name]), axis=1)\njoint_prob['True2'] = joint_prob.apply(lambda x: (prob_deal[True] * prob_industry[x.name]), axis=1)\n\njoint_prob\n\n\n\n\n\n\n\nGot Deal\nFalse\nTrue\nFalse2\nTrue2\n\n\nIndustry\n\n\n\n\n\n\n\n\nAutomotive\n0.002930\n0.009524\n0.004918\n0.007536\n\n\nBusiness Services\n0.015385\n0.013919\n0.011571\n0.017733\n\n\nChildren/Education\n0.032967\n0.057143\n0.035582\n0.054528\n\n\nElectronics\n0.006593\n0.005128\n0.004629\n0.007093\n\n\nFashion/Beauty\n0.071795\n0.093773\n0.065378\n0.100190\n\n\nFitness/Sports/Outdoors\n0.035165\n0.057875\n0.036739\n0.056301\n\n\nFood and Beverage\n0.084982\n0.131868\n0.085628\n0.131222\n\n\nGreen/CleanTech\n0.003663\n0.004396\n0.003182\n0.004876\n\n\nHealth/Wellness\n0.019780\n0.029304\n0.019382\n0.029702\n\n\nLifestyle/Home\n0.060806\n0.119414\n0.071164\n0.109056\n\n\nLiquor/Alcohol\n0.003663\n0.003663\n0.002893\n0.004433\n\n\nMedia/Entertainment\n0.006593\n0.012454\n0.007521\n0.011526\n\n\nPet Products\n0.017582\n0.024176\n0.016489\n0.025269\n\n\nSoftware/Tech\n0.022711\n0.027839\n0.019961\n0.030589\n\n\nTravel\n0.004396\n0.003663\n0.003182\n0.004876\n\n\nUncertain/Other\n0.005861\n0.010989\n0.006654\n0.010196\n\n\n\n\n\n\n\n\njoint_prob = joint_prob.round(2)\njoint_prob\n\n\n\n\n\n\n\nGot Deal\nFalse\nTrue\nFalse2\nTrue2\n\n\nIndustry\n\n\n\n\n\n\n\n\nAutomotive\n0.00\n0.01\n0.00\n0.01\n\n\nBusiness Services\n0.02\n0.01\n0.01\n0.02\n\n\nChildren/Education\n0.03\n0.06\n0.04\n0.05\n\n\nElectronics\n0.01\n0.01\n0.00\n0.01\n\n\nFashion/Beauty\n0.07\n0.09\n0.07\n0.10\n\n\nFitness/Sports/Outdoors\n0.04\n0.06\n0.04\n0.06\n\n\nFood and Beverage\n0.08\n0.13\n0.09\n0.13\n\n\nGreen/CleanTech\n0.00\n0.00\n0.00\n0.00\n\n\nHealth/Wellness\n0.02\n0.03\n0.02\n0.03\n\n\nLifestyle/Home\n0.06\n0.12\n0.07\n0.11\n\n\nLiquor/Alcohol\n0.00\n0.00\n0.00\n0.00\n\n\nMedia/Entertainment\n0.01\n0.01\n0.01\n0.01\n\n\nPet Products\n0.02\n0.02\n0.02\n0.03\n\n\nSoftware/Tech\n0.02\n0.03\n0.02\n0.03\n\n\nTravel\n0.00\n0.00\n0.00\n0.00\n\n\nUncertain/Other\n0.01\n0.01\n0.01\n0.01\n\n\n\n\n\n\n\n\njoint_prob[False] == joint_prob['False2']\n\nIndustry\nAutomotive                  True\nBusiness Services          False\nChildren/Education         False\nElectronics                False\nFashion/Beauty              True\nFitness/Sports/Outdoors     True\nFood and Beverage          False\nGreen/CleanTech             True\nHealth/Wellness             True\nLifestyle/Home             False\nLiquor/Alcohol              True\nMedia/Entertainment         True\nPet Products                True\nSoftware/Tech               True\nTravel                      True\nUncertain/Other             True\ndtype: bool\n\n\n\njoint_prob[True] == joint_prob['True2']\n\nIndustry\nAutomotive                  True\nBusiness Services          False\nChildren/Education         False\nElectronics                 True\nFashion/Beauty             False\nFitness/Sports/Outdoors     True\nFood and Beverage           True\nGreen/CleanTech             True\nHealth/Wellness             True\nLifestyle/Home             False\nLiquor/Alcohol              True\nMedia/Entertainment         True\nPet Products               False\nSoftware/Tech               True\nTravel                      True\nUncertain/Other             True\ndtype: bool",
    "crumbs": [
      "Home",
      "Joint Probability"
    ]
  },
  {
    "objectID": "beyond/pickle.html",
    "href": "beyond/pickle.html",
    "title": "Pickle",
    "section": "",
    "text": "pickle is a Python module used to serialize and deserialize Python objects. It can be used to store and retrieve Python objects from disk."
  },
  {
    "objectID": "beyond/pickle.html#serialization",
    "href": "beyond/pickle.html#serialization",
    "title": "Pickle",
    "section": "Serialization",
    "text": "Serialization\nSerialization is the process of converting a Python object into a byte stream. This byte stream can be stored on disk or sent over a network.\nThe pickle.dump() function is used to serialize a Python object. It takes two arguments: the object to serialize and a file object to write the byte stream to.\nimport pickle\n\ndata = {'name': 'Alice', 'age': 25}\n\nwith open('data.pickle', 'wb') as f:\n    pickle.dump(data, f)\nIn this example, we serialize a dictionary containing a person’s name and age to a file called data.pickle."
  },
  {
    "objectID": "beyond/pickle.html#deserialization",
    "href": "beyond/pickle.html#deserialization",
    "title": "Pickle",
    "section": "Deserialization",
    "text": "Deserialization\nDeserialization is the process of converting a byte stream back into a Python object.\nThe pickle.load() function is used to deserialize a Python object. It takes a file object containing the byte stream as an argument and returns the deserialized object.\n\nwith open('data.pickle', 'rb') as f:\n    data = pickle.load(f)\n\nprint(data)\nIn this example, we deserialize the byte stream from the data.pickle file back into a Python object and print it."
  },
  {
    "objectID": "beyond/pickle.html#security",
    "href": "beyond/pickle.html#security",
    "title": "Pickle",
    "section": "Security",
    "text": "Security\nIt is important to note that the pickle module is not secure. Deserializing untrusted data can lead to security vulnerabilities, as malicious code can be executed during deserialization. It is recommended to only deserialize data from trusted sources."
  },
  {
    "objectID": "beyond/tqdm.html",
    "href": "beyond/tqdm.html",
    "title": "Tqdm",
    "section": "",
    "text": "tqdm is a Python library that allows you to add a progress bar to your loops. It’s very easy to use and can be a great help when you’re working with large datasets or when you’re running long computations.\nIn this notebook, I’ll show you how to use tqdm with a few examples."
  },
  {
    "objectID": "beyond/tqdm.html#installation",
    "href": "beyond/tqdm.html#installation",
    "title": "Tqdm",
    "section": "Installation",
    "text": "Installation\nYou can install tqdm using pip:\npip install tqdm"
  },
  {
    "objectID": "beyond/tqdm.html#usage",
    "href": "beyond/tqdm.html#usage",
    "title": "Tqdm",
    "section": "Usage",
    "text": "Usage\nTo use tqdm, you just need to wrap your iterable with the tqdm function. Here’s an example:\n\nfrom tqdm import tqdm\n\nfor i in tqdm(range(100)):\n    # do something\nThis will create a progress bar that shows the progress of the loop. You can customize the progress bar by passing additional arguments to the tqdm function. For example, you can set the total number of iterations, the width of the progress bar, and the format of the progress bar.\nHere’s an example that shows how to customize the progress bar:\nfrom tqdm import tqdm\n\nfor i in tqdm(range(100), total=100, desc=\"Processing\", ncols=100):\n    # do something\nThis will create a progress bar with a width of 100 characters and a description that says “Processing”."
  },
  {
    "objectID": "beyond/tqdm.html#examples",
    "href": "beyond/tqdm.html#examples",
    "title": "Tqdm",
    "section": "Examples",
    "text": "Examples\nHere are a few examples that show how to use tqdm with different types of iterables:\n\nList\nfrom tqdm import tqdm\n\ndata = [1, 2, 3, 4, 5]\n\nfor i in tqdm(data, desc=\"Processing\", ncols=100):\n    # do something\n\n\nRange\n\nfrom tqdm import tqdm\n\nfor i in tqdm(range(100), desc=\"Processing\", ncols=100):\n    # do something\n\n\nFile\nfrom tqdm import tqdm\n\nwith open(\"data.txt\", \"r\") as f:\n    for line in tqdm(f, desc=\"Processing\", ncols=100):\n        # do something\n\n\nPandas DataFrame\nimport pandas as pd\nfrom tqdm import tqdm\n\ndf = pd.read_csv(\"data.csv\")\n\nfor index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\", ncols=100):\n    # do something"
  },
  {
    "objectID": "beyond/21_clustering.html",
    "href": "beyond/21_clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is the most well-known unsupervised learning technique. The goal of clustering is to discover groups in observations. The groups are called clusters.\nThe data points in the same cluster are similar to each other, compared to points in different clusters, which are relatively dissimilar.\nThere are many clustering algorithms. In this notebook, we will focus on two of them:\nTo compare the performance of the clustering algorithms, in the code below we will use the same six datasets capturing a wide variety of patterns and structures.\nimport pandas as pd \nfrom matplotlib import pyplot as plt\nimport seaborn as sns \n\nplt.style.use('dark_background')\n\nurl = \"https://raw.githubusercontent.com/fahadsultan/datascience_ml/main/data/clusters/\"\n\nfnames = [\"aniso\", \"blobs\", \"no_structure\", \"noisy_circles\", \"noisy_moons\", \"varied\"]\n\ndatasets = {}\n\nfig, axs = plt.subplots(1, len(fnames), figsize=(17, 3))\nfor i, fname in enumerate(fnames):\n    df = pd.read_csv(url + fname + \".csv\", index_col=0)\n    df.columns = ['x1', 'x2']\n    ax = sns.scatterplot(data=df, x='x1', y='x2', ax=axs[i]);\n    ax.set(title=fname)\n    datasets[fname] = df\nNote that the data sets are not labeled. Also note that unsupervised learning algorithms do not work only with 2-dimensional data but with data of any dimensionality. Here we use 2-dimensional data only to be able to visualize the results.",
    "crumbs": [
      "Home",
      "Clustering"
    ]
  },
  {
    "objectID": "beyond/21_clustering.html#challenges",
    "href": "beyond/21_clustering.html#challenges",
    "title": "Clustering",
    "section": "Challenges",
    "text": "Challenges\n\nLimitations of Clustering\nNote that not all data sets are suitable for clustering. Some data sets do not have a well-defined cluster structure.\nFor example, below we try the k-means algorithm on the sentiments dataset. We know that the data set has three classes: positive, negative, and neutral. However, the k-means algorithm fails to discover the three classes. This is because the data set does not have a well-defined cluster structure.\n\nimport pandas as pd\ndata = pd.read_csv('https://raw.githubusercontent.com/fahadsultan/datascience_ml/main/data/chat_dataset.csv')\ndata.head()\n\n\n\n\n\n\n\n\nmessage\nsentiment\n\n\n\n\n0\nI really enjoyed the movie\npositive\n\n\n1\nThe food was terrible\nnegative\n\n\n2\nI'm not sure how I feel about this\nneutral\n\n\n3\nThe service was excellent\npositive\n\n\n4\nI had a bad experience\nnegative\n\n\n\n\n\n\n\n\nvocab = ' '.join(data['message'].values).lower().split()\nvocab = list(set(vocab))\n\nbow = pd.DataFrame(0, columns=vocab, index=data.index)\nfor word in vocab:\n    bow[word] = data['message'].apply(lambda x: x.lower().split().count(word))\n\nkmeans = KMeans(n_clusters=3, random_state=0)\n\ndef l2_norm(x):\n    return (sum(x**2))**(1/2)\n\nbow = bow.apply(lambda x: x/l2_norm(x), axis=1)\nkmeans.fit(bow)\n\ndata['label'] = kmeans.labels_\n\n\ndata['label'].value_counts()\n\n1    201\n2    199\n0    184\nName: label, dtype: int64\n\n\n\ndata.groupby(['label', 'sentiment']).size()\n\nlabel  sentiment\n0      negative      50\n       neutral       72\n       positive      62\n1      negative      69\n       neutral       70\n       positive      62\n2      negative      28\n       neutral      117\n       positive      54\ndtype: int64\n\n\n\n\nRelationship between Clusters and Labels\nPlease take caution in comparing the discovered clusters with any available labels for a dataset.\nIn clustering, the label ‘values’ are arbitrary. For example, if we have a dataset with three classes, we can label them as 0, 1, and 2 or as 1, 2, and 3 or as 100, 200, and 300.\n\nfrom sklearn.datasets import load_iris\n\ndata = load_iris(as_frame=True)\nX    = data['data']\n\nkmeans = KMeans(n_clusters=3, random_state=0)\nkmeans.fit(X)\ncluster_labels = kmeans.labels_\n\ny    = data['target']\n\nsum(cluster_labels == y)/len(y)\n\n0.24\n\n\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\nax = sns.scatterplot(data=X, \\\n                x='sepal length (cm)', \\\n                y='sepal width (cm)', \\\n                hue=y, \\\n                palette='viridis', ax=axs[0]);\nax.set(title=\"True labels\");\n\nax = sns.scatterplot(data=X, \\\n                x='sepal length (cm)', \\\n                y='sepal width (cm)', \\\n                hue=cluster_labels, \\\n                palette='viridis', ax=axs[1]);\nax.set(title=\"Cluster labels\");\n\n\n\n\n\n\n\n\n\n\nSilhouette Score\nThe Silhouette Score is calculated using the mean intra-cluster distance (\\(a\\)) and the mean nearest-cluster distance (\\(b\\)) for each sample. The Silhouette Coefficient for a sample is\n\\[\\text{Silhouette Coefficient} = \\frac{(b - a)}{\\text{max}(a, b)}\\]\nwhere \\(b\\) is the distance between a sample and the nearest cluster that the sample is not a part of.\nNote that Silhouette Coefficient is only defined if number of labels is 2 &lt;= n_labels &lt;= n_samples - 1.\nsklearn.metrics.silhouette_score function returns the mean Silhouette Coefficient over all samples. To obtain the values for each sample, use silhouette_samples.\nThe best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.\nA score of 1 indicates that the object is far away from the neighboring clusters. A score of 0 indicates that the object is close to the decision boundary between two neighboring clusters. A score of -1 indicates that the object may have been assigned to the wrong cluster.\n\nSilhouette Score to identify \\(k\\), \\(\\epsilon\\) and min_samples\n\nX = datasets['noisy_circles']\nax = sns.scatterplot(data=X, x='x1', y='x2');\nax.set(title=\"Noisy Circles\");\n\nplt.figure();\nX = datasets['blobs']\nax = sns.scatterplot(data=X, x='x1', y='x2');\nax.set(title=\"Blobs\");\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting Silhouette score to find k\n\nfrom sklearn.metrics import silhouette_score\n\nsscores = {'noisy_circles':[], 'blobs':[]}\nks = [2, 3, 4, 5, 10, 15]\nurl = \"https://raw.githubusercontent.com/fahadsultan/datascience_ml/main/data/clusters/\"\n\nfor name in ['noisy_circles', 'blobs']:\n\n    X = pd.read_csv(url + name + \".csv\", index_col=0)\n\n    for k in ks:\n        kmeans = KMeans(n_clusters=k, random_state=0)\n        kmeans.fit(X)\n        score = silhouette_score(X, kmeans.labels_)\n        sscores[name].append(score)\n\nax = sns.lineplot(x=ks, y=sscores['noisy_circles'], marker='s');\nax = sns.lineplot(x=ks, y=sscores['blobs'], marker='s');\nax.set(xlabel='k', ylabel='silhouette_score');\n\nplt.grid()\nplt.legend(['noisy_circles', 'blobs']);\nplt.title('Because K-Means does not work for `noisy circles` data, \\nSilhouette Score never reaches close to 1 for any `k`');\n\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n\n\nepsilons = [0.1, 0.2, 0.5, 1, 2]\n\nfig, axs = plt.subplots(1, len(epsilons), figsize=(17, 3))\n\n\nX = datasets['blobs']\nsscores = []\nfor i, e in enumerate(epsilons):\n    dbscan = DBSCAN(eps=e, min_samples=2)\n    dbscan.fit(X[['x1', 'x2']])\n    score = silhouette_score(X, dbscan.labels_)\n    sscores.append(score)\n    # sns.scatterplot(data=X, x='x1', y='x2', hue=dbscan.labels_)\n    axs[i].scatter(X['x1'], X['x2'], c=dbscan.labels_)\n    axs[i].set_title(\"epsilon = \"+ str(e))\n\nplt.figure();\nax = sns.lineplot(x=epsilons, y=sscores, marker='s');\nax.set(xlabel='eps', ylabel='Silhouette Score');\n\nplt.grid()\nplt.legend(['blobs']);",
    "crumbs": [
      "Home",
      "Clustering"
    ]
  },
  {
    "objectID": "beyond/beyond.html",
    "href": "beyond/beyond.html",
    "title": "BEYOND PANDAS",
    "section": "",
    "text": "BEYOND PANDAS\nPython has a lot of libraries that are useful for data analysis in addition to pandas. In this section, we will explore some of these libraries and how they can be used to analyze data.\n\nHere we will explore some of the libraries that are commonly used in data analysis and machine learning."
  },
  {
    "objectID": "beyond/sklearn.html",
    "href": "beyond/sklearn.html",
    "title": "Scikit-learn",
    "section": "",
    "text": "Sklearn is a Python library for machine learning. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.",
    "crumbs": [
      "Home",
      "Scikit-learn"
    ]
  },
  {
    "objectID": "beyond/sklearn.html#fitting-and-predicting-estimator-basics",
    "href": "beyond/sklearn.html#fitting-and-predicting-estimator-basics",
    "title": "Scikit-learn",
    "section": "Fitting and predicting: estimator basics",
    "text": "Fitting and predicting: estimator basics\nScikit-learn provides dozens of built-in machine learning algorithms and models, called estimators. Each estimator can be fitted to some data using its fit method.\nHere is a simple example where we fit a RandomForestClassifier to some very basic data:\n\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\nX, y = load_iris(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n\nnb = MultinomialNB()\nnn = KNeighborsClassifier()\n\nnb.fit(X_train, y_train)\nnn.fit(X_train, y_train)\n\npreds_nb = nb.predict(X_test)\npreds_nn = nn.predict(X_test)\n\nprint(\"Naive Bayes accuracy: \", (preds_nb == y_test).mean())\nprint(\"KNN accuracy: \", (preds_nn == y_test).mean())\n\nNaive Bayes accuracy:  0.6\nKNN accuracy:  0.96\n\n\n\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(preds_nb, y_test))\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        21\n           1       0.00      0.00      0.00         0\n           2       1.00      0.44      0.62        54\n\n    accuracy                           0.60        75\n   macro avg       0.67      0.48      0.54        75\nweighted avg       1.00      0.60      0.72        75\n\n\n\nThe fit method generally accepts 2 inputs:\n\nThe samples matrix (or design matrix) X. The size of X is typically (n_samples, n_features), which means that samples are represented as rows and features are represented as columns.\nThe target values y which are real numbers for regression tasks, or integers for classification (or any other discrete set of values). For unsupervised learning tasks, y does not need to be specified. y is usually a 1d array where the i th entry corresponds to the target of the i th sample (row) of X.\n\nBoth X and y are usually expected to be numpy arrays or equivalent array-like data types, though some estimators work with other formats such as sparse matrices.\nOnce the estimator is fitted, it can be used for predicting target values of new data. You don’t need to re-train the estimator:",
    "crumbs": [
      "Home",
      "Scikit-learn"
    ]
  },
  {
    "objectID": "beyond/sklearn.html#transformers-and-pre-processors",
    "href": "beyond/sklearn.html#transformers-and-pre-processors",
    "title": "Scikit-learn",
    "section": "Transformers and pre-processors",
    "text": "Transformers and pre-processors\nMachine learning workflows are often composed of different parts. A typical pipeline consists of a pre-processing step that transforms or imputes the data, and a final predictor that predicts target values.\nIn scikit-learn, pre-processors and transformers follow the same API as the estimator objects (they actually all inherit from the same BaseEstimator class). The transformer objects don’t have a predict method but rather a transform method that outputs a newly transformed sample matrix X:\n\nfrom sklearn.preprocessing import StandardScaler\nX = [[0, 15],\n     [1, -10]]\n# scale data according to computed scaling values\nStandardScaler().fit(X).transform(X)\n\nSometimes, you want to apply different transformations to different features: the ColumnTransformer is designed for these use-cases.",
    "crumbs": [
      "Home",
      "Scikit-learn"
    ]
  },
  {
    "objectID": "beyond/sklearn.html#pipelines-chaining-pre-processors-and-estimators",
    "href": "beyond/sklearn.html#pipelines-chaining-pre-processors-and-estimators",
    "title": "Scikit-learn",
    "section": "Pipelines: chaining pre-processors and estimators",
    "text": "Pipelines: chaining pre-processors and estimators\nTransformers and estimators (predictors) can be combined together into a single unifying object: a Pipeline. The pipeline offers the same API as a regular estimator: it can be fitted and used for prediction with fit and predict. As we will see later, using a pipeline will also prevent you from data leakage, i.e. disclosing some testing data in your training data.\nIn the following example, we load the Iris dataset, split it into train and test sets, and compute the accuracy score of a pipeline on the test data:\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n# create a pipeline object\npipe = make_pipeline(\n    StandardScaler(),\n    LogisticRegression()\n)\n# load the iris dataset and split it into train and test sets\nX, y = load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n# fit the whole pipeline\npipe.fit(X_train, y_train)\n# we can now use it like any other estimator\naccuracy_score(pipe.predict(X_test), y_test)",
    "crumbs": [
      "Home",
      "Scikit-learn"
    ]
  },
  {
    "objectID": "beyond/sklearn.html#model-evaluation",
    "href": "beyond/sklearn.html#model-evaluation",
    "title": "Scikit-learn",
    "section": "Model evaluation",
    "text": "Model evaluation\nFitting a model to some data does not entail that it will predict well on unseen data. This needs to be directly evaluated. We have just seen the train_test_split helper that splits a dataset into train and test sets, but scikit-learn provides many other tools for model evaluation, in particular for cross-validation.\nWe here briefly show how to perform a 5-fold cross-validation procedure, using the cross_validate helper. Note that it is also possible to manually iterate over the folds, use different data splitting strategies, and use custom scoring functions. Please refer to our User Guide for more details\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_validate\nX, y = make_regression(n_samples=1000, random_state=0)\nlr = LinearRegression()\nresult = cross_validate(lr, X, y)  # defaults to 5-fold CV\nresult['test_score']  # r_squared score is high because dataset is easy",
    "crumbs": [
      "Home",
      "Scikit-learn"
    ]
  },
  {
    "objectID": "beyond/sklearn.html#automatic-parameter-searches",
    "href": "beyond/sklearn.html#automatic-parameter-searches",
    "title": "Scikit-learn",
    "section": "Automatic parameter searches",
    "text": "Automatic parameter searches\nAll estimators have parameters (often called hyper-parameters in the literature) that can be tuned. The generalization power of an estimator often critically depends on a few parameters. For example a RandomForestRegressor has a n_estimators parameter that determines the number of trees in the forest, and a max_depth parameter that determines the maximum depth of each tree. Quite often, it is not clear what the exact values of these parameters should be since they depend on the data at hand.\nScikit-learn provides tools to automatically find the best parameter combinations (via cross-validation). In the following example, we randomly search over the parameter space of a random forest with a RandomizedSearchCV object. When the search is over, the RandomizedSearchCV behaves as a RandomForestRegressor that has been fitted with the best set of parameters. Read more in the User Guide:\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import randint\nX, y = fetch_california_housing(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n# define the parameter space that will be searched over\nparam_distributions = {'n_estimators': randint(1, 5),\n                       'max_depth': randint(5, 10)}\n# now create a searchCV object and fit it to the data\nsearch = RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0),\n                            n_iter=5,\n                            param_distributions=param_distributions,\n                            random_state=0)\nsearch.fit(X_train, y_train)\nsearch.best_params_\n\n# the search object now acts like a normal random forest estimator\n# with max_depth=9 and n_estimators=4\nsearch.score(X_test, y_test)\n\nIn practice, you almost always want to search over a pipeline, instead of a single estimator. One of the main reasons is that if you apply a pre-processing step to the whole dataset without using a pipeline, and then perform any kind of cross-validation, you would be breaking the fundamental assumption of independence between training and testing data. Indeed, since you pre-processed the data using the whole dataset, some information about the test sets are available to the train sets. This will lead to over-estimating the generalization power of the estimator (you can read more in this Kaggle post).\nUsing a pipeline for cross-validation and searching will largely keep you from this common pitfall.",
    "crumbs": [
      "Home",
      "Scikit-learn"
    ]
  },
  {
    "objectID": "beyond/42_regression.html",
    "href": "beyond/42_regression.html",
    "title": "Regression",
    "section": "",
    "text": "Recall that all supervised learning is based on the assumption that there is a relationship between the input variables \\(X\\) and the output variable \\(y\\) i.e.\n\\[\\textbf{y} = f(\\textbf{X})\\]\nwhere \\(f\\) is some unknown function.\n\\(X\\) here simply is some data that we have as a pd.DataFrame whereas \\(y\\) here is the target variable, one value for each observation, that we want to predict, as of type pd.Series.\nThe form of supervised learning we have talked about so far is classification. As discussed previously, in classification, the output variable \\(y\\) is a discrete target variable e.g. sentiment \\(\\in\\) {positive, neutral or negative}, ring \\(\\in\\) {A, B} or diagnosis \\(\\in\\) {malignant, benign} etc.\nThe other type of supervised learning that we will talk about in this notebook is called Regression. In regression, the target variable is to predict a continuous target variable i.e. \\[\\mathbf{y} \\in \\mathbb{R^N}\\].\nFor example, predicting the stock price of a publicly listed company, predicting the price of a house in dollars, or predicting the average surface temperature on Earth next year are all examples of regression problems.\nNote that the splitting of the data into training and test sets is exactly the same as in classification. The only difference is that the target variable is continuous instead of discrete.",
    "crumbs": [
      "Home",
      "Regression"
    ]
  },
  {
    "objectID": "beyond/42_regression.html#errors-in-regression",
    "href": "beyond/42_regression.html#errors-in-regression",
    "title": "Regression",
    "section": "Errors in Regression",
    "text": "Errors in Regression\nThe evaluation of regression models is done similar to classification models. The model is trained on the training set and then evaluated on the test set.\n\n\n\nThe difference is that the training has an internal evaluation metric that is minimized to find the values of coefficients such as \\(m\\) and \\(b\\) that best fit the training data.\nFor example, model-1 and model-2 above would yield different scores for how well they fit the training data. The model with the lowest score is the one that best fits the training data.\nOnce the model is trained, the test set is used to evaluate the model.\nThe internal evaluation metrics for linear regression during training are similar to the ones used in extrinsic evaluation on the test set.\nThe most common evaluation metrics for linear regression are as follows:\n\nResiduals\nResiduals are the difference between the true values of y and the predicted values of y.\n\\[\\text{Residual}_i = y_i - \\hat{y}_i\\]\nwhere \\(y_i\\) is the \\(i^{th}\\) true value of the target variable and \\(\\hat{y_i}\\) is the \\(i^{th}\\) predicted value of the target variable i.e. \\[\\hat{y_i} = m x_i + b\\]\n\ndf = pd.DataFrame()\ndf['y'] = y_train\ndf['model1'] = model1\ndf['model2'] = model2\n\nsns.scatterplot(x=X_train, y=y_train);\nsns.scatterplot(x=X_train, y=model1.values);\ndf.apply(lambda x: plt.plot((x.name, x.name), (x['y'], x['model1']), color='red', linewidth=1), axis=1);\nplt.legend(['Train set', 'Model 1', 'Residuals']);\n\n\n\n\n\n\n\n\n\nsns.scatterplot(x=X_train, y=y_train);\nsns.scatterplot(x=X_train, y=model2.values, color='green');\ndf.apply(lambda x: plt.plot((x.name, x.name), (x['y'], x['model2']), color='red', linewidth=1), axis=1);\nplt.legend(['Train set', 'Model 2', 'Residuals']);\n\n\n\n\n\n\n\n\n\n\nMean Absolute Error\nThe Mean Absolute Error (or MAE) is the average of the absolute differences between predictions and actual values. It gives an idea of how wrong the predictions were. The measure gives an idea of the magnitude of the error, but no idea of the direction (e.g. over or under predicting).\n\\[MAE = \\frac{1}{n}\\sum_{i=1}^{n}|\\text{residual}_i|\\]\nor\n\\[MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|\\]\nIn the code cell below, two linear functions are plotted against the training data. Both implement the same linear function \\(f(x) = mx + b\\) but with different values of \\(m\\) and \\(b\\).\n\nsns.scatterplot(x=X_train, y=y_train,  label='Train Set');\n\ndef mae(y, y_hat):\n    return sum(abs(y - y_hat)) / len(y)\n\nb = -5.5\nm = 0.0101\nmodel1 = m * X_train + b\nmae_model1 = mae(y_train, model1)\nsns.scatterplot(x=X_train, y=model1.values,  label=r'Model 1 (MAE$_{~train}$ = %s)' % round(mae_model1, 2));\n\n\nb = 14.5\nm = 0\nmodel2 = m * X_train + b\nmae_model2 = mae(y_train, model2)\nsns.scatterplot(x=X_train, y=model2.values,  label=r'Model 2 (MAE$_{~train}$ = %s)' % round(mae_model2, 2));\n\n\n\n\n\n\n\n\nIn order to find the best values of \\(m\\) and \\(b\\), we need to define an evaluation metric that we want to minimize.\nThe code cell below plots model 1 and model 2 against the test set and also calculates the MAE for each model.\n\nsns.scatterplot(x=X_train, y=y_train,  label='Test Set');\n\ndef mae(y, y_hat):\n    return sum(abs(y - y_hat)) / len(y)\n\nb = -5.5\nm = 0.0101\nmodel1 = m * X_test + b\nmae_model1 = mae(y_test, model1)\nsns.lineplot(x=X_test, y=model1.values, linewidth=3,\\\n             label=r'Model 1 (MAE$_{~test}$ = %s)' % round(mae_model1, 2), color='orange');\n\n\nb = 14.5\nm = 0\nmodel2 = m * X_test + b\nmae_model2 = mae(y_test, model2)\nsns.lineplot(x=X_test, y=model2.values, linewidth=3,  \\\n             label=r'Model 2 (MAE$_{~test}$ = %s)' % round(mae_model2, 2), color='green');\n\n\n\n\n\n\n\n\n\n\nMean Squared Error\nThe Mean Squared Error (or MSE) is much like the mean absolute error in that it provides a gross idea of the magnitude of error.\n\\[MSE = \\frac{1}{n}\\sum_{i=1}^{n}(\\text{Residual}_i)^2\\]\nor\n\\[MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\\]\n\ndef mse(y, y_hat):\n    return sum((y - y_hat)**2) / len(y)\n\nmse1 = mse(df['y'], df['model1'])\nmse2 = mse(df['y'], df['model2'])\n\nprint(\" MSE model 1: \", round(mse1, 2), \"\\n\", \"MSE model 2: \", round(mse2, 2))\n\n MSE model 1:  0.17 \n MSE model 2:  0.37\n\n\n\n\nRoot Mean Squared Error\nTaking the square root of the mean squared error converts the units back to the original units of the output variable and can be meaningful for description and presentation. This is called the Root Mean Squared Error (or RMSE).\n\\[RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(\\text{Residual}_i)^2}\\]\nor\n\\[RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}\\]\n\ndef rmse(y, y_hat):\n    return (sum((y - y_hat)**2) / len(y))**(1/2)\n\nmse1 = rmse(df['y'], df['model1'])\nmse2 = rmse(df['y'], df['model2'])\n\nprint(\" RMSE model 1: \", round(mse1, 2), \"\\n\", \"RMSE model 2: \", round(mse2, 2))\n\n RMSE model 1:  0.41 \n RMSE model 2:  0.61\n\n\n\n\n\\(\\text{R}^2\\)\nThe \\(\\text{R}^2\\) (or R Squared) metric provides an indication of the goodness of fit of a set of predictions to the actual values. In statistical literature, this measure is called the coefficient of determination. This is a value between 0 and 1 for no-fit and perfect fit respectively.\n\\[\\text{R}^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\\]\nwhere \\(\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n}y_i\\) is the mean of the observed data.\n\ndef r2(y, y_hat):\n    return 1 - sum((y - y_hat)**2) / sum((y - y.mean())**2)\n\nr2_1 = r2(df['y'], df['model1'])\nr2_2 = r2(df['y'], df['model2'])\n\nprint(\" R2 model 1: \", round(r2_1, 2), \"\\n\", \"R2 model 2: \", round(r2_2, 2))\n\n R2 model 1:  0.36 \n R2 model 2:  -0.4",
    "crumbs": [
      "Home",
      "Regression"
    ]
  },
  {
    "objectID": "beyond/42_regression.html#multivariate-regression",
    "href": "beyond/42_regression.html#multivariate-regression",
    "title": "Regression",
    "section": "Multivariate Regression",
    "text": "Multivariate Regression\nMultiple linear regression (MLR), also known simply as multiple regression, uses multiple (&gt; 1) input variables (\\(X\\)) to predict the outcome of a target variable (\\(y \\in \\mathbb{R}\\)) by fitting a linear equation to observed data.\n\\[\\mathbf{y} = f(\\mathbf{X})\\]\nHere \\(f\\) is a linear function of the form:\n\\[f(\\mathbf{X}) = \\mathbf{X}\\mathbf{m} + b\\]\nwhere \\(X\\) is a matrix of \\(N\\) observations and \\(D\\) features and \\(y\\) is a vector of \\(N\\) observations.\n\\[X = \\begin{bmatrix}\nx_{11} & x_{12} & \\dots & x_{1D} \\\\\nx_{21} & x_{22} & \\dots & x_{2D} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{N1} & x_{N2} & \\dots & x_{ND} \\\\\n\\end{bmatrix}\\]\n\\[y = \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_N \\\\\n\\end{bmatrix}\\]\nwhere \\(\\mathbf{m}\\) is a vector of \\(D\\) slopes and \\(b\\) is the y-intercept. i.e. \n\\[\\mathbf{m} = \\begin{bmatrix}\nm_1 \\\\\nm_2 \\\\\n\\vdots \\\\\nm_D \\\\\n\\end{bmatrix}\\]\nPutting it all together, we get:\n\\[\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_N \\\\\n\\end{bmatrix} = \\begin{bmatrix}\nx_{11} & x_{12} & \\dots & x_{1D} \\\\\nx_{21} & x_{22} & \\dots & x_{2D} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{N1} & x_{N2} & \\dots & x_{ND} \\\\\n\\end{bmatrix} \\begin{bmatrix}\nm_1 \\\\\nm_2 \\\\\n\\vdots \\\\\nm_D \\\\\n\\end{bmatrix} + b\\]\nThe goal of multiple linear regression is to find the values of \\(m_1, m_2, \\dots, m_D\\) and \\(b\\) that best fit the data.\n\nNow let’s load a dataset and try to find the best values of \\(m_1\\) and \\(b\\) that fit the data.\n\nThe code below uses data from California Housing Dataset to predict the median house value in California districts given the following input variables:\n\nMedInc: Median income in block.\nHouseAge: Median house age within a block (measured in years).\nAveRooms: Average number of rooms within a block of houses.\nAveBedrms: Average number of bedrooms within a block of houses.\nPopulation: Total number of people residing within a block.\nAveOccup: Average number of people occupying each house within a block.\nLongitude: A measure of how far west a house is; a higher value is farther west.\nLatitude: A measure of how far north a house is; a higher value is farther north.\n\nThe target variable is:\n\nMedian house value for households within a block (measured in US Dollars).\n\n\nimport pandas as pd \nfrom sklearn import  datasets\n\ncalifornia = datasets.fetch_california_housing()\nX = pd.DataFrame(california.data, columns=california.feature_names)\ny = pd.Series(california.target, name='Price')\n\n\nX.shape\n\n(20640, 8)\n\n\n\nX.head()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n\n\n\n\n\n\n\n\ny.head()\n\n0    4.526\n1    3.585\n2    3.521\n3    3.413\n4    3.422\nName: Price, dtype: float64\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nmodel = LinearRegression()\n\nmodel.fit(X_train, y_train)\n\ny_hat = model.predict(X_test)\n\nprint(\"MAE: \",  round(mean_absolute_error(y_test, y_hat), 2))\nprint(\"MSE: \",  round(mean_squared_error(y_test, y_hat), 2))\nprint(\"RMSE:\",  round((mean_squared_error(y_test, y_hat))**(1/2), 2))\nprint(\"R2:  \",  round(r2_score(y_test, y_hat), 2))\n\nMAE:  0.54\nMSE:  0.54\nRMSE: 0.73\nR2:   0.61\n\n\nChoosing between these metrics depends on the specific context of the problem:\n\nUse MAE if you want a metric that’s easy to understand and not influenced much by outliers.\nUse RMSE when larger errors should be penalized more heavily and a metric in the same unit as the target variable is desired.\nUse MSE when optimizing models since it emphasizes larger errors, making it useful in minimizing those errors during training\n\n\nresults = pd.DataFrame()\nresults['Actual'] = y_test\nresults['Prediction'] = y_hat\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nplt.figure(figsize=(10, 3))\nsns.scatterplot(x='Actual', y='Prediction', data=results);",
    "crumbs": [
      "Home",
      "Regression"
    ]
  },
  {
    "objectID": "beyond/42_regression.html#interpreting-the-model",
    "href": "beyond/42_regression.html#interpreting-the-model",
    "title": "Regression",
    "section": "Interpreting the Model",
    "text": "Interpreting the Model\nThe model we have trained is a linear function of the form:\n\\[\\text{MedianHouseValue} = (m_1 \\times \\text{MedInc}) + (m_2 \\times \\text{HouseAge}) + \\\\(m_3 \\times \\text{AveRooms}) + (m_4 \\times \\text{AveBedrms}) + \\\\(m_5 \\times \\text{Population}) + (m_6 \\times \\text{AveOccup}) + \\\\(m_7 \\times \\text{Longitude}) + (m_8 \\times \\text{Latitude}) + b\\]\nwhere \\(m_1, m_2, \\dots, m_8\\) are the slopes and \\(b\\) is the y-intercept.\nThe slopes \\(m_1, m_2, \\dots, m_8\\) tell us how much the target variable changes when the corresponding input variable changes by 1 unit.\nThe code cell below plots the slopes in decreasing order of magnitude.\n\nweights = pd.Series(model.coef_, index=X.columns)\nweights = weights.sort_values(ascending=False)\nsns.barplot(x=weights.index, y=weights.values, palette='bwr');\nplt.xticks(rotation=90);\n\n\n\n\n\n\n\n\nThe plot indicates that AveBedrms, MedInc and AveRooms have the highest positive relationship with the Price of the house whereas AveRooms, Latitude, Longitude have the strongest negative relationship with the target variable Price of the house.",
    "crumbs": [
      "Home",
      "Regression"
    ]
  },
  {
    "objectID": "beyond/42_regression.html#polynomial-regression",
    "href": "beyond/42_regression.html#polynomial-regression",
    "title": "Regression",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\nPolynomial functions are functions that have the form:\n\\[f(x) = b + m_1 x + m_2 x^2 + m_3 x^3 + ... + m_n x^n\\]\nwhere \\(b, m_1, m_2, m_3, ..., w_n\\) are the coefficients of the polynomial function and \\(n\\) is called the degree of the polynomial. In other words, the degree of a polynomial function is the highest power of the variable in the polynomial function.\nNote that the linear function \\(f(x) = mx + b\\) is a special case of the polynomial function. More specifically, a linear function is a polynomial function of degree 1.\nPolynomial functions of degree 2 or higher are called non-linear functions. As the degree of the polynomial function increases, the function becomes more flexible and can fit more complex patterns in the data.\nIf we have only one input variable \\(x\\) to predict the output variable \\(y\\), then the polynomial function becomes:\n\\[y = f(x) = b + m_1 x + m_2 x^2 + m_3 x^3 + ... + m_n x^n\\]\nIn matrix notation, polynomial regression can be written as:\n\\[f(\\mathbf{x}) = \\mathbf{X}\\mathbf{m} + b\\]\nwhere \\(\\mathbf{X}\\) is a matrix of \\(N\\) observations and each feature is raised to a power from 1 to \\(D\\).\n\\[\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_N \\\\\n\\end{bmatrix}  = \\begin{bmatrix}\nx_1 & x_1^2 & x_1^3 & \\dots & x_1^D \\\\\nx_2 & x_2^2 & x_2^3 & \\dots & x_2^D \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_N & x_N^2 & x_N^3 & \\dots & x_N^D \\\\\n\\end{bmatrix} \\cdot \\begin{bmatrix}\nm_1 \\\\\nm_2 \\\\\n\\vdots \\\\\nm_D \\\\\n\\end{bmatrix} + b\\]\n\nCode Example\nLet’s implement polynomial regression on the Average Land Temperature dataset.\n\nimport pandas as pd\n\ndata         = pd.read_csv('../data/GlobalLandTemperaturesByCountry.csv')\ndata['dt']   = pd.to_datetime(data['dt'])\ndata['year'] = data['dt'].apply(lambda x: x.year)\ndata['month']= data['dt'].apply(lambda x: x.month)\ndata         = data.dropna()\ndata         = data[(data['year'] &gt;= 1900) & (data['month'] == 1)]\navgs         = data.groupby('year').mean()['AverageTemperature']\navgs.name    = 'Average Temperature (C)'\n\nsns.scatterplot(x=avgs.index, y=avgs);\n\n\n\n\n\n\n\n\n\nX = avgs.index.values.reshape(-1, 1)\ny = avgs\n\n\nannual_means = data.groupby('year').mean()['AverageTemperature'][::10]\n\nX = annual_means.index.values.reshape(-1, 1)\ny = annual_means.values\n\n\ntrain_set = avgs.sample(int(0.8*len(avgs)), random_state=42).sort_index()\ntest_set  = avgs.drop(train_set.index).sort_index()\n\nX_train   = train_set.index.values.reshape(-1, 1)\ny_train   = train_set.values.reshape(-1, 1)\n\nX_test    = test_set.index.values.reshape(-1, 1)\ny_test    = test_set.values.reshape(-1, 1)\n\nImplementing polynomial regression in sklearn is similar to linear regression but with one additional step. We need to transform the input data into a polynomial matrix before fitting the model. In sklearn, this is done using the PolynomialFeatures class.\nThe code cell below implements polynomial regression of degrees 1, 2 and 5 on the Average Land Temperature dataset.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\nplt.figure(figsize=(5, 5))\nplt.scatter(X_train, y_train, label='X', alpha=0.7);\n\ncolors = ['orange', 'green', 'red']\n\nfor i, degree in enumerate([1, 2, 5]):\n\n    # Create polynomial features for X_train and X_test\n    poly         = PolynomialFeatures(degree=degree)\n    X_train_poly = poly.fit_transform(X_train)\n    X_test_poly  = poly.fit_transform(X_test)\n\n    # Fit a linear regression model to the training data\n    model        = LinearRegression()\n    model.fit(X_train_poly, y_train)\n\n    # Predict y values for X_test\n    y_pred       = model.predict(X_test_poly)\n    \n    # Plot the predictions\n    plt.plot(X_test, y_pred, linewidth=3, label='Degree = %s' % degree, alpha=0.7, color=colors[i]);\n\nplt.legend();\n\n\n\n\n\n\n\n\nNote that with increasing degree, the polynomial function can fit more complex patterns non-linear trends in the data.",
    "crumbs": [
      "Home",
      "Regression"
    ]
  },
  {
    "objectID": "beyond/42_regression.html#underfitting-vs.-overfitting",
    "href": "beyond/42_regression.html#underfitting-vs.-overfitting",
    "title": "Regression",
    "section": "Underfitting vs. Overfitting",
    "text": "Underfitting vs. Overfitting\nThis example demonstrates the problems of underfitting and overfitting and how we can use linear regression with polynomial features to approximate nonlinear functions.\nThe plot shows the function that we want to approximate, which is a part of the cosine function. In addition, the samples from the real function and the approximations of different models are displayed. The models have polynomial features of different degrees. We can see that a linear function (polynomial with degree 1) is not sufficient to fit the training samples. This is called underfitting.\nA polynomial of degree 4 approximates the true function almost perfectly. However, for higher degrees the model will overfit the training data, i.e. it learns the noise of the training data. This is called overfitting. We evaluate quantitatively overfitting / underfitting by using cross-validation.\nWe calculate the mean squared error (MSE) on the validation set, the higher, the less likely the model generalizes correctly from the training data.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\ndef true_fun(X):\n    return np.cos(1.5 * np.pi * X)\n\nnp.random.seed(0)\n\nn_samples = 30\ndegrees = [1, 4, 15]\n\nX = np.sort(np.random.rand(n_samples))\ny = true_fun(X) + np.random.randn(n_samples) * 0.1\n\n\nax = sns.scatterplot(x=X, y=y, label='Samples');\nax.set(xlabel='X', ylabel='y', title = r'$y = cos(1.5 \\pi  x) + \\epsilon$');\n\nsns.lineplot(x=X, y=true_fun(X), label='True function', color='blue');\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(14, 5))\nfor i in range(len(degrees)):\n    ax = plt.subplot(1, len(degrees), i + 1)\n    plt.setp(ax, xticks=(), yticks=())\n\n    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)\n    linear_regression = LinearRegression()\n    pipeline = Pipeline(\n        [\n            (\"polynomial_features\", polynomial_features),\n            (\"linear_regression\", linear_regression),\n        ]\n    )\n    pipeline.fit(X[:, np.newaxis], y)\n\n    # Evaluate the models using crossvalidation\n    scores = cross_val_score(\n        pipeline, X[:, np.newaxis], y, scoring=\"neg_mean_squared_error\", cv=10\n    )\n\n    X_test = np.linspace(0, 1, 100)\n    plt.scatter(X, y, edgecolor=\"b\", s=20, label=\"Samples\")\n    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model %s (degree = %s)\" % (i+1, degrees[i]))\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.xlim((0, 1))\n    plt.ylim((-2, 2))\n    plt.legend(loc=\"best\")\n    plt.title(\n        \"MSE = {:.2} +/- {:.2}\".format(\n            round(-scores.mean(), 2), round(scores.std(), 2)\n        )\n    )\n\nplt.suptitle(\"Polynomial Regression with increasing degrees, leading to overfitting\", fontsize=14);\nplt.show()\n\n\n\n\n\n\n\n\nIdeally, you want to strike a balance between underfitting (high training error, high testing error) and overfitting (low training error, high testing error) by picking a model complexity (number of parameters) that generalizes well to unseen data.\n\n\n\n\nNote that model complexity here refers to the number of parameters in the model. For example, univariate linear regression model has 2 parameters (slope and y-intercept) whereas a polynomial regression model of degree 2 has 3 parameters (slope, y-intercept and coefficient of \\(x^2\\)).",
    "crumbs": [
      "Home",
      "Regression"
    ]
  },
  {
    "objectID": "beyond/regression.html",
    "href": "beyond/regression.html",
    "title": "Regression",
    "section": "",
    "text": "Regression is similar to classification, but instead of predicting a class label, we predict a continuous value. Both regression and classification are types of supervised learning, and the difference between them is the type of output they produce. In regression, the output is a real number, while in classification, the output is a class label.\nIn this notebook, we will learn about regression and implement a simple linear regression model using the scikit-learn library."
  },
  {
    "objectID": "beyond/regression.html#table-of-contents",
    "href": "beyond/regression.html#table-of-contents",
    "title": "Regression",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nSimple Linear Regression\nMultiple Linear Regression\nPolynomial Regression"
  },
  {
    "objectID": "beyond/regression.html#simple-linear-regression",
    "href": "beyond/regression.html#simple-linear-regression",
    "title": "Regression",
    "section": "1. Simple Linear Regression",
    "text": "1. Simple Linear Regression"
  },
  {
    "objectID": "beyond/41_classification.html",
    "href": "beyond/41_classification.html",
    "title": "Classification",
    "section": "",
    "text": "Classification lies at the heart of both human and machine intelligence. Deciding what letter, word, or image has been presented to our senses, recognizing faces or voices, sorting mail, assigning grades to homeworks; these are all examples of assigning a category to an input.\nOne method for classification is to use handwritten rules. There are many areas of data mining where handwritten rule-based classifiers constitute a state-of-the-art system, or at least part of it. Rules can be fragile, however, as situations or data change over time, and for some tasks humans aren’t necessarily good at coming up with the rules. Most cases of classification therefore are instead done via supervised machine learning.\nClassification is the type of supervised learning where \\(\\mathcal{y}\\) is a discrete categorical variable.\nThe discrete output variable \\(\\mathcal{y}\\) is often also called the label or target or class.\nFor example, we might want to predict whether a patient has a disease or not, based on their symptoms. In this case, \\(\\mathcal{y}\\) is a binary variable, taking the value 1 if the patient has the disease, and 0 otherwise. Other examples of classification problems include predicting the sentiment of a movie review: positive, negative, or neutral.\nFor example,\nIn other words, the classification problem is to learn a function \\(f\\) that maps the input \\(\\mathcal{X}\\) to the discrete output \\(\\mathcal{Y}\\).",
    "crumbs": [
      "Home",
      "Classification"
    ]
  },
  {
    "objectID": "beyond/41_classification.html#evaluation-metrics",
    "href": "beyond/41_classification.html#evaluation-metrics",
    "title": "Classification",
    "section": "Evaluation Metrics",
    "text": "Evaluation Metrics\nThe most common metric for evaluating a classifier is accuracy. Accuracy is the proportion of correct predictions. It is the number of correct predictions divided by the total number of predictions.\n\\[Accuracy = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\\]\nFor example, if we have a test set of 100 documents, and our classifier correctly predicts the class of 80 of them, then the accuracy is 80%.\nAccuracy is a good metric when the classes are balanced \\(N_{class1} \\approx N_{class2}\\). However, when the classes are imbalanced, accuracy can be misleading. For example, if we have a test set of 100 documents, and 95 of them are positive and 5 of them are negative, then a classifier that always predicts positive will have an accuracy of 95%. However, this classifier is not useful, because it never predicts negative.\n\nMulti-class classification as multiple Binary classifications\nEvery multi-class classification problem can be decomposed into multiple binary classification problems. For example, if we have a multi-class classification problem with 3 classes, we can decompose it into 3 binary classification problems.\n\n\n \nAssuming the categorical variable that we are trying to predict is binary, we can define the accuracy in terms of the four possible outcomes of a binary classifier:\n\nTrue Positive (TP): The classifier correctly predicted the positive class.\nFalse Positive (FP): The classifier incorrectly predicted the negative class as positive.\nTrue Negative (TN): The classifier correctly predicted the negative class.\nFalse Negative (FN): The classifier incorrectly predicted the positive class as negative.\n\nTrue positive means that the classifier correctly predicted the positive class. False positive means that the classifier incorrectly predicted the positive class. True negative means that the classifier correctly predicted the negative class. False negative means that the classifier incorrectly predicted the negative class.\nThese definitions are summarized in the table below:\n\n\n\n\nPrediction \\(\\hat{y} = f'(x)\\)\nTruth \\(y = f(x)\\)\n\n\n\n\nTrue Negative (TN)\n0\n0\n\n\nFalse Negative (FN)\n0\n1\n\n\nFalse Positive (FP)\n1\n0\n\n\nTrue Positive (TP)\n1\n1\n\n\n\nIn terms of the four outcomes above, the accuracy is:\n\\[ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\\]\nAccuracy is a useful metric, but it can be misleading.\nOther metrics that are often used to evaluate classifiers are:\n\nPrecision: The proportion of positive predictions that are correct. Mathematically, it is defined as:\n\n\\[\\text{Precision} = \\frac{TP}{TP + FP}\\]\n\nRecall: The proportion of positive instances that are correctly predicted. Mathematically, it is defined as:\n\n\\[\\text{Recall} = \\frac{TP}{TP + FN}\\]\nThe precision and recall are often combined into a single metric called the F1 score. The F1 score is the harmonic mean of precision and recall. The harmonic mean of two numbers is given by:\n\nF1 Score: The harmonic mean of precision and recall.\n\n\\[\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\]\n\n\nMany kinds of machine learning algorithms are used to build classifiers. Two common classifiers are Naive Bayes and Logistic Regression.\nThese exemplify two primary category of models for doing classification:\n\nGenerative models like naive Bayes build a model of how a class could generate some input data. Given an observation, they return the class most likely to have generated the observation.\nDiscriminative models like logistic regression instead learn what features from the input are most useful to discriminate between the different possible classes.\n\nWhile discriminative systems are often more accurate and hence more commonly used, generative classifiers still have a role. They can be more robust to missing data, and can be used to generate synthetic data.\n\n\nCode Example\nThe code below shows how to train a Nearest Neighbor classifier on the Iris dataset. The Iris dataset is a dataset of 150 observations of iris flowers. There are 3 classes of iris flowers: setosa, versicolor, and virginica. For each observation, there are 4 features: sepal length, sepal width, petal length, and petal width. The goal is to predict the class of iris flower given the 4 features.\nThe code below uses the scikit-learn library to train a Nearest Neighbor classifier on the Iris dataset. The Nearest Neighbor classifier is a simple classifier that works by finding the training observation that is closest to the test observation, and predicting the class of the closest training observation. The Nearest Neighbor classifier is a discriminative classifier.\nThere are 5 steps shown in the code below:\n\nImport the dataset: The Iris dataset is included in scikit-learn. We import it using the load_iris function.\nSplit the dataset into training and test sets: We split the dataset into a training set and a test set. The training set is used to train the classifier, and the test set is used to evaluate the classifier.\nInstantiate the classifier: We instantiate the classifier using the KNeighborsClassifier class.\nTrain the classifier: We train the classifier using the fit method.\nMake predictions: We make predictions on the test set using the predict method.\nPrint the classification report: We evaluate the classifier using the classification_report function.\n\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\n\n# 1. Load the data\ndata = load_iris(as_frame=True)\nX    = data['data']\ny    = data['target']\n\n# 2. Create a train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# 3. Instantiate a model\nmodel = KNeighborsClassifier()\n\n# 4. Fit a model\nmodel.fit(X_train, y_train)\n\n# 5. Predict on the test set\npreds = model.predict(X_test)\n\n# 6. Print classification report\nprint(classification_report(y_test, preds))\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00         8\n           1       0.91      1.00      0.95        10\n           2       1.00      0.92      0.96        12\n\n    accuracy                           0.97        30\n   macro avg       0.97      0.97      0.97        30\nweighted avg       0.97      0.97      0.97        30",
    "crumbs": [
      "Home",
      "Classification"
    ]
  },
  {
    "objectID": "beyond/gc.html",
    "href": "beyond/gc.html",
    "title": "Garbage Collection",
    "section": "",
    "text": "Garbage collection is a form of automatic memory management. The garbage collector, or just collector, attempts to reclaim garbage, or memory occupied by objects that are no longer in use by the program.\nGarbage collection is important because it helps to prevent memory leaks. A memory leak occurs when a program allocates memory but does not deallocate it, leading to memory exhaustion.\nGarbage collection is also important because it helps to prevent memory fragmentation. Memory fragmentation occurs when the memory is divided into small, non-contiguous blocks, leading to inefficient memory usage.\nIn the context of data science and pandas, garbage collection is important because data science projects often involve working with large datasets, which can consume a significant amount of memory. If memory is not properly managed, it can lead to performance issues and crashes.\nIn Python, garbage collection is handled by the gc module. The gc module provides an interface to the optional garbage collector. It provides the ability to disable the collector, tune the collection frequency, and set debugging options.",
    "crumbs": [
      "Home",
      "Garbage Collection"
    ]
  },
  {
    "objectID": "beyond/gc.html#how-does-python-manage-memory",
    "href": "beyond/gc.html#how-does-python-manage-memory",
    "title": "Garbage Collection",
    "section": "How does Python manage memory?",
    "text": "How does Python manage memory?\nPython uses a private heap to manage memory. The heap holds all the objects and data structures created by the program. The heap is the memory space where all the objects are stored. The heap is managed by the Python memory manager.\nThe Python memory manager has different components:\n\nThe memory allocator: The memory allocator is responsible for allocating memory for objects and data structures.\nThe garbage collector: The garbage collector is responsible for reclaiming memory occupied by objects that are no longer in use by the program.\nThe memory deallocator: The memory deallocator is responsible for deallocating memory when an object is no longer needed.",
    "crumbs": [
      "Home",
      "Garbage Collection"
    ]
  },
  {
    "objectID": "beyond/gc.html#what-is-garbage-collection",
    "href": "beyond/gc.html#what-is-garbage-collection",
    "title": "Garbage Collection",
    "section": "What is garbage collection?",
    "text": "What is garbage collection?",
    "crumbs": [
      "Home",
      "Garbage Collection"
    ]
  },
  {
    "objectID": "beyond/gc.html#how-does-garbage-collection-work-in-python",
    "href": "beyond/gc.html#how-does-garbage-collection-work-in-python",
    "title": "Garbage Collection",
    "section": "How does garbage collection work in Python?",
    "text": "How does garbage collection work in Python?\nPython uses a reference counting mechanism to manage memory. Reference counting is a simple and efficient memory management technique. It works by keeping track of the number of references to an object. When an object is created, its reference count is set to one. When an object is referenced by another object, its reference count is incremented. When an object is no longer referenced, its reference count is decremented. When an object’s reference count reaches zero, the object is no longer in use by the program and can be reclaimed by the garbage collector.\nPython also uses a cyclic garbage collector to handle reference cycles. A reference cycle occurs when two or more objects reference each other in a circular manner. The cyclic garbage collector is responsible for detecting and breaking reference cycles.",
    "crumbs": [
      "Home",
      "Garbage Collection"
    ]
  },
  {
    "objectID": "beyond/gc.html#how-to-disable-garbage-collection",
    "href": "beyond/gc.html#how-to-disable-garbage-collection",
    "title": "Garbage Collection",
    "section": "How to disable garbage collection?",
    "text": "How to disable garbage collection?\nYou can disable garbage collection in Python using the gc module. The gc module provides an interface to the optional garbage collector. You can disable the garbage collector by setting the gc.disable() function.\nHere is an example that demonstrates how to disable garbage collection in Python:\n\nimport gc\n\ngc.disable()",
    "crumbs": [
      "Home",
      "Garbage Collection"
    ]
  },
  {
    "objectID": "beyond/gc.html#how-to-enable-garbage-collection",
    "href": "beyond/gc.html#how-to-enable-garbage-collection",
    "title": "Garbage Collection",
    "section": "How to enable garbage collection?",
    "text": "How to enable garbage collection?\nYou can enable garbage collection in Python using the gc module. The gc module provides an interface to the optional garbage collector. You can enable the garbage collector by setting the gc.enable() function.\nHere is an example that demonstrates how to enable garbage collection in Python:\n\nimport gc \n\ngc.enable()",
    "crumbs": [
      "Home",
      "Garbage Collection"
    ]
  },
  {
    "objectID": "beyond/gc.html#how-to-manually-run-garbage-collection",
    "href": "beyond/gc.html#how-to-manually-run-garbage-collection",
    "title": "Garbage Collection",
    "section": "How to manually run garbage collection?",
    "text": "How to manually run garbage collection?\nYou can manually run garbage collection in Python using the gc module. The gc module provides an interface to the optional garbage collector. You can manually run garbage collection by setting the gc.collect() function.\nHere is an example that demonstrates how to manually run garbage collection in Python:\n\nimport gc \n\ngc.collect()",
    "crumbs": [
      "Home",
      "Garbage Collection"
    ]
  },
  {
    "objectID": "beyond/gc.html#how-to-get-the-garbage-collection-statistics",
    "href": "beyond/gc.html#how-to-get-the-garbage-collection-statistics",
    "title": "Garbage Collection",
    "section": "How to get the garbage collection statistics?",
    "text": "How to get the garbage collection statistics?\nYou can get the garbage collection statistics in Python using the gc module. The gc module provides an interface to the optional garbage collector. You can get the garbage collection statistics by setting the gc.get_stats() function.\nHere is an example that demonstrates how to get the garbage collection statistics in Python:\n\nimport gc \n\ngc.get_stats()\n\ngc.get_stats() returns a list of dictionaries, where each dictionary contains the following keys:\n\ncollections: The number of garbage collections that have occurred.\ncollected: The total number of objects collected.\nuncollectable: The total number of uncollectable objects.\ngarbage: The total number of objects that are garbage.",
    "crumbs": [
      "Home",
      "Garbage Collection"
    ]
  },
  {
    "objectID": "beyond/dimensionality.html",
    "href": "beyond/dimensionality.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "Dimensionality reduction is the process of reducing the number of random variables under consideration by obtaining a set of principal variables. It can be divided into feature selection and feature extraction.\nFeature selection is the process of selecting a subset of relevant features for use in model construction. It is a common technique in machine learning, where it is often used to simplify the model and make it easier to interpret. Feature selection is also useful for eliminating irrelevant or redundant features that do not contribute to the predictive power of the model.\nFeature extraction is the process of transforming the data from a high-dimensional space into a lower-dimensional space. This is done by projecting the data onto a lower-dimensional subspace that captures the most important information in the data. Feature extraction is useful for reducing the computational complexity of the model and for visualizing the data in a more interpretable form.\nIn this notebook, we will explore some common techniques for dimensionality reduction, including principal component analysis (PCA), linear discriminant analysis (LDA), and t-distributed stochastic neighbor embedding (t-SNE). We will apply these techniques to a dataset and visualize the results to see how they can help us understand the structure of the data.\n\nPrincipal Component Analysis (PCA)\nPrincipal component analysis (PCA) is a technique for reducing the dimensionality of a dataset by projecting it onto a lower-dimensional subspace that captures the most important information in the data. PCA works by finding the principal components of the data, which are the directions in which the data varies the most. These principal components are orthogonal to each other and form a new coordinate system for the data.\nPCA is commonly used for dimensionality reduction in machine learning, as it can help to reduce the computational complexity of the model and improve its performance. PCA is also useful for visualizing high-dimensional data in a lower-dimensional space, as it can reveal the underlying structure of the data and help to identify patterns and relationships.\nIn this notebook, we will apply PCA to a dataset and visualize the results to see how it can help us understand the structure of the data.\n\n\nLinear Discriminant Analysis (LDA)\nLinear discriminant analysis (LDA) is a technique for dimensionality reduction that is commonly used in machine learning for classification tasks. LDA works by finding the directions in which the data is most separable into different classes, and projecting the data onto these directions.\nLDA is similar to PCA, but it is specifically designed for classification tasks. LDA aims to maximize the separation between different classes in the data, while PCA aims to capture the most important information in the data. LDA is useful for reducing the dimensionality of the data and improving the performance of classification models.\nIn this notebook, we will apply LDA to a dataset and visualize the results to see how it can help us understand the structure of the data and improve the performance of a classification model.\n\n\nt-Distributed Stochastic Neighbor Embedding (t-SNE)\nt-Distributed Stochastic Neighbor Embedding (t-SNE) is a technique for dimensionality reduction that is commonly used for visualizing high-dimensional data in a lower-dimensional space. t-SNE works by finding a low-dimensional representation of the data that preserves the local structure of the data points.\nt-SNE is useful for visualizing complex datasets that have a non-linear structure, as it can reveal patterns and relationships that are not apparent in the original data. t-SNE is commonly used for exploratory data analysis and for generating visualizations that help to understand the structure of the data.\nIn this notebook, we will apply t-SNE to a dataset and visualize the results to see how it can help us understand the structure of the data and identify patterns and relationships.\n\n\nDataset\nThe dataset we will use in this notebook is the Iris dataset, which is a classic dataset in machine learning. The Iris dataset contains 150 samples of iris flowers, each with four features: sepal length, sepal width, petal length, and petal width. The goal of the dataset is to classify the iris flowers into three species: setosa, versicolor, and virginica.\nThe Iris dataset is commonly used for classification tasks and for exploring dimensionality reduction techniques, as it is a small and well-known dataset that is easy to work with. In this notebook, we will apply PCA, LDA, and t-SNE to the Iris dataset and visualize the results to see how these techniques can help us understand the structure of the data.\n\n\nDimensionality Reduction"
  },
  {
    "objectID": "probability/conditional_prob.html",
    "href": "probability/conditional_prob.html",
    "title": "Conditional Probability",
    "section": "",
    "text": "Conditional probability is the probability of one event occurring given that another event has occurred.\nThe conditional probability is usually denoted by \\(P(A | B)\\) and is defined as:\n\\[ P(A | B) = \\frac{P(A, B)}{P(B)} \\]\nThe denominator is the marginal probability of \\(B\\).\n\n\nFor example, if we are flipping two coins, the conditional probability of flipping heads in the second toss, knowing the first toss was tails is:\n\n\n\nPossible world\n\\(\\text{Coin}_1\\)\n\\(\\text{Coin}_2\\)\n\\(P(\\omega)\\)\n\n\n\n\n\\(\\omega_1\\)\nH\nH\n0.25\n\n\n\\(\\omega_2\\)\nH\nT\n0.25\n\n\n\\(\\omega_3\\)\nT\nH\n0.25\n\n\n\\(\\omega_4\\)\nT\nT\n0.25\n\n\n\n\\[ P(\\text{Coin}_2 = H | \\text{Coin}_1 = T) = \\frac{P(\\text{Coin}_2 = H, \\text{Coin}_1 = T)}{P(\\text{Coin}_1 = T)} = \\frac{0.25}{0.5} = 0.5 \\]\n\nmerged\n\n\n\n\n\n\n\n\nC\nD\nP(C, D)\nP(C)\nP(D)\nP(C) P(D)\n\n\n\n\n0\nH\n1\n0.24\n0.51\n0.43\n0.2193\n\n\n1\nT\n1\n0.19\n0.49\n0.43\n0.2107\n\n\n2\nH\n2\n0.13\n0.51\n0.22\n0.1122\n\n\n3\nT\n2\n0.09\n0.49\n0.22\n0.1078\n\n\n4\nH\n3\n0.09\n0.51\n0.22\n0.1122\n\n\n5\nT\n3\n0.13\n0.49\n0.22\n0.1078\n\n\n6\nH\n4\n0.01\n0.51\n0.05\n0.0255\n\n\n7\nT\n4\n0.04\n0.49\n0.05\n0.0245\n\n\n8\nH\n5\n0.03\n0.51\n0.03\n0.0153\n\n\n9\nT\n5\n0.00\n0.49\n0.03\n0.0147\n\n\n10\nH\n6\n0.01\n0.51\n0.05\n0.0255\n\n\n11\nT\n6\n0.04\n0.49\n0.05\n0.0245\n\n\n\n\n\n\n\n\nmerged['P(D | C)'] = merged['P(C, D)'] / merged['P(C)']\nmerged['P(D | C)']\n\n0     0.470588\n1     0.387755\n2     0.254902\n3     0.183673\n4     0.176471\n5     0.265306\n6     0.019608\n7     0.081633\n8     0.058824\n9     0.000000\n10    0.019608\n11    0.081633\nName: P(D | C), dtype: float64\n\n\n\nmerged[['C', 'D', 'P(D | C)']]\n\naxs = sns.catplot(data=merged, x=\"D\", y=\"P(D | C)\", hue=\"C\", kind=\"bar\");\naxs.set(title=\"Conditional probability distribution of D given C\\nNote that the blue bars add up to 1\");\n\n\n\n\n\n\n\n\nNote that the sum of conditional probabilites, unlike joint probability, is not 1.\n\nmerged[\"P(D | C)\"].sum()\n\n2.0\n\n\nThis is because\n\\[ \\sum_C \\sum_D P(D|C) = \\sum_D P(D|C=\\text{Heads}) + \\sum_D P(D|C=\\text{Tails}) \\]\nAnd \\(\\sum_D P(D|C=\\text{Heads})\\) and \\(\\sum_D P(D|C=\\text{Tails})\\) are individually probability distributions that each sum to 1, over different values of \\(D\\).\nIn other words, in the plot above, the blue bars add up to 1 and the orange bars add up to 1.\n\nheads = merged[merged[\"C\"] == \"H\"]\ntails = merged[merged[\"C\"] == \"T\"]\n\nheads[\"P(D | C)\"].sum(), tails[\"P(D | C)\"].sum()\n\n(1.0, 1.0)",
    "crumbs": [
      "Home",
      "Conditional Probability"
    ]
  },
  {
    "objectID": "probability/conditional_prob.html#conditional-probability-pa-b",
    "href": "probability/conditional_prob.html#conditional-probability-pa-b",
    "title": "Conditional Probability",
    "section": "",
    "text": "Conditional probability is the probability of one event occurring given that another event has occurred.\nThe conditional probability is usually denoted by \\(P(A | B)\\) and is defined as:\n\\[ P(A | B) = \\frac{P(A, B)}{P(B)} \\]\nThe denominator is the marginal probability of \\(B\\).\n\n\nFor example, if we are flipping two coins, the conditional probability of flipping heads in the second toss, knowing the first toss was tails is:\n\n\n\nPossible world\n\\(\\text{Coin}_1\\)\n\\(\\text{Coin}_2\\)\n\\(P(\\omega)\\)\n\n\n\n\n\\(\\omega_1\\)\nH\nH\n0.25\n\n\n\\(\\omega_2\\)\nH\nT\n0.25\n\n\n\\(\\omega_3\\)\nT\nH\n0.25\n\n\n\\(\\omega_4\\)\nT\nT\n0.25\n\n\n\n\\[ P(\\text{Coin}_2 = H | \\text{Coin}_1 = T) = \\frac{P(\\text{Coin}_2 = H, \\text{Coin}_1 = T)}{P(\\text{Coin}_1 = T)} = \\frac{0.25}{0.5} = 0.5 \\]\n\nmerged\n\n\n\n\n\n\n\n\nC\nD\nP(C, D)\nP(C)\nP(D)\nP(C) P(D)\n\n\n\n\n0\nH\n1\n0.24\n0.51\n0.43\n0.2193\n\n\n1\nT\n1\n0.19\n0.49\n0.43\n0.2107\n\n\n2\nH\n2\n0.13\n0.51\n0.22\n0.1122\n\n\n3\nT\n2\n0.09\n0.49\n0.22\n0.1078\n\n\n4\nH\n3\n0.09\n0.51\n0.22\n0.1122\n\n\n5\nT\n3\n0.13\n0.49\n0.22\n0.1078\n\n\n6\nH\n4\n0.01\n0.51\n0.05\n0.0255\n\n\n7\nT\n4\n0.04\n0.49\n0.05\n0.0245\n\n\n8\nH\n5\n0.03\n0.51\n0.03\n0.0153\n\n\n9\nT\n5\n0.00\n0.49\n0.03\n0.0147\n\n\n10\nH\n6\n0.01\n0.51\n0.05\n0.0255\n\n\n11\nT\n6\n0.04\n0.49\n0.05\n0.0245\n\n\n\n\n\n\n\n\nmerged['P(D | C)'] = merged['P(C, D)'] / merged['P(C)']\nmerged['P(D | C)']\n\n0     0.470588\n1     0.387755\n2     0.254902\n3     0.183673\n4     0.176471\n5     0.265306\n6     0.019608\n7     0.081633\n8     0.058824\n9     0.000000\n10    0.019608\n11    0.081633\nName: P(D | C), dtype: float64\n\n\n\nmerged[['C', 'D', 'P(D | C)']]\n\naxs = sns.catplot(data=merged, x=\"D\", y=\"P(D | C)\", hue=\"C\", kind=\"bar\");\naxs.set(title=\"Conditional probability distribution of D given C\\nNote that the blue bars add up to 1\");\n\n\n\n\n\n\n\n\nNote that the sum of conditional probabilites, unlike joint probability, is not 1.\n\nmerged[\"P(D | C)\"].sum()\n\n2.0\n\n\nThis is because\n\\[ \\sum_C \\sum_D P(D|C) = \\sum_D P(D|C=\\text{Heads}) + \\sum_D P(D|C=\\text{Tails}) \\]\nAnd \\(\\sum_D P(D|C=\\text{Heads})\\) and \\(\\sum_D P(D|C=\\text{Tails})\\) are individually probability distributions that each sum to 1, over different values of \\(D\\).\nIn other words, in the plot above, the blue bars add up to 1 and the orange bars add up to 1.\n\nheads = merged[merged[\"C\"] == \"H\"]\ntails = merged[merged[\"C\"] == \"T\"]\n\nheads[\"P(D | C)\"].sum(), tails[\"P(D | C)\"].sum()\n\n(1.0, 1.0)",
    "crumbs": [
      "Home",
      "Conditional Probability"
    ]
  },
  {
    "objectID": "probability/conditional_prob.html#product-rule-pa-b",
    "href": "probability/conditional_prob.html#product-rule-pa-b",
    "title": "Conditional Probability",
    "section": "Product Rule \\(P(A, B)\\)",
    "text": "Product Rule \\(P(A, B)\\)\nRearranging the definition of conditional probability, we get the product rule:\n\\[ P(A, B) = P(A | B) \\cdot P(B) \\]\nSimilarly, we can also write:\n\\[ P(A, B) = P(B | A) \\cdot P(A)\\]\nIn summary,\n\\[ P(A, B) = P(A | B) \\cdot P(B) = P(B | A) \\cdot P(A)\\]",
    "crumbs": [
      "Home",
      "Conditional Probability"
    ]
  },
  {
    "objectID": "probability/conditional_prob.html#chain-rule-pa-b-c",
    "href": "probability/conditional_prob.html#chain-rule-pa-b-c",
    "title": "Conditional Probability",
    "section": "Chain Rule \\(P(A, B, C)\\)",
    "text": "Chain Rule \\(P(A, B, C)\\)\nThe chain rule is a generalization of the product rule to more than two events.\n$ P(A, B, C) = P(A | B, C) P(B, C) $\n\\(P(A, B, C) = P(A | B, C) \\cdot P(B | C) \\cdot P(C)\\)\nsince \\(P(B, C) = P(B | C) \\cdot P(C)\\) as per the product rule.\nChain rule essentially allows expressing the joint probability of multiple random variables as a product of conditional probabilities. This is useful because conditional probabilities are often easier to estimate from data than joint probabilities.",
    "crumbs": [
      "Home",
      "Conditional Probability"
    ]
  },
  {
    "objectID": "probability/conditional_prob.html#inclusion-exclusion-principle-pa-vee-b",
    "href": "probability/conditional_prob.html#inclusion-exclusion-principle-pa-vee-b",
    "title": "Conditional Probability",
    "section": "Inclusion-Exclusion Principle \\(P(A \\vee B)\\)",
    "text": "Inclusion-Exclusion Principle \\(P(A \\vee B)\\)\nInclusion-Exclusion Principle is a way of calculating the probability of two events occurring i.e. $ P(A=a  B=b) $ denoted generally as \\(P(A = a \\vee B = b)\\).\nIt is defined as:\n\\[ P(A = a \\vee B = b) = P(A = a) + P(B = b) - P(A = a \\wedge B = b) \\]\n\nFor example, if we are rolling two dice, the Inclusion-Exclusion Principle can be used to calculate the probability of rolling a 1 on the first die or a 2 on the second die.\n$P(_1=H _2=T) $\n$ = P(_2=H) + P(_1=T) - P(_2=H ∧ _1=T)$\n$ = 0.5 + 0.5 - 0.25 $\n$ = 0.75$",
    "crumbs": [
      "Home",
      "Conditional Probability"
    ]
  },
  {
    "objectID": "probability/conditional_prob.html#bayes-theorem-pab",
    "href": "probability/conditional_prob.html#bayes-theorem-pab",
    "title": "Conditional Probability",
    "section": "Bayes Theorem \\(P(A|B)\\)",
    "text": "Bayes Theorem \\(P(A|B)\\)\nBayes theorem is a way of calculating conditional probability. For example, if we are rolling two dice, Bayes theorem can be used to calculate the probability of rolling a 1 on the first die given that we rolled a 2 on the second die.\n\\[ P(A | B) = \\frac{P(B | A) \\cdot P(A)}{P(B)} \\]\n\\(P(A|B)\\) in the context of Bayes theorem is called the Posterior probability.\n\\(P(B|A)\\) is called the Likelihood.\n\\(P(A)\\) is called the Prior probability.\n\\(P(B)\\) is called the Evidence, also known as Marginal Likelihood.\n\\[ P(\\text{Posterior}) = \\frac{P(\\text{Likelihood})\\cdot P(\\text{Prior})}{P(\\text{Evidence})}\\]\n\n\n\n\nBayes Theorem allows a formal method of updating prior beliefs with new evidence and is the foundation of Bayesian Statistics. We will talk more about this when we talk about Statistics.\nIn machine learning, the task is often to find \\(P(Y | X_1 = x_1, X_2 = x_2, \\ldots X_D = x_D)\\) i.e. the probability of an unknown Y, given some values for \\(D\\) features (\\(X_1, X_2 \\ldots X_D\\)). Bayes theorem allows us to calculate this probability from the data.\nLet’s assume we are interested in predicting if a person is a football player (\\(Y_F=1\\)) or not (\\(Y_F=0\\)), given their height (\\(X_H\\)) and weight (\\(X_W\\)).\nSay, we observe a person who is 7 feet tall and weighs 200 pounds. We can use Bayes theorem to calculate the probability of this person being a football player using the following equation:\n\\(P(Y | X_H = 7, X_W = 200) = \\frac{P(X_H = 7, X_W = 200 | Y_F) \\cdot P(Y_F)}{P(X_H = 7, X_W = 200)}\\)\nNote that here \\(P(X_H = 7, X_W = 200 | Y_F)\\) is the Likelihood probability of observing someone who is 7 feet tall and weighs 200 pounds, knowing if they are a football player.\n\\(P(Y_F)\\) is the Prior probability of a person being a football player out of the entire population.\n\\(P(X_H = 7, X_W = 200)\\) is the probability of the Evidence i.e. probability of observing anyone who is 7 feet tall and weighs 200 pounds in the entire population.",
    "crumbs": [
      "Home",
      "Conditional Probability"
    ]
  },
  {
    "objectID": "probability/probability.html",
    "href": "probability/probability.html",
    "title": "Probability",
    "section": "",
    "text": "Probability allows us to talk about uncertainty, in certain terms. Once, we are able to quantify uncertainties, we can deterministically make deductions about the future. The language of statistics also allows us to talk about uncertainty in uncertain but tractable terms that we can reason about.\nIn the context of probability, note that we are going to be concerned with column vectors more than row vectors. In a pandas DataFrame, these column vectors are the columns of the DataFrame aka features.\nimport pandas as pd\n\ndata = pd.read_csv(\"../data/Shark Tank US dataset.csv\")\n\ndata = data[data.columns[:30]]\ndata.head(2)\n\n\n\n\n\n\n\n\nSeason Number\nStartup Name\nEpisode Number\nPitch Number\nSeason Start\nSeason End\nOriginal Air Date\nIndustry\nBusiness Description\nCompany Website\n...\nGot Deal\nTotal Deal Amount\nTotal Deal Equity\nDeal Valuation\nNumber of Sharks in Deal\nInvestment Amount Per Shark\nEquity Per Shark\nRoyalty Deal\nAdvisory Shares Equity\nLoan\n\n\n\n\n0\n1\nAvaTheElephant\n1\n1\n9-Aug-09\n5-Feb-10\n9-Aug-09\nHealth/Wellness\nAva The Elephant - Baby and Child Care\nhttp://www.avatheelephant.com/\n...\n1\n50000.0\n55.0\n90909.0\n1.0\n50000.0\n55.0\nNaN\nNaN\nNaN\n\n\n1\n1\nMrTod'sPieFactory\n1\n2\n9-Aug-09\n5-Feb-10\n9-Aug-09\nFood and Beverage\nMr. Tod's Pie Factory - Specialty Food\nhttp://whybake.com/\n...\n1\n460000.0\n50.0\n920000.0\n2.0\n230000.0\n25.0\nNaN\nNaN\nNaN\n\n\n\n\n2 rows × 30 columns",
    "crumbs": [
      "Home",
      "Probability"
    ]
  },
  {
    "objectID": "probability/probability.html#random-variable",
    "href": "probability/probability.html#random-variable",
    "title": "Probability",
    "section": "Random Variable",
    "text": "Random Variable\nA random variable is a mathematical formalization of an abstract quantity that has some degree of uncertainty associated with the values it may take on. The set of all possible values that a random variable can take on is called its range.\nIn the context of pandas, we are going to model columns or features as random variables.\nJust as numerical features in a DataFrame can be either discrete or continuous, random variables can also be either discrete or continuous. The two types require different mathematical formalizations as we will see later.\nRandom variables are usually denoted by capital letters, such as \\(X\\) or \\(Y\\). The values that a random variable can take on are denoted by lower case letters, such as \\(x\\) or \\(y\\).\nIt is also important to remember that \\(x\\) is a single value but \\(X\\) is a collection of values (i.e. pd.Series).\nIn the example below, Got Deal and Deal Valuation are two random variables in data\n\ndata[['Got Deal', 'Deal Valuation']]\n\n\n\n\n\n\n\n\nGot Deal\nDeal Valuation\n\n\n\n\n0\n1\n90909.0\n\n\n1\n1\n920000.0\n\n\n2\n0\nNaN\n\n\n3\n0\nNaN\n\n\n4\n0\nNaN\n\n\n...\n...\n...\n\n\n1360\n1\n3125000.0\n\n\n1361\n0\nNaN\n\n\n1362\n0\nNaN\n\n\n1363\n1\n3000000.0\n\n\n1364\n1\n15000000.0\n\n\n\n\n1365 rows × 2 columns\n\n\n\nThe ranges of \\(C\\) and \\(D\\) are \\(\\{H, T\\}\\) and \\(\\{1, 2, 3, 4, 5, 6\\}\\) respectively. It is worth repeating for emphasis that the ranges of the two variables is independent of observed data, since the observed data is a limited sample.",
    "crumbs": [
      "Home",
      "Probability"
    ]
  },
  {
    "objectID": "probability/probability.html#experiment-outcome-omega-and-sample-space-omega",
    "href": "probability/probability.html#experiment-outcome-omega-and-sample-space-omega",
    "title": "Probability",
    "section": "Experiment, Outcome \\(\\omega\\) and Sample Space \\(\\Omega\\)",
    "text": "Experiment, Outcome \\(\\omega\\) and Sample Space \\(\\Omega\\)\nAn outcome, denoted by \\(\\omega\\), is the set of values that one or more random variables take on as a result of an experiment.\nAn experiment is a process that yields outcomes out of set of all possible outcomes.\nThe sample space, denoted by \\(\\Omega\\), is the set of all possible outcomes.\nThe important operative word here is “possible”. The sample space is not the set of all observed outcomes, the set of all possible outcomes.\nIf an experiment involves two random variables say \\(X\\) and \\(Y\\) which can take on \\(n\\) possible values (i.e. \\(~\\text{range}_X = \\{x_1, x_2, \\ldots, x_n\\})\\) and \\(m\\) possible values (i.e. \\(~\\text{range}_Y = \\{y_1, y_2, \\ldots, y_m\\}\\)) respectively, then the sample space \\(\\Omega\\) is the set of all possible combinations of \\(x_i\\) and \\(y_j\\) and is of size \\(n \\times m\\).\n\n\n\n\n\n\\(\\omega_i\\)\n\\(X\\)\n\\(Y\\)\n\n\n\n\n\\(\\omega_1\\)\n\\(x_1\\)\n\\(y_1\\)\n\n\n\\(\\omega_2\\)\n\\(x_1\\)\n\\(y_2\\)\n\n\n:\n:\n:\n\n\n\\(\\omega_{m}\\)\n\\(x_1\\)\n\\(y_m\\)\n\n\n\\(\\omega_{m+1}\\)\n\\(x_2\\)\n\\(y_1\\)\n\n\n\\(\\omega_{m+2}\\)\n\\(x_2\\)\n\\(y_2\\)\n\n\n:\n:\n:\n\n\n\\(\\omega_{n \\times m}\\)\n\\(x_n\\)\n\\(y_m\\)\n\n\n\n\n\nIn other words, the sample space is the cross product of the ranges of all random variables involved in the experiment.\nIn our example, the experiment is the act of tossing a coin and rolling a dice.\nEach row in the data is an outcome \\(w_i\\) from the set of all possible outcomes \\(\\Omega\\).\nFor example, coin flip \\(C\\) variable can take on two (\\(n=2\\)) values: \\(\\{H, T\\}\\) and dice roll \\(D\\) variable can take on six \\(m=6\\) value: \\(\\{1, 2, 3, 4, 5, 6\\}\\). This means that the sample space \\(\\Omega\\) is of size \\(n \\times m = 2 \\times 6 = 12\\).",
    "crumbs": [
      "Home",
      "Probability"
    ]
  },
  {
    "objectID": "probability/probability.html#observed-sample-space",
    "href": "probability/probability.html#observed-sample-space",
    "title": "Probability",
    "section": "Observed Sample Space",
    "text": "Observed Sample Space\nIf we have a Series with many repeated values, then .unique() can be used to identify only the unique values.\nIt is important to note that in the real world, it is often impossible to obtain the range of a random variable. Since most real-world datasets are samples, df['X'].unique() does not necessarily give us the range of \\(X\\).\nHere we return an array of all the Industries in the Shark Tank dataset.\n\ndata['Industry'].unique()\n\narray(['Health/Wellness', 'Food and Beverage', 'Business Services',\n       'Lifestyle/Home', 'Software/Tech', 'Children/Education',\n       'Automotive', 'Fashion/Beauty', 'Media/Entertainment',\n       'Fitness/Sports/Outdoors', 'Pet Products', 'Electronics',\n       'Green/CleanTech', 'Travel', 'Uncertain/Other', 'Liquor/Alcohol'],\n      dtype=object)",
    "crumbs": [
      "Home",
      "Probability"
    ]
  },
  {
    "objectID": "probability/probability.html#frequency-distributions",
    "href": "probability/probability.html#frequency-distributions",
    "title": "Probability",
    "section": "Frequency Distributions",
    "text": "Frequency Distributions\nA frequency distribution is a table that shows the frequency of various values in a dataset. The frequency of a value is the number of times it appears in the dataset.\nThe Series.value_counts() methods counts the number of occurrence of each unique value in a Series. In other words, it counts the number of times each unique value appears. This is often useful for determining the most or least common entries in a Series.\nIn the example below, we can determine the frequency of startups that got a deal in the dataset.\n\ndata[\"Got Deal\"].value_counts()\n\n1    826\n0    539\nName: Got Deal, dtype: int64",
    "crumbs": [
      "Home",
      "Probability"
    ]
  },
  {
    "objectID": "probability/probability.html#probability-model-px",
    "href": "probability/probability.html#probability-model-px",
    "title": "Probability",
    "section": "Probability Model \\(P(X)\\)",
    "text": "Probability Model \\(P(X)\\)\nProbability model is a function that assigns a probability score \\(P(\\omega_i)\\) to each possible outcome \\(\\omega_i\\) for every \\(\\omega_i \\in \\Omega\\) such that\n\\[ 0 \\lt P(\\omega_i) \\lt 1 ~~~\\text{and}~~~ \\sum_{\\omega \\in \\Omega} P(\\omega_i) = 1\\]\nFor example, if we have a random variable \\(D\\) for rolling a die, the probability model assigns a probability to each number that we can roll. The probability model is usually denoted by \\(P(\\omega_i)\\) or \\(P(D=d)\\)\n\n\n\n\\(\\omega\\)\n\\(D\\)\n\\(P(D=d)\\)\n\n\n\n\n\\(\\omega_1\\)\n\\(1\\)\n\\(P(D=1)\\)\n\n\n\\(\\omega_2\\)\n\\(2\\)\n\\(P(D=2)\\)\n\n\n\\(\\omega_3\\)\n\\(3\\)\n\\(P(D=3)\\)\n\n\n\\(\\omega_4\\)\n\\(4\\)\n\\(P(D=4)\\)\n\n\n\\(\\omega_5\\)\n\\(5\\)\n\\(P(D=5)\\)\n\n\n\\(\\omega_6\\)\n\\(6\\)\n\\(P(D=6)\\)\n\n\n\nsuch that \\(0 \\leq P(D=d) \\leq 1\\) and and \\(\\sum_{d \\in D} P(d=D) = 1\\).\n\ndata['Got Deal'].value_counts() / len(data)\n\n1    0.605128\n0    0.394872\nName: Got Deal, dtype: float64\n\n\n\ndata['Got Deal'].value_counts(normalize=True)\n\n1    0.605128\n0    0.394872\nName: Got Deal, dtype: float64\n\n\n\n\n\n\n\n\nCaution\n\n\n\nA word of caution on mathematical notation and dimensionality:\nUppercase letters (\\(X, Y ...\\)) often refer to a random variable. Lowercase letters (\\(x, y ...\\)) often refer to a particular outcome of a random variable.\nThe following refer to a probability value (int, float etc.):\n\n\\(P(X = x)\\)\n\nalso written in shorthand as \\(P(x)\\)\n\n\\(P(X = x ∧ Y = y)\\)\n\nalso written in shorthand as \\(P(x, y)\\)\n\n\nThe following refer to a collection of values (pd.Series, pd.DataFrame etc.):\n\n\\(P(X)\\)\n\\(P(X ∧ Y)\\)\n\nalso written as P(X, Y)\n\n\\(P(X = x, Y)\\)",
    "crumbs": [
      "Home",
      "Probability"
    ]
  },
  {
    "objectID": "probability/probability.html#probability-of-an-event-pphi",
    "href": "probability/probability.html#probability-of-an-event-pphi",
    "title": "Probability",
    "section": "Probability of an Event \\(P(\\phi)\\)",
    "text": "Probability of an Event \\(P(\\phi)\\)\nAn event \\(\\phi\\) is a set of possible worlds \\(\\{\\omega_i, \\omega_j, ... \\omega_n\\}\\). In other words, an event \\(\\phi\\) is a subset of \\(\\Omega\\) i.e. \\(\\phi \\subset \\Omega\\)\nIf we continue with the example of rolling a die, we can define an event \\(\\phi\\) as the set of all possible worlds where the die rolls an even number. From the table above, we can see that there are three possible worlds where the die rolls an even number.\nTherefore, the event \\(\\phi\\) is the set \\(\\{\\omega_2, \\omega_4, \\omega_6\\}\\) or \\(\\{D=2, D=4, D=6\\}\\).\n\n\\(P (\\phi) = \\sum_{\\omega \\in \\phi} P(\\omega)\\) is the sum of probabilities of the set of possible worlds defining \\(\\phi\\)\n$P (_1) = P() = P(_2) +P(_4) + P(_6) = 0.167 + 0.167 + 0.167 $\n\nevent_condition = fair_die['D'].apply(lambda x: x % 2 == 0)\n\nevent = fair_die[event_condition]\n\nP_event = event['P(D)'].sum()\n\nround(P_event, 2)\n\n0.5",
    "crumbs": [
      "Home",
      "Probability"
    ]
  },
  {
    "objectID": "probability/probability.html#visualizing-probability-distributions",
    "href": "probability/probability.html#visualizing-probability-distributions",
    "title": "Probability",
    "section": "Visualizing Probability Distributions",
    "text": "Visualizing Probability Distributions\n\nCategorical Random Variables: Bar Plot\n\nfig, ax = plt.subplots()\n\nplt.style.use('dark_background')\n\nx = data['Got Deal'].value_counts(normalize=True).index\ny = data['Got Deal'].value_counts(normalize=True).values\nax.bar(x, y);\n\nax.set_title(\"Deal or No Deal\")\nax.set_xlabel(\"Deal\")\nax.set_ylabel(\"P(Got Deal)\");\n\n\n\n\n\n\n\n\n\n\nContinuous Random Variables: Histogram\nHistograms are used to visualize the distribution of continuous random variables.\nNote that the histogram is a visual representation of the frequency distribution of a continuous random variable. The height of each bar represents the frequency of the values in the bin.\nBins are intervals that divide the range of a continuous random variable into equal segments. The number of bins determines the number of intervals.\n\ndata.head()\n\n\n\n\n\n\n\n\nSeason Number\nStartup Name\nEpisode Number\nPitch Number\nSeason Start\nSeason End\nOriginal Air Date\nIndustry\nBusiness Description\nCompany Website\n...\nGot Deal\nTotal Deal Amount\nTotal Deal Equity\nDeal Valuation\nNumber of Sharks in Deal\nInvestment Amount Per Shark\nEquity Per Shark\nRoyalty Deal\nAdvisory Shares Equity\nLoan\n\n\n\n\n0\n1\nAvaTheElephant\n1\n1\n9-Aug-09\n5-Feb-10\n9-Aug-09\nHealth/Wellness\nAva The Elephant - Baby and Child Care\nhttp://www.avatheelephant.com/\n...\n1\n50000.0\n55.0\n90909.0\n1.0\n50000.0\n55.0\nNaN\nNaN\nNaN\n\n\n1\n1\nMrTod'sPieFactory\n1\n2\n9-Aug-09\n5-Feb-10\n9-Aug-09\nFood and Beverage\nMr. Tod's Pie Factory - Specialty Food\nhttp://whybake.com/\n...\n1\n460000.0\n50.0\n920000.0\n2.0\n230000.0\n25.0\nNaN\nNaN\nNaN\n\n\n2\n1\nWispots\n1\n3\n9-Aug-09\n5-Feb-10\n9-Aug-09\nBusiness Services\nWispots - Consumer Services\nhttp://www.wispots.com/\n...\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n1\nCollegeFoxesPackingBoxes\n1\n4\n9-Aug-09\n5-Feb-10\n9-Aug-09\nLifestyle/Home\nCollege Foxes Packing Boxes - Consumer Services\nhttp://collegehunkshaulingjunk.com/\n...\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n1\nIonicEar\n1\n5\n9-Aug-09\n5-Feb-10\n9-Aug-09\nSoftware/Tech\nIonic Ear - Novelties\nNaN\n...\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 30 columns\n\n\n\nNote that histograms by default have a bin size of 10. This means that the range of the random variable is divided into 10 equal intervals.\n\nfig, ax = plt.subplots()\n\nax.hist(data['Total Deal Equity'], bins=20);\nax.set_title(\"Total Deal Equity\")\nax.set_xlabel(\"Deal Equity\")\nax.set_ylabel(\"Frequency (count of deals)\");\n\n\n\n\n\n\n\n\nIn order to plot probabilities, we need to normalize the histogram. This is done by setting the density parameter to True. This will normalize the histogram such that the area under the histogram is equal to 1.\nAlso note that histograms by default do not plot probabilities. The height of each bar is the frequency of the values in the bin.\n\nfig, ax = plt.subplots()\n\nax.hist(data['Total Deal Equity'], bins=20, density=True);\nax.set_title(\"Total Deal Equity\")\nax.set_xlabel(\"Deal Equity\")\nax.set_ylabel(\"P(Deal Equity)\");",
    "crumbs": [
      "Home",
      "Probability"
    ]
  },
  {
    "objectID": "encoding/spatial.html",
    "href": "encoding/spatial.html",
    "title": "Geospatial Data",
    "section": "",
    "text": "Geographic data is data that is associated with a location on the Earth. Geographic data is often represented as a latitude and longitude. The latitude is the distance north or south of the equator. The longitude is the distance east or west of the prime meridian .\n\nimport pandas as pd \nfrom matplotlib import pyplot as plt \n\nurl  = 'https://raw.githubusercontent.com/fahadsultan/csc343/main/data/uscities.csv'\ndata = pd.read_csv(url, index_col='city')\n\nplt.scatter(data['lng'], data['lat'], c=data['county_fips'], s=1);\nplt.xlim(-130, -65);\nplt.ylim(20, 55);\nplt.xlabel('Longitude');\nplt.ylabel('Latitude');",
    "crumbs": [
      "Home",
      "Geospatial Data"
    ]
  },
  {
    "objectID": "encoding/sound.html",
    "href": "encoding/sound.html",
    "title": "Audio Data",
    "section": "",
    "text": "In this section, we will learn how to use representations of audio data in machine learning.\nAudio files can be represented in a variety of ways. The most common is the waveform, which is a time series of the amplitude of the sound wave at each time point. The waveform is a one-dimensional array of numbers. The sampling rate is the number of samples per second.\nTo load an audio file, we can use the librosa library. The librosa.load function returns the waveform and the sampling rate.\n\n\n\n\n\n\nNote\n\n\n\nYou may have to install the librosa library using !pip install librosa in a new code cell for the code below to work.\nThe audio file can be downloaded from this link.\n\n\n\nfrom matplotlib import pyplot as plt \nplt.style.use('dark_background')\n\nimport librosa\ny, sr = librosa.load('../assets/StarWars3.wav')\nplt.plot(y);\nplt.xlabel('Time (samples)');\nplt.ylabel('Amplitude');\nplt.title('Star Wars Theme\\nSampling rate: %s Hz\\nLength: %s seconds' % (sr, len(y)/sr));\n\n\n\n\n\n\n\n\n\nsr, len(y)\n\n(22050, 66150)\n\n\n\nS = librosa.stft(y)\nS.shape\n\n(1025, 130)\n\n\nPower Spectral Density (PSD) is a measure of the power of a signal at different frequencies. The PSD is calculated using the Fourier Transform. The PSD is a useful representation of audio data because it is often easier to distinguish different sounds in the frequency domain than in the time domain.\n\nimport numpy as np \n\nfig, ax = plt.subplots()\nimg = librosa.display.specshow(librosa.amplitude_to_db(np.abs(S), ref=np.max),\n    y_axis='log', x_axis='time', ax=ax);\nax.set_title('Power spectrogram');\nfig.colorbar(img, ax=ax, format=\"%+2.0f dB\");",
    "crumbs": [
      "Home",
      "Audio Data"
    ]
  },
  {
    "objectID": "encoding/types.html",
    "href": "encoding/types.html",
    "title": "Feature Types",
    "section": "",
    "text": "Tabular data (pd.DataFrame), as discussed previously, is made up of observations (rows) and features (columns). Data type (df.dtypes) of features fall into two primary categories: numeric and categorical.\nThere also exists a third special category of data type called missing. Missing data is a special data type because it is not a data type at all. It is a placeholder for a value that is not known or not applicable. Missing data is represented by NaN (not a number) in pandas. More on missing data in a bit.\nTo study these feature types, we will use the dataset of food safety scores for restaurants in San Francisco. The scores and violation information have been made available by the San Francisco Department of Public Health.\nimport pandas as pd \n\ndata = pd.read_csv('https://raw.githubusercontent.com/fahadsultan/csc272/main/data/restaurants_truncated.csv', index_col=0)\ndata.head()\n\n\n\n\n\n\n\n\nid\nzip\nphone\nlat\nlng\ntype\nscore\nrisk\nviolation\n\n\n\n\n0\n70064\n94103.0\n1.415565e+10\nNaN\nNaN\nRoutine - Unscheduled\n75.0\nHigh Risk\nImproper reheating of food\n\n\n1\n90039\n94103.0\nNaN\nNaN\nNaN\nRoutine - Unscheduled\n81.0\nHigh Risk\nHigh risk food holding temperature\n\n\n2\n89059\n94115.0\n1.415369e+10\nNaN\nNaN\nComplaint\nNaN\nNaN\nNaN\n\n\n3\n91044\n94112.0\nNaN\nNaN\nNaN\nRoutine - Unscheduled\n84.0\nModerate Risk\nInadequate and inaccessible handwashing facili...\n\n\n4\n62768\n94122.0\nNaN\n37.765421\n-122.477256\nRoutine - Unscheduled\n90.0\nLow Risk\nFood safety certificate or food handler card n...",
    "crumbs": [
      "Home",
      "Feature Types"
    ]
  },
  {
    "objectID": "encoding/types.html#missing-data",
    "href": "encoding/types.html#missing-data",
    "title": "Feature Types",
    "section": "Missing Data",
    "text": "Missing Data\nMissing data occurs commonly in many data analysis applications. One of the goals of pandas is to make working with missing data as painless as possible. For example, all of the descriptive statistics on pandas objects exclude missing data by default.\nThe way that missing data is represented in pandas objects is somewhat imperfect, but it is sufficient for most real-world use. For data with float64 dtype, pandas uses the floating-point value NaN (Not a Number) to represent missing data.\nWe call this a sentinel value: when present, it indicates a missing (or null) value.\nThe isna method gives us a Boolean Series with True where values are null:\n\ndata.isna().apply(lambda x: sum(x))\n\nid            0\nzip           1\nphone        27\nlat          30\nlng          30\ntype          0\nscore        15\nrisk         17\nviolation    17\ndtype: int64\n\n\nIn pandas, missing data is also refered to as NA, which stands for Not Available. In statistics applications, NA data may either be data that does not exist or that exists but was not observed (through problems with data collection, for example). When cleaning up data for analysis, it is often important to do analysis on the missing data itself to identify data collection problems or potential biases in the data caused by missing data.\nThe built-in Python None value is also treated as NA.\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\ndropna\nFilter axis labels based on whether values for each label have missing data, with varying thresholds for how much missing data to tolerate.\n\n\nfillna\nFill in missing data with some value\n\n\nisna\nReturn Boolean values indicating which values are missing/NA.\n\n\nnotna\nNegation of isna, returns True for non-NA values and False for NA values.",
    "crumbs": [
      "Home",
      "Feature Types"
    ]
  },
  {
    "objectID": "encoding/types.html#numerical-features",
    "href": "encoding/types.html#numerical-features",
    "title": "Feature Types",
    "section": "Numerical Features",
    "text": "Numerical Features\nNumeric data is data that can be represented as numbers. These variables generally describe some numeric quantity or amount and are also sometimes referred to as “quantitative” variables.\nSince numerical features are already represented as numbers, they are already ready to be used in machine learning models and there is no need to encode them.\nIn the example above, numerical features include zip, phone, lat, lng, score.\n\ndata[['zip', 'phone', 'lat', 'lng', 'score']].head()\n\n\n\n\n\n\n\n\nzip\nphone\nlat\nlng\nscore\n\n\n\n\n0\n94105\nNaN\n37.787925\n-122.400953\n82.0\n\n\n1\n94109\nNaN\n37.786108\n-122.425764\nNaN\n\n\n2\n94115\nNaN\n37.791607\n-122.434563\n82.0\n\n\n3\n94115\nNaN\n37.788932\n-122.433895\n78.0\n\n\n4\n94110\nNaN\n37.739161\n-122.416967\n94.0\n\n\n\n\n\n\n\n\nDiscrete Features\nDiscrete data is data that is counted. For example, the number of students in a class is discrete data. You can count the number of students in a class. You can not count the number of students in a class and get a fraction of a student. You can only count whole students.\nIn the restaurants inspection data set, zip, phone, score are discrete features.\n\ndata[['zip', 'phone', 'score']].head()\n\n\n\n\n\n\n\n\nzip\nphone\nscore\n\n\n\n\n0\n94105\nNaN\n82.0\n\n\n1\n94109\nNaN\nNaN\n\n\n2\n94115\nNaN\n82.0\n\n\n3\n94115\nNaN\n78.0\n\n\n4\n94110\nNaN\n94.0\n\n\n\n\n\n\n\n\n\nContinuous Features\nContinuous data is data that is measured. For example, the height of a student is continuous data. You can measure the height of a student. You can measure the height of a student and get a fraction of a student. You can measure a student and get a height of 5 feet and 6.5 inches.\nIn the restaurants inspection data set, lat, lng are continuous features.\n\ndata[['lat', 'lng']].head()\n\n\n\n\n\n\n\n\nlat\nlng\n\n\n\n\n0\n37.787925\n-122.400953\n\n\n1\n37.786108\n-122.425764\n\n\n2\n37.791607\n-122.434563\n\n\n3\n37.788932\n-122.433895\n\n\n4\n37.739161\n-122.416967",
    "crumbs": [
      "Home",
      "Feature Types"
    ]
  },
  {
    "objectID": "encoding/types.html#categorical-features",
    "href": "encoding/types.html#categorical-features",
    "title": "Feature Types",
    "section": "Categorical Features",
    "text": "Categorical Features\nCategorical data is data that is not numeric. It is often represented as text or a set of text values. These variables generally describe some characteristic or quality of a data unit, and are also sometimes referred to as “qualitative” variables.\n\ndata[['type', 'risk', 'violation']].head()\n\n\n\n\n\n\n\n\ntype\nrisk\nviolation\n\n\n\n\n0\nRoutine - Unscheduled\nHigh Risk\nHigh risk food holding temperature\n\n\n1\nComplaint\nNaN\nNaN\n\n\n2\nRoutine - Unscheduled\nLow Risk\nInadequate warewashing facilities or equipment\n\n\n3\nRoutine - Unscheduled\nLow Risk\nImproper food storage\n\n\n4\nRoutine - Unscheduled\nLow Risk\nUnapproved or unmaintained equipment or utensils\n\n\n\n\n\n\n\n\nOrdinal Features\nOrdinal data is data that is ordered in some way. For example, the size of a t-shirt is ordinal data. The sizes are ordered from smallest to largest. The sizes are more or less than each other. They are different and ordered.\n\ndata[['risk']].head()\n\n\n\n\n\n\n\n\nrisk\n\n\n\n\n0\nHigh Risk\n\n\n1\nNaN\n\n\n2\nLow Risk\n\n\n3\nLow Risk\n\n\n4\nLow Risk\n\n\n\n\n\n\n\n\nEncoding Ordinal Features\nOrdinal features can be encoded using a technique called label encoding. Label encoding is simply converting each value in a column to a number. For example, the sizes of t-shirts could be represented as 0 (XS), 1 (S), 2 (M), 3 (L), 4 (XL), 5 (XXL).\n\ndata['risk_enc'] = data['risk'].replace({'Low Risk': 0, 'Moderate Risk': 1, 'High Risk': 2})\ndata.head()\n\n\n\n\n\n\n\n\nid\nzip\nphone\nlat\nlng\ntype\nscore\nrisk\nviolation\nrisk_enc\n\n\n\n\n0\n64454\n94105\nNaN\n37.787925\n-122.400953\nRoutine - Unscheduled\n82.0\nHigh Risk\nHigh risk food holding temperature\n2.0\n\n\n1\n33014\n94109\nNaN\n37.786108\n-122.425764\nComplaint\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1526\n94115\nNaN\n37.791607\n-122.434563\nRoutine - Unscheduled\n82.0\nLow Risk\nInadequate warewashing facilities or equipment\n0.0\n\n\n3\n73\n94115\nNaN\n37.788932\n-122.433895\nRoutine - Unscheduled\n78.0\nLow Risk\nImproper food storage\n0.0\n\n\n4\n66402\n94110\nNaN\n37.739161\n-122.416967\nRoutine - Unscheduled\n94.0\nLow Risk\nUnapproved or unmaintained equipment or utensils\n0.0\n\n\n\n\n\n\n\n\n\n\nNominal Features\nNominal data is data that is not ordered in any way. For example, the color of a car is nominal data. There is no order to the colors. The colors are not more or less than each other. They are just different.\n\ndata[['type', 'violation']].head()\n\n\n\n\n\n\n\n\ntype\nviolation\n\n\n\n\n0\nRoutine - Unscheduled\nHigh risk food holding temperature\n\n\n1\nComplaint\nNaN\n\n\n2\nRoutine - Unscheduled\nInadequate warewashing facilities or equipment\n\n\n3\nRoutine - Unscheduled\nImproper food storage\n\n\n4\nRoutine - Unscheduled\nUnapproved or unmaintained equipment or utensils\n\n\n\n\n\n\n\n\nEncoding Nominal Features\nSince nominal features don’t have any order, encoding them requires some creativity. The most common way to encode nominal features is to use a technique called one-hot encoding. One-hot encoding creates a new column for each unique value in the nominal feature. Each new column is a binary feature that indicates whether or not the original observation had that value.\nThe figure below illustrates how one-hot encoding for a “day” (of the week) column:\n\n\n\n\npandas has a built-in .get_dummies function for doing this:\n\npd.get_dummies(data['type']).head()\n\n\n\n\n\n\n\n\nComplaint\nNew Construction\nNew Ownership\nReinspection/Followup\nRoutine - Unscheduled\n\n\n\n\n0\n0\n0\n0\n0\n1\n\n\n1\n0\n0\n0\n0\n1\n\n\n2\n1\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n1\n\n\n4\n0\n0\n0\n0\n1",
    "crumbs": [
      "Home",
      "Feature Types"
    ]
  },
  {
    "objectID": "encoding/types.html#dtypes-attribute",
    "href": "encoding/types.html#dtypes-attribute",
    "title": "Feature Types",
    "section": ".dtypes attribute",
    "text": ".dtypes attribute\n.dtypes is an attribute of a DataFrame that returns the data type of each column. The data types are returned as a Series with the column names as the index labels.\n\ndata.dtypes\n\nid             int64\nzip          float64\nphone        float64\nlat          float64\nlng          float64\ntype          object\nscore        float64\nrisk          object\nviolation     object\ndtype: object\n\n\nIn pandas, object is the data type used for string columns, while int64 and float64 are used for integer and floating-point columns, respectively.",
    "crumbs": [
      "Home",
      "Feature Types"
    ]
  },
  {
    "objectID": "encoding/types.html#astype",
    "href": "encoding/types.html#astype",
    "title": "Feature Types",
    "section": ".astype()",
    "text": ".astype()\nCast a pandas object to a specified dtype\n\ndata.head()\n\n\n\n\n\n\n\n\nid\nzip\nphone\nlat\nlng\ntype\nscore\nrisk\nviolation\n\n\n\n\n0\n70064\n94103.0\n1.415565e+10\nNaN\nNaN\nRoutine - Unscheduled\n75.0\nHigh Risk\nImproper reheating of food\n\n\n1\n90039\n94103.0\nNaN\nNaN\nNaN\nRoutine - Unscheduled\n81.0\nHigh Risk\nHigh risk food holding temperature\n\n\n2\n89059\n94115.0\n1.415369e+10\nNaN\nNaN\nComplaint\nNaN\nNaN\nNaN\n\n\n3\n91044\n94112.0\nNaN\nNaN\nNaN\nRoutine - Unscheduled\n84.0\nModerate Risk\nInadequate and inaccessible handwashing facili...\n\n\n4\n62768\n94122.0\nNaN\n37.765421\n-122.477256\nRoutine - Unscheduled\n90.0\nLow Risk\nFood safety certificate or food handler card n...\n\n\n\n\n\n\n\n\ndata['zip'].astype(int)",
    "crumbs": [
      "Home",
      "Feature Types"
    ]
  },
  {
    "objectID": "encoding/network.html",
    "href": "encoding/network.html",
    "title": "Networks Data",
    "section": "",
    "text": "Networks are a data structure that consists of a set of nodes (vertices) and a set of edges that relate the nodes to each other. The set of edges describes relationships among the vertices. Graphs are used to model many real-world systems, including computer networks, social networks, and transportation systems.\n\n\n\n\nThere are two common ways to represent a graph as a matrix: 1. Adjacency Matrix and 2. Edge List.\n\nThe first way is to use an Adjacency matrix, which is a matrix where each row and column represents a vertex. If there is an edge from vertex \\(i\\) to vertex \\(j\\), then the entry in row \\(i\\) and column \\(j\\) is 1. Otherwise, the entry is 0. For example, the following matrix represents a graph with 4 vertices and 4 edges: \n\n\n\n\n\nThe second way to represent a graph as a matrix is to use an Edge list. An edge list is a list of pairs of vertices that are connected by an edge. For example, the following edge list represents the same graph as the adjacency matrix above:",
    "crumbs": [
      "Home",
      "Networks Data"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSC-272: Data Mining",
    "section": "",
    "text": "Hi! Welcome to the CSC-272 Introduction to Data Mining course website 👋🏾\nI am excited to talk math 🔢, programming 💻 and all-things data 📊 in this course with you!\nFrom the navigation bar on the top ☝🏾 and sidebar on the left 👈🏾, you should be able to navigate to any topic relevant to the course. If that does not help, there should also be a search icon 🔍 in the top left corner ↗️\n👇🏾 Below, you can find important links and important announcements.\n\n\n\n\n\n\n\n\nGo over the syllabus\n\n\n\nPlease go over the syllabus to familiarize yourself with the course policies, grading scheme and other important information.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "syllabus/grading.html",
    "href": "syllabus/grading.html",
    "title": "Grading",
    "section": "",
    "text": "Component\nPercentage\n\n\n\n\nProject\n40%\n\n\nAssignments\n20%\n\n\nExam 1\n10%\n\n\nExam 2\n10%\n\n\nExam 3 (Last class)\n10%\n\n\nProfessionalism\n5%\n\n\nAsking Questions and Engaging in Class\n5%\n\n\n\n\n\n\n\n(+/- at instructor’s discretion)\n\n\n\nLetter Grade\nRange\n\n\n\n\nA\n&gt; 90 %\n\n\nB\n80 - 90 %\n\n\nC\n70 - 80 %\n\n\nD\n60 - 70 %\n\n\nF\n&lt; 60 %\n\n\n\n\n\n\nIn order to pass this class, you must\n\nEarn \\(\\geq\\) 60% of the total points\nAttend \\(\\geq\\) 80% of the lectures and labs.\nSubmit \\(\\geq\\) 80% of written and programmming assignments.\nTake ALL tests and final!\nEarn \\(\\geq\\) 50% in each of the 7 components above. In other words, you cannot blow off an entire aspect of the course and pass this class!\n\nNote that this basic requirement is necessary but not sufficient to pass the class.",
    "crumbs": [
      "Grading"
    ]
  },
  {
    "objectID": "syllabus/grading.html#breakdown",
    "href": "syllabus/grading.html#breakdown",
    "title": "Grading",
    "section": "",
    "text": "Component\nPercentage\n\n\n\n\nProject\n40%\n\n\nAssignments\n20%\n\n\nExam 1\n10%\n\n\nExam 2\n10%\n\n\nExam 3 (Last class)\n10%\n\n\nProfessionalism\n5%\n\n\nAsking Questions and Engaging in Class\n5%",
    "crumbs": [
      "Grading"
    ]
  },
  {
    "objectID": "syllabus/grading.html#scale",
    "href": "syllabus/grading.html#scale",
    "title": "Grading",
    "section": "",
    "text": "(+/- at instructor’s discretion)\n\n\n\nLetter Grade\nRange\n\n\n\n\nA\n&gt; 90 %\n\n\nB\n80 - 90 %\n\n\nC\n70 - 80 %\n\n\nD\n60 - 70 %\n\n\nF\n&lt; 60 %",
    "crumbs": [
      "Grading"
    ]
  },
  {
    "objectID": "syllabus/grading.html#passing-requirements",
    "href": "syllabus/grading.html#passing-requirements",
    "title": "Grading",
    "section": "",
    "text": "In order to pass this class, you must\n\nEarn \\(\\geq\\) 60% of the total points\nAttend \\(\\geq\\) 80% of the lectures and labs.\nSubmit \\(\\geq\\) 80% of written and programmming assignments.\nTake ALL tests and final!\nEarn \\(\\geq\\) 50% in each of the 7 components above. In other words, you cannot blow off an entire aspect of the course and pass this class!\n\nNote that this basic requirement is necessary but not sufficient to pass the class.",
    "crumbs": [
      "Grading"
    ]
  },
  {
    "objectID": "syllabus/integrity.html",
    "href": "syllabus/integrity.html",
    "title": "Academic Integrity",
    "section": "",
    "text": "Academic Integrity\nAcademic Integrity standards are important to our Furman community and will be upheld in this class. Students should review the Academic Integrity Pledge and other resources available on the Academic Integrity page on the Furman website. Pay special attention to definitions of cheating, plagiarism, unacceptable collaboration, facilitating misconduct and other types of misrepresentation. All those apply in this course.\nFor programming assignments/homeworks and labs, follow the 50 foot policy in its spirit.\n\nIn this class, the grade penalty for an academic integrity violation is an F for the course. Academic Discipline procedures will be followed through the Office of the Academic Dean.",
    "crumbs": [
      "Academic Integrity"
    ]
  },
  {
    "objectID": "syllabus/about.html",
    "href": "syllabus/about.html",
    "title": "Course Description and Goals",
    "section": "",
    "text": "CSC-121 Introduction to Computer Programming\n\n\n\n\n\nCore course requirement for Data Analytics minor\nHuman Behavior (HB) General Education Requirement (GER)\nPre-requisite for CSC-372: Machine Learning with Big Data\n\n\n\n\nThis course introduces students to the core tools, methods, and mindset of modern data science, with a focus on practical skills in Python’s data ecosystem. Students will learn how to navigate the complete data science pipeline: collecting, cleaning, transforming, analyzing, and visualizing data, using industry-standard libraries such as pandas for data manipulation, matplotlib and seaborn for visualization, and introductory applications of scikit-learn for basic machine learning tasks.\nWhile the course emphasizes hands-on coding and problem solving with real-world datasets, it also highlights the mathematical underpinnings that make data science work. Connections to Probability, Statistics, Linear Algebra, and Calculus are introduced throughout, ensuring students understand not just how to run analyses, but why the methods work and how they relate to future study in machine learning and artificial intelligence.\n\n\n\n\n\nBy the end of this course, students will be able to:\n\nNavigate the data science pipeline by collecting, cleaning, transforming, analyzing, and visualizing data.\nDevelop proficiency with core Python libraries:\n\n\npandas for data wrangling and manipulation,\nmatplotlib and seaborn for effective visualizations,\nscikit-learn for introductory machine learning workflows.\n\n\nApply statistical and mathematical reasoning to data analysis, recognizing how probability, statistics, linear algebra, and calculus underpin modern data science methods.\nWork with real-world datasets to practice hands-on coding, problem-solving, and critical interpretation of results.\nBuild a foundation for advanced study in machine learning, statistical modeling, and artificial intelligence.",
    "crumbs": [
      "Course Description and Goals"
    ]
  },
  {
    "objectID": "syllabus/about.html#pre-requisites",
    "href": "syllabus/about.html#pre-requisites",
    "title": "Course Description and Goals",
    "section": "",
    "text": "CSC-121 Introduction to Computer Programming",
    "crumbs": [
      "Course Description and Goals"
    ]
  },
  {
    "objectID": "syllabus/about.html#fulfills-requirements",
    "href": "syllabus/about.html#fulfills-requirements",
    "title": "Course Description and Goals",
    "section": "",
    "text": "Core course requirement for Data Analytics minor\nHuman Behavior (HB) General Education Requirement (GER)\nPre-requisite for CSC-372: Machine Learning with Big Data",
    "crumbs": [
      "Course Description and Goals"
    ]
  },
  {
    "objectID": "syllabus/about.html#course-description",
    "href": "syllabus/about.html#course-description",
    "title": "Course Description and Goals",
    "section": "",
    "text": "This course introduces students to the core tools, methods, and mindset of modern data science, with a focus on practical skills in Python’s data ecosystem. Students will learn how to navigate the complete data science pipeline: collecting, cleaning, transforming, analyzing, and visualizing data, using industry-standard libraries such as pandas for data manipulation, matplotlib and seaborn for visualization, and introductory applications of scikit-learn for basic machine learning tasks.\nWhile the course emphasizes hands-on coding and problem solving with real-world datasets, it also highlights the mathematical underpinnings that make data science work. Connections to Probability, Statistics, Linear Algebra, and Calculus are introduced throughout, ensuring students understand not just how to run analyses, but why the methods work and how they relate to future study in machine learning and artificial intelligence.",
    "crumbs": [
      "Course Description and Goals"
    ]
  },
  {
    "objectID": "syllabus/about.html#course-goals",
    "href": "syllabus/about.html#course-goals",
    "title": "Course Description and Goals",
    "section": "",
    "text": "By the end of this course, students will be able to:\n\nNavigate the data science pipeline by collecting, cleaning, transforming, analyzing, and visualizing data.\nDevelop proficiency with core Python libraries:\n\n\npandas for data wrangling and manipulation,\nmatplotlib and seaborn for effective visualizations,\nscikit-learn for introductory machine learning workflows.\n\n\nApply statistical and mathematical reasoning to data analysis, recognizing how probability, statistics, linear algebra, and calculus underpin modern data science methods.\nWork with real-world datasets to practice hands-on coding, problem-solving, and critical interpretation of results.\nBuild a foundation for advanced study in machine learning, statistical modeling, and artificial intelligence.",
    "crumbs": [
      "Course Description and Goals"
    ]
  },
  {
    "objectID": "syllabus/title_9.html",
    "href": "syllabus/title_9.html",
    "title": "Nondiscrimination Policy and Sexual Misconduct",
    "section": "",
    "text": "Nondiscrimination Policy and Sexual Misconduct\nFurman University and its faculty are committed to supporting our students and seeking an environment that is free of bias, discrimination, and harassment. Furman does not unlawfully discriminate on the basis of race, color, national origin, sex, sexual orientation, gender identity, pregnancy, disability, age, religion, veteran status, or any other characteristic or status protected by applicable local, state, or federal law in admission, treatment, or access to, or employment in, its programs and activities.\nIf you have encountered any form of discrimination or harassment, including sexual misconduct (e.g. sexual assault, sexual harassment or gender-based harassment, sexual exploitation or intimidation, stalking, intimate partner violence), we encourage you to report this to the institution. If you wish to report such an incident of misconduct, you may contact Furman’s Title IX Coordinator, Melissa Nichols (Trone Center, Suite 215; Melissa.nichols@furman.edu; 864.294.2221).\nIf you would like to speak with someone who can advise you but maintain complete confidentiality, you can talk with a counselor, a professional in the Student Health Center or someone in the Office of Spiritual Life. If you speak with a faculty member, understand that as a “Responsible Employee” of the University, the faculty member MUST report to the University’s Title IX Coordinator what you share to help ensure that your safety and welfare are being addressed, consistent with the requirements of the law.\nAdditional information about Furman’s Sexual Misconduct Policy, how to report sexual misconduct and your rights can be found at the Furman Title IX Webpage. You do not have to go through the experience alone.",
    "crumbs": [
      "Nondiscrimination Policy and Sexual Misconduct"
    ]
  },
  {
    "objectID": "syllabus/accomodations.html",
    "href": "syllabus/accomodations.html",
    "title": "Accomodations",
    "section": "",
    "text": "Accomodations\nFurman University recognizes a student with a disability as anyone whose impairment substantially limits one or more major life activity. Students may receive a variety of services including classroom accommodations such as extended time on tests, test proctoring, note-taking assistance and access to assistive technology. However, receipt of reasonable accommodations cannot guarantee success–all students are responsible for meeting academic standards. Students with a diagnosed disability may be entitled to accommodations under the Americans with Disabilities Act (ADA).\nPlease visit Student Office for Accessibility Resources for more info.",
    "crumbs": [
      "Accomodations"
    ]
  },
  {
    "objectID": "syllabus/academic_success.html",
    "href": "syllabus/academic_success.html",
    "title": "Center for Academic Success",
    "section": "",
    "text": "Center for Academic Success\nPeer Tutors are available free of charge for many classes and may be requested by dropping by CAS (LIB 002) or on the Center for Academic Success website. Tutors are typically recommended by faculty and have performed well in the class. \nThe Writing & Media Lab (WML) is staffed by student Consultants who are trained to help you improve your writing and multimodal communication skills. The consultation process is non-directive and intended to allow students to maintain ownership of their work. In addition to helping with the nuts and bolts, WML Consultants also support you in developing your own ideas thoughtfully and critically, whether you’re writing an essay or planning a video or other multimedia project. You may drop into the WML during its regular hours (LIB 002; 9 AM to 10 PM) or visit the **Writing and Media Lab website to make an appointment online.\nProfessional Academic Assistance Staff in CAS can provide students assistance with time management, study skills, and organizational skills.\nThe Writing and ESL Specialist provides professional writing support as well as support for students whose primary language is not English.",
    "crumbs": [
      "Center for Academic Success"
    ]
  },
  {
    "objectID": "project/data.html",
    "href": "project/data.html",
    "title": "Data sets",
    "section": "",
    "text": "Data sets\nGood places to look for data sets:\n\nGoogle’s Dataset Search\nKaggle\n\nAwesome datasets"
  },
  {
    "objectID": "plotting/22_multivariate.html",
    "href": "plotting/22_multivariate.html",
    "title": "Multivariate Visualizations",
    "section": "",
    "text": "Up until now, we’ve discussed how to visualize single-feature distributions. Now, let’s understand how to visualize the relationship between more than one features.\nWe will continue to use the World Bank dataset, which contains information from 2015/16 about countries around the world. We will use the same features as before: GDP per capita, life expectancy, and population.\nimport pandas as pd \nimport seaborn as sns \nfrom matplotlib import pyplot as plt\n\nsns.set_style('whitegrid')\n\ndata = pd.read_csv('https://raw.githubusercontent.com/fahadsultan/csc272/main/data/world_bank.csv', index_col=0)\ndata.head()\n\n\n\n\n\n\n\n\nContinent\nCountry\nPrimary completion rate: Male: % of relevant age group: 2015\nPrimary completion rate: Female: % of relevant age group: 2015\nLower secondary completion rate: Male: % of relevant age group: 2015\nLower secondary completion rate: Female: % of relevant age group: 2015\nYouth literacy rate: Male: % of ages 15-24: 2005-14\nYouth literacy rate: Female: % of ages 15-24: 2005-14\nAdult literacy rate: Male: % ages 15 and older: 2005-14\nAdult literacy rate: Female: % ages 15 and older: 2005-14\n...\nAccess to improved sanitation facilities: % of population: 1990\nAccess to improved sanitation facilities: % of population: 2015\nChild immunization rate: Measles: % of children ages 12-23 months: 2015\nChild immunization rate: DTP3: % of children ages 12-23 months: 2015\nChildren with acute respiratory infection taken to health provider: % of children under age 5 with ARI: 2009-2016\nChildren with diarrhea who received oral rehydration and continuous feeding: % of children under age 5 with diarrhea: 2009-2016\nChildren sleeping under treated bed nets: % of children under age 5: 2009-2016\nChildren with fever receiving antimalarial drugs: % of children under age 5 with fever: 2009-2016\nTuberculosis: Treatment success rate: % of new cases: 2014\nTuberculosis: Cases detection rate: % of new estimated cases: 2015\n\n\n\n\n0\nAfrica\nAlgeria\n106.0\n105.0\n68.0\n85.0\n96.0\n92.0\n83.0\n68.0\n...\n80.0\n88.0\n95.0\n95.0\n66.0\n42.0\nNaN\nNaN\n88.0\n80.0\n\n\n1\nAfrica\nAngola\nNaN\nNaN\nNaN\nNaN\n79.0\n67.0\n82.0\n60.0\n...\n22.0\n52.0\n55.0\n64.0\nNaN\nNaN\n25.9\n28.3\n34.0\n64.0\n\n\n2\nAfrica\nBenin\n83.0\n73.0\n50.0\n37.0\n55.0\n31.0\n41.0\n18.0\n...\n7.0\n20.0\n75.0\n79.0\n23.0\n33.0\n72.7\n25.9\n89.0\n61.0\n\n\n3\nAfrica\nBotswana\n98.0\n101.0\n86.0\n87.0\n96.0\n99.0\n87.0\n89.0\n...\n39.0\n63.0\n97.0\n95.0\nNaN\nNaN\nNaN\nNaN\n77.0\n62.0\n\n\n5\nAfrica\nBurundi\n58.0\n66.0\n35.0\n30.0\n90.0\n88.0\n89.0\n85.0\n...\n42.0\n48.0\n93.0\n94.0\n55.0\n43.0\n53.8\n25.4\n91.0\n51.0\n\n\n\n\n5 rows × 47 columns"
  },
  {
    "objectID": "plotting/22_multivariate.html#distribution-of-a-numeric-feature-w.r.t-a-categorical-features",
    "href": "plotting/22_multivariate.html#distribution-of-a-numeric-feature-w.r.t-a-categorical-features",
    "title": "Multivariate Visualizations",
    "section": "Distribution of a numeric feature w.r.t a categorical features",
    "text": "Distribution of a numeric feature w.r.t a categorical features\nLet’s start by visualizing the distribution of a numeric feature, across the categories defined by a categorical feature. In other words, we want to visualize the distribution of a numeric feature, separately for each category of another categorical feature.\n\nOverlaid Histograms (1 numeric, 1 categorical)\nWe can use a histogram to visualize the distribution of a numeric variable. To visualize how this distribution differs between the groups created by another categorical variable, we can create a histogram for each group separately.\nIn order to create overlaid histograms, we will continue to use sns.histplot. The only addition we need to make is to use the hue argument to specify the categorical feature that defines the groups.\n\namericas = data[data['Continent'].apply(lambda x: \"America\" in x)]\n\ncol = \"Gross domestic product: % growth : 2016\"\n\nax = sns.histplot(data = americas, x = col, hue=\"Continent\",  multiple=\"stack\");\n\nax.set(title=\"GDP of North American vs. South American countries\");\n\n\n\n\n\n\n\n\nNote the use of the hue argument to histplot. It adds a new dimension to the plot, by coloring the bars according to the value of the categorical feature.\nmultiple=\"stack\" is an optional argument and is used to improve the visibility when bars are stacked on top of each other.\nThese visualizations are arguably the most ubiquitous in science. The canonical version of overlaid histograms are where a new drug is tested against a placebo, and the distribution of some outcome (e.g. blood pressure) is plotted for the placebo group and the drug group.\nMost common statistical tests are designed to answer the question: “Do these two groups differ?” This question is answered by comparing the distributions of the two groups.\n\n\nSide-by-side box plots\n\ncol = \"Gross domestic product: % growth : 2016\"\n\nax = sns.boxplot(data = data, y = col, x=\"Continent\", width=0.9);\n\nax.set(title=\"GDP distribution of countries by continent 2016\");"
  },
  {
    "objectID": "plotting/22_multivariate.html#visualizing-relationships",
    "href": "plotting/22_multivariate.html#visualizing-relationships",
    "title": "Multivariate Visualizations",
    "section": "Visualizing Relationships",
    "text": "Visualizing Relationships\nIn addition to visualizing the distribution of features, we often want to understand how two features are related.\n\nScatter Plots (2 or more numeric features)\nScatter plots are one of the most useful tools in representing the relationship between two numerical features. They are particularly important in gauging the strength (correlation) of the relationship between features. Knowledge of these relationships can then motivate decisions in our modeling process.\nIn Matplotlib, we use the function plt.scatter to generate a scatter plot. Notice that unlike our examples of plotting single-variable distributions, now we specify sequences of values to be plotted along the x axis and the y axis.\n\nwb = pd.read_csv('https://raw.githubusercontent.com/fahadsultan/csc272/main/data/world_bank.csv', index_col=0)\n\nax = sns.scatterplot(data = wb, \\\n                     x ='Adult literacy rate: Female: % ages 15 and older: 2005-14', \\\n                     y = \"per capita: % growth: 2016\")\n\nax.set(title=\"Female adult literacy against % growth\");\n\n\n\n\n\n\n\n\nIn Seaborn, we call the function sns.scatterplot. We use the x and y parameters to indicate the values to be plotted along the x and y axes, respectively. By using the hue parameter, we can specify a third variable to be used for coloring each scatter point.\n\nsns.scatterplot(data = wb, \\\n                y = \"per capita: % growth: 2016\", \\\n                x = \"Adult literacy rate: Female: % ages 15 and older: 2005-14\", \n                hue = \"Continent\")\n\nplt.title(\"Female adult literacy against % growth\");\n\n\n\n\n\n\n\n\n\nax = sns.scatterplot(data = wb, \\\n                    y = \"per capita: % growth: 2016\", \\\n                    x = \"Adult literacy rate: Female: % ages 15 and older: 2005-14\", \n                    hue = \"Continent\", \\\n                    size=\"Population: millions: 2016\")\n\nax.figure.set_size_inches(8, 6);\n\nax.set(title=\"Female adult literacy against % growth\");\n\n\n\n\n\n\n\n\n\n\nJoint Plots (2 or more numeric features)\nsns.jointplot creates a visualization with three components: a scatter plot, a histogram of the distribution of x values, and a histogram of the distribution of y values.\nA joint plot visualizes both: relationship and distributions.\n\nsns.jointplot(data = wb, \n              x = \"per capita: % growth: 2016\", \\\n              y = \"Adult literacy rate: Female: % ages 15 and older: 2005-14\")\n\n# plt.suptitle allows us to shift the title up so it does not overlap with the histogram\nplt.suptitle(\"Female adult literacy against % growth\")\nplt.subplots_adjust(top=0.9);\n\n\n\n\n\n\n\n\n\n\nHex plots\nHex plots can be thought of as a two dimensional histograms that shows the joint distribution between two variables. This is particularly useful working with very dense data. In a hex plot, the x-y plane is binned into hexagons. Hexagons that are darker in color indicate a greater density of data – that is, there are more datapoints that lie in the region enclosed by the hexagon.\nWe can generate a hex plot using sns.jointplot modified with the kind parameter.\n\nsns.jointplot(data = wb, \\\n              x = \"per capita: % growth: 2016\", \\\n              y = \"Adult literacy rate: Female: % ages 15 and older: 2005-14\", \\\n              kind = \"hex\")\n\n# plt.suptitle allows us to shift the title up so it does not overlap with the histogram\nplt.suptitle(\"Female adult literacy against % growth\")\nplt.subplots_adjust(top=0.9);"
  },
  {
    "objectID": "plotting/22_multivariate.html#temporal-data-line-plot",
    "href": "plotting/22_multivariate.html#temporal-data-line-plot",
    "title": "Multivariate Visualizations",
    "section": "Temporal Data: Line Plot",
    "text": "Temporal Data: Line Plot\nIf you are trying to visualize the relationship between two numeric variables, and one of those variables is time, then you should use a line plot.\nLine plots are useful for visualizing the relationship between two numeric variables when one of them is time.\nIn seaborn, we can create a line plot using the function sns.lineplot. We use the x and y parameters to specify the variable to be plotted along the x and y axes, respectively.\n\ndata = pd.read_csv('https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv')\n\nsns.lineplot(data = data, x = \"Year\", y = \"Popular vote\", hue='Result', marker='o');\n\n\n\n\n\n\n\n\nNote that seaborn automatically aggregates the data by taking the mean of each numeric variable at each time point. The shaded region around the line represents the 95% confidence interval for the mean. We’ll talk more about confidence intervals in a later lecture."
  },
  {
    "objectID": "plotting/22_multivariate.html#multi-panel-visualizations",
    "href": "plotting/22_multivariate.html#multi-panel-visualizations",
    "title": "Multivariate Visualizations",
    "section": "Multi-panel Visualizations",
    "text": "Multi-panel Visualizations\nTo create a multi-panel visualization, we can use the sns.FacetGrid.\nThis class takes in a dataframe, the names of the variables that will form the row, column, or hue dimensions of the grid, and the plot type to be produced for each subset of the data. The plot type is provided as a method of the FacetGrid object.\n\nimport pandas as pd \nimport seaborn as sns \n\ntips = sns.load_dataset(\"tips\")\n\ng = sns.FacetGrid(tips, col=\"time\",  row=\"sex\");\ng.map(sns.scatterplot, \"total_bill\", \"tip\");\n\n\n\n\n\n\n\n\nThe variable specification in FacetGrid.map() requires a positional argument mapping, but if the function has a data parameter and accepts named variable assignments, you can also use FacetGrid.map_dataframe():\n\n\ng = sns.FacetGrid(tips, col=\"time\",  row=\"sex\");\ng.map_dataframe(sns.histplot, x=\"total_bill\");\n\n\n\n\n\n\n\n\nThe FacetGrid constructor accepts a hue parameter. Setting this will condition the data on another variable and make multiple plots in different colors. Where possible, label information is tracked so that a single legend can be drawn:\n\ng = sns.FacetGrid(tips, col=\"time\", hue=\"sex\");\ng.map_dataframe(sns.scatterplot, x=\"total_bill\", y=\"tip\");\ng.add_legend();\n\n\n\n\n\n\n\n\nThe FacetGrid object has some other useful parameters and methods for tweaking the plot:\n\ng = sns.FacetGrid(tips, col=\"sex\", row=\"time\", margin_titles=True)\ng.map_dataframe(sns.scatterplot, x=\"total_bill\", y=\"tip\")\ng.set_axis_labels(\"Total bill ($)\", \"Tip ($)\")\ng.set_titles(col_template=\"{col_name} patrons\", row_template=\"{row_name}\")\ng.set(xlim=(0, 60), ylim=(0, 12), xticks=[10, 30, 50], yticks=[2, 6, 10])\ng.tight_layout()\ng.savefig(\"facet_plot.png\")"
  },
  {
    "objectID": "plotting/boxplot.html",
    "href": "plotting/boxplot.html",
    "title": "Box Plots",
    "section": "",
    "text": "Box plots display distributions using information about quartiles.\nA quartile represents a 25% portion of the data. We say that:\n\nThe first quartile (Q1) repesents the 25th percentile – 25% of the data lies below the first quartile\nThe second quartile (Q2) represents the 50th percentile, also known as the median – 50% of the data lies below the second quartile\nThe third quartile (Q3) represents the 75th percentile – 75% of the data lies below the third quartile.\n\nIn a box plot, the lower extent of the box lies at Q1, while the upper extent of the box lies at Q3. The horizontal line in the middle of the box corresponds to Q2 (equivalently, the median).\nThe Inter-Quartile Range (IQR) measures the spread of the middle % of the distribution, calculated as the (\\(3^{rd}\\) Quartile \\(-\\) \\(1^{st}\\) Quartile).\nThe whiskers of a box-plot are the two points that lie at the [\\(1^{st}\\) Quartile \\(-\\)(\\(1.5 \\times\\) IQR)], and the [\\(3^{rd}\\) Quartile \\(+\\) (\\(1.5 \\times\\) IQR)]. They are the lower and upper ranges of “normal” data (the points excluding outliers). Subsequently, the outliers are the data points that fall beyond the whiskers, or further than ( \\(1.5 \\times\\) IQR) from the extreme quartiles.\n\nimport seaborn as sns\n\nsns.boxplot(data=elections, y='%', hue='Result', width=0.2);\n\n\n\n\n\n\n\n\n\nax = sns.boxplot(data = elections, y = '%', x=\"Result\", width=0.2);",
    "crumbs": [
      "Home",
      "Box Plots"
    ]
  },
  {
    "objectID": "plotting/lineplots.html",
    "href": "plotting/lineplots.html",
    "title": "Line Plots (Temporal Data)",
    "section": "",
    "text": "In this notebook, we will learn how to create line plots using the Matplotlib library in Python. Line plots are useful for visualizing the relationship between two numeric variables when one of them is time.\nA line plot is a type of plot that displays information as a series of data points called ‘markers’ connected by straight line segments. Line plots are useful for visualizing the relationship between two numeric variables when one of them is time.",
    "crumbs": [
      "Home",
      "Line Plots (Temporal Data)"
    ]
  },
  {
    "objectID": "plotting/lineplots.html#marker-styles",
    "href": "plotting/lineplots.html#marker-styles",
    "title": "Line Plots (Temporal Data)",
    "section": "Marker Styles",
    "text": "Marker Styles\nMarkers are used to highlight individual data points on a line plot. Matplotlib provides a variety of marker styles that can be used to customize the appearance of the data points.\nHere are some common marker styles that can be used in line plots:\n\no: Circle marker\ns: Square marker\n^: Triangle marker\n*: Star marker\nx: X marker\n+: Plus marker\n\nTo specify a marker style in a line plot, we can use the marker parameter of the plot() function. The marker parameter takes a string value that represents the desired marker style.\nHere is an example of how to create a line plot with different marker styles using the Matplotlib library:\n\nfig, ax = plt.subplots()\n\nwinners = elections[elections['Result']=='win']\n\nax.plot(winners['Year'], winners['%'], marker='s');\n\nax.set_xlabel('Year')\nax.set_ylabel('Percentage of Votes')\nax.set_title('Winning Percentage of Votes by Year');\n\n\n\n\n\n\n\n\nThe markers are particularly useful when\n\nwe have a small number of data points and want to highlight individual data points on the plot\nwe have multiple lines on the same plot and want to distinguish between them\nwe want to use grayscale or black-and-white printing, where color is not available to distinguish between lines\n\n\nfig, ax = plt.subplots()\n\ndemocrats = elections[elections['Party']=='Democratic']\nrepublicans = elections[elections['Party']=='Republican']\n\nax.plot(democrats['Year'], democrats['%'], marker='s', label='Democrats');\nax.plot(republicans['Year'],  republicans['%'],  marker='o', label='Republicans');\n\nax.legend()\n\nax.set_xlabel('Year')\nax.set_ylabel('Percentage of Votes')\nax.set_title('Percentage of Votes by Year, by Party');\n\n\n\n\n\n\n\n\nNote that the above example is not a great example of using markers, as the data points are too close together to be easily distinguished and there is a lot of clutter on the plot. However, it demonstrates how to use different marker styles in a line plot.",
    "crumbs": [
      "Home",
      "Line Plots (Temporal Data)"
    ]
  },
  {
    "objectID": "plotting/lineplots.html#marker-size",
    "href": "plotting/lineplots.html#marker-size",
    "title": "Line Plots (Temporal Data)",
    "section": "Marker Size",
    "text": "Marker Size\nMarkers can be customized further by changing their size. The size of the markers can be adjusted using the markersize parameter of the plot() function. The markersize parameter takes a numeric value that represents the desired size of the markers.\nHere is an example of how to create a line plot with different marker sizes using the Matplotlib library:\n\nfig, ax = plt.subplots()\n\ndemocrats = elections[elections['Party']=='Democratic']\nrepublicans = elections[elections['Party']=='Republican']\n\nax.plot(democrats['Year'], democrats['%'], marker='o', label='Democrats', markersize=10);\nax.plot(republicans['Year'],  republicans['%'],  marker='s', label='Republicans', markersize=5);\n\nax.legend();\n\nax.set_xlabel('Year');\nax.set_ylabel('Percentage of Votes');\n\nax.set_title('Percentage of Votes by Year, by Party');",
    "crumbs": [
      "Home",
      "Line Plots (Temporal Data)"
    ]
  },
  {
    "objectID": "plotting/lineplots.html#line-styles",
    "href": "plotting/lineplots.html#line-styles",
    "title": "Line Plots (Temporal Data)",
    "section": "Line Styles",
    "text": "Line Styles\nIn addition to markers, line plots can also be customized by changing the style of the lines. Matplotlib provides a variety of line styles that can be used to customize the appearance of the lines.\nHere are some common line styles that can be used in line plots:\n\n-: Solid line\n--: Dashed line\n-.: Dash-dot line\n:: Dotted line\n\nTo specify a line style in a line plot, we can use the linestyle parameter of the plot() function. The linestyle parameter takes a string value that represents the desired line style.\nHere is an example of how to create a line plot with different line styles using the Matplotlib library:\n\nfig, ax = plt.subplots()\n\ndemocrats = elections[elections['Party']=='Democratic']\nrepublicans = elections[elections['Party']=='Republican']\n\nax.plot(democrats['Year'], democrats['%'], label='Democrats', linestyle='-');\nax.plot(republicans['Year'],  republicans['%'],  label='Republicans', linestyle=':');\n\nax.legend();\n\nax.set_xlabel('Year');\nax.set_ylabel('Percentage of Votes');\n\nax.set_title('Percentage of Votes by Year, by Party');",
    "crumbs": [
      "Home",
      "Line Plots (Temporal Data)"
    ]
  },
  {
    "objectID": "plotting/lineplots.html#line-width",
    "href": "plotting/lineplots.html#line-width",
    "title": "Line Plots (Temporal Data)",
    "section": "Line Width",
    "text": "Line Width\nLines can be customized further by changing their width. The width of the lines can be adjusted using the linewidth parameter of the plot() function. The linewidth parameter takes a numeric value that represents the desired width of the lines.\nHere is an example of how to create a line plot with different line widths using the Matplotlib library:\n\nfig, ax = plt.subplots()\n\ndemocrats = elections[elections['Party']=='Democratic']\nrepublicans = elections[elections['Party']=='Republican']\n\nax.plot(democrats['Year'],    democrats['%'],    label='Democrats',   linewidth=10);\nax.plot(republicans['Year'],  republicans['%'],  label='Republicans', linewidth=10);\n\nax.legend();\n\nax.set_xlabel('Year');\nax.set_ylabel('Percentage of Votes');\n\nax.set_title('Percentage of Votes by Year, by Party');",
    "crumbs": [
      "Home",
      "Line Plots (Temporal Data)"
    ]
  },
  {
    "objectID": "plotting/lineplots.html#line-color",
    "href": "plotting/lineplots.html#line-color",
    "title": "Line Plots (Temporal Data)",
    "section": "Line Color",
    "text": "Line Color\nLines can also be customized by changing their color. The color of the lines can be adjusted using the color parameter of the plot() function. The color parameter takes a string value that represents the desired color of the lines.\nHere is an example of how to create a line plot with different line colors using the Matplotlib library:\n\nfig, ax = plt.subplots()\n\ndems = elections[elections['Party']=='Democratic']\nreps = elections[elections['Party']=='Republican']\n\nax.plot(dems['Year'],    dems['%'],    label='Democrats',   color='blue');\nax.plot(reps['Year'],  reps['%'],  label='Republicans', color='red');\n\nax.legend();\n\nax.set_xlabel('Year');\nax.set_ylabel('Percentage of Votes');\n\nax.set_title('Percentage of Votes by Year, by Party');",
    "crumbs": [
      "Home",
      "Line Plots (Temporal Data)"
    ]
  },
  {
    "objectID": "plotting/lineplots.html#seaborn-and-pandas-for-line-plots",
    "href": "plotting/lineplots.html#seaborn-and-pandas-for-line-plots",
    "title": "Line Plots (Temporal Data)",
    "section": "Seaborn and Pandas for Line Plots",
    "text": "Seaborn and Pandas for Line Plots\nIn addition to using the Matplotlib library directly, we can also create line plots using the Seaborn and Pandas libraries in Python. Seaborn and Pandas provide high-level interfaces for creating line plots that are more user-friendly and require less code.\nHere is an example of how to create a line plot using the Seaborn library in Python:\n\nimport seaborn as sns \n\nurl = 'https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv'\ndata = pd.read_csv(url)\n\nfig, ax = plt.subplots()\n\nsns.lineplot(data = winners,      \\\n             x       = \"Year\",    \\\n             y       = \"%\",       \\\n             hue     = \"Party\",   \\\n             style   = \"Party\",   \\\n             markers = True,      \\\n             dashes  = False,     \\\n             ax      = ax);\n\nax.set_xlabel('Year');\nax.set_ylabel('Percentage of Votes');\nax.set_title('Winning Percentage of Votes by Year, by Party');\n\n\n\n\n\n\n\n\n\nimport seaborn as sns \n\nurl = 'https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv'\ndata = pd.read_csv(url)\n\nfig, ax = plt.subplots()\n\ndems = data[data['Party']=='Democratic']\nreps = data[data['Party']=='Republican']\n\ndems.plot(x='Year', y='%', kind='line', marker='s', color='blue', label='Democrats', ax=ax);\nreps.plot(x='Year', y='%', kind='line', marker='o', color='red', label='Republicans' ,ax=ax);\n\nax.set_xlabel('Year');\nax.set_ylabel('Percentage of Votes');\n\nax.set_title('Percentage of Votes by Year, by Party');\n\n\n\n\n\n\n\n\nNote that seaborn automatically aggregates the data by taking the mean of each numeric variable at each time point. The shaded region around the line represents the 95% confidence interval for the mean. We’ll talk more about confidence intervals in a later lecture.",
    "crumbs": [
      "Home",
      "Line Plots (Temporal Data)"
    ]
  },
  {
    "objectID": "plotting/lineplots.html#please-dont",
    "href": "plotting/lineplots.html#please-dont",
    "title": "Line Plots (Temporal Data)",
    "section": "Please Don’t",
    "text": "Please Don’t\n\nUse line graphs where the x-axis is not time\nPlotting a line graph where the x-axis is not time is confusing. Use a bar graph instead.\n\n\n\n\n\nForget to label\nAll visualizations should have at minimum contain the following:\n\nA clear and descriptive Title.\nAll axes should be labeled using name of the variable and the units of measurement.\nThe axes should also be labeled with the range of values shown.\nLegend, if applicable.\n\nDon’t be like this guy:\n\n\n\n\nNote that the figure above is missing axis labels. No, “wave1”, “wave2”, “wave3” and “wave4” are not proper labels for the x-axis. “Are we stuck?” is also not a very informative title.\n\n\nUse too many lines\n\n\n\nUse the wrong colors\nIf the data is categorical, then use qualitative colormaps. Do not use sequential colormaps.\n\n\n\nIf the data is categorical, then use qualitative colormaps. If your data ranges from negative to positive values use divergent colormaps. If your data ranges from low to high values, then use sequential colormaps.",
    "crumbs": [
      "Home",
      "Line Plots (Temporal Data)"
    ]
  },
  {
    "objectID": "linearalgebra/matrices.html",
    "href": "linearalgebra/matrices.html",
    "title": "Matrices",
    "section": "",
    "text": "We denote matrices by bold capital letters (e.g., \\(X\\), \\(Y\\), and \\(Z\\)), and represent them in code by pd.DataFrame. The expression \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) indicates that a matrix \\(\\textbf{A}\\) contains \\(m \\times n\\) real-valued scalars, arranged as \\(m\\) rows and \\(n\\) columns. When \\(m = n\\), we say that a matrix is square. Visually, we can illustrate any matrix as a table. To refer to an individual element, we subscript both the row and column indices, e.g., \\(a_{ij}\\) is the value that belongs to \\(\\mathbf{A}\\)’s \\(i^{th}\\) row and \\(j^{th}\\) column:\n\\[ \\begin{split}\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix}.\\end{split} \\]\nimport pandas as pd \n\ndf = pd.DataFrame({'a': [1, 20, 3, 40], 'b': [50, 6, 70, 8]})\n\ndf.index = ['v1', 'v2', 'v3', 'v4']\n\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\nv1\n1\n50\n\n\nv2\n20\n6\n\n\nv3\n3\n70\n\n\nv4\n40\n8",
    "crumbs": [
      "Home",
      "Matrices"
    ]
  },
  {
    "objectID": "linearalgebra/matrices.html#geometry-of-matrices",
    "href": "linearalgebra/matrices.html#geometry-of-matrices",
    "title": "Matrices",
    "section": "Geometry of Matrices",
    "text": "Geometry of Matrices\nAn (n, d) matrix can be interpreted as a collection of n vectors in d-dimensional space.\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nplt.style.use('dark_background')\n\nax = sns.scatterplot(x='a', y='b', data=df, s=100);\nax.set(title='Scatterplot of a vs b', xlabel='a', ylabel='b');\n\ndef annotate(row):\n    plt.text(x=row['a']+0.05, y=row['b'], s=row.name, size=20);\n\ndf.apply(annotate, axis=1);",
    "crumbs": [
      "Home",
      "Matrices"
    ]
  },
  {
    "objectID": "linearalgebra/matrices.html#transpose",
    "href": "linearalgebra/matrices.html#transpose",
    "title": "Matrices",
    "section": "Transpose",
    "text": "Transpose\nSometimes we want to flip the axes. When we exchange a matrix’s rows and columns, the result is called its transpose. Formally, we signify a matrix’s \\(\\textbf{A}\\) transpose by \\(\\mathbf{A}^\\top\\) and if \\(\\mathbf{B} = \\mathbf{A}^\\top\\), then \\(b_{ij} = a_{ij}\\) for all \\(i\\) and \\(j\\). Thus, the transpose of an \\(m \\times n\\) matrix is an \\(n \\times m\\) matrix:\n\\[ \\begin{split}\\mathbf{A}^\\top =\n\\begin{bmatrix}\n    a_{11} & a_{21} & \\dots  & a_{m1} \\\\\n    a_{12} & a_{22} & \\dots  & a_{m2} \\\\\n    \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n    a_{1n} & a_{2n} & \\dots  & a_{mn}\n\\end{bmatrix}.\\end{split} \\]\n\nIn pandas, you can transpose a DataFrame with the .T attribute:\n\ndf.T\n\n\n\n\n\n\n\n\nv1\nv2\nv3\nv4\n\n\n\n\na\n1\n20\n3\n40\n\n\nb\n50\n6\n70\n8\n\n\n\n\n\n\n\nNote that columns of original dataframe df are the same as index of df.T\n\ndf.columns == df.T.index\n\narray([ True,  True])\n\n\n\ndf.T\n\n\n\n\n\n\n\n\nv1\nv2\nv3\nv4\n\n\n\n\na\n1\n20\n3\n40\n\n\nb\n50\n6\n70\n8",
    "crumbs": [
      "Home",
      "Matrices"
    ]
  },
  {
    "objectID": "linearalgebra/matrices.html#matrix-vector-products",
    "href": "linearalgebra/matrices.html#matrix-vector-products",
    "title": "Matrices",
    "section": "Matrix-Vector Products",
    "text": "Matrix-Vector Products\nNow that we know how to calculate dot products, we can begin to understand the product between an \\(m \\times n\\) matrix and an \\(n\\)-dimensional vector \\(\\mathbf{x}\\).\nTo start off, we visualize our matrix in terms of its row vectors\n\\[\n\\begin{split}\\mathbf{A}=\n\\begin{bmatrix}\n\\mathbf{a}^\\top_{1} \\\\\n\\mathbf{a}^\\top_{2} \\\\\n\\vdots \\\\\n\\mathbf{a}^\\top_m \\\\\n\\end{bmatrix},\\end{split}\n\\]\nwhere each \\(\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^n\\) is a row vector representing the \\(i^\\textrm{th}\\) row of the matrix \\(\\mathbf{A}\\).\nThe matrix–vector product \\(\\mathbf{A}\\mathbf{x}\\) is simply a column vector of length \\(m\\) , whose \\(i^{th}\\) element is the dot product \\(\\mathbf{a}^\\top_i \\mathbf{x}\\)\n\\[\n\\begin{split}\\mathbf{A}\\mathbf{x}\n= \\begin{bmatrix}\n\\mathbf{a}^\\top_{1} \\\\\n\\mathbf{a}^\\top_{2} \\\\\n\\vdots \\\\\n\\mathbf{a}^\\top_m \\\\\n\\end{bmatrix}\\mathbf{x}\n= \\begin{bmatrix}\n\\mathbf{a}^\\top_{1} \\mathbf{x}  \\\\\n\\mathbf{a}^\\top_{2} \\mathbf{x} \\\\\n\\vdots\\\\\n\\mathbf{a}^\\top_{m} \\mathbf{x}\\\\\n\\end{bmatrix}.\\end{split}\n\\]\nWe can think of multiplication with a matrix \\(\\mathbf{A}\\in \\mathbb{R}^{m \\times n}\\) as a transformation that projects vectors from \\(\\mathbb{R}^{n}\\) to \\(\\mathbb{R}^{m}\\).\nThese transformations are remarkably useful. For example, we can represent rotations as multiplications by certain square matrices. Matrix–vector products also describe the key calculation involved in computing the outputs of each layer in a neural network given the outputs from the previous layer.\n\nFinding similar vectors\nNote that, given a vector \\(\\mathbf{v}\\), vector-matrix products can also be used to compute the similarity of \\(\\mathbf{v}\\) and each row \\(\\mathbf{a}^\\top_i\\) of matrix \\(\\mathbf{A}\\). This is because the matrix-vector product \\(\\mathbf{A}\\mathbf{v}\\) will contain the dot products of \\(\\mathbf{v}\\) and each row in \\(\\mathbf{A}\\).\nThere is one thing to be careful about: recall that the formula for cosine similarity is:\n\\[\\text{cos}(\\theta) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\|\\mathbf{b}\\|}\\]\nThe dot product (numerator, on the right hand side) is equal to the cosine of the angle between the two vectors when the vectors are normalized (i.e. each divided by their norms).\nThe example below shows how to compute the cosine similarity between a vector \\(\\mathbf{v}\\) and each row of matrix \\(\\mathbf{A}\\).\n\nimport pandas as pd\ndata = pd.read_csv('https://raw.githubusercontent.com/fahadsultan/csc272/main/data/chat_dataset.csv')\n\n\n# creating bow representation\nvocab = (' '.join(data['message'].values)).lower().split()\nbow = pd.DataFrame(columns=vocab)\nfor word in vocab: \n    bow[word] = data['message'].apply(lambda msg: msg.count(word))\n\n\n# l2 norm of a vector\ndef l2_norm(vec):\n    return (sum(vec**2))**(1/2)\n\n# bow where each row is a unit vectors i.e. ||row|| = 1\nbow_unit = bow.apply(lambda row: row/l2_norm(row), axis=1)\n\n\n# random message : I don't have an opinion on this\nmsg = bow_unit.iloc[20] \n\n# cosine similarity of first message with all other messages\nmsg_sim = bow_unit.dot(msg.T)\nmsg_sim.index = data['message']\n\nmsg_sim.sort_values(ascending=False)\n\nmessage\nI don't have an opinion on this           1.000000\nI don't really have an opinion on this    0.984003\nI have no strong opinion about this       0.971575\nI have no strong opinions about this      0.971226\nI have no strong opinion on this          0.969768\n                                            ...   \nI'm not sure what to do 😕                 0.204587\nI'm not sure what to do next 🤷‍♂️         0.201635\nI'm not sure what to do next 🤔            0.200593\nThe food was not good                     0.200295\nThe food was not very good                0.197220\nLength: 584, dtype: float64",
    "crumbs": [
      "Home",
      "Matrices"
    ]
  },
  {
    "objectID": "linearalgebra/matrices.html#matrix-matrix-multiplication",
    "href": "linearalgebra/matrices.html#matrix-matrix-multiplication",
    "title": "Matrices",
    "section": "Matrix-Matrix Multiplication",
    "text": "Matrix-Matrix Multiplication\nOnce you have gotten the hang of dot products and matrix–vector products, then matrix–matrix multiplication should be straightforward.\nSay that we have two matrices \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times k}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{k \\times m}\\):\n\\[\n\\begin{split}\\mathbf{A}=\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1k} \\\\\na_{21} & a_{22} & \\cdots & a_{2k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n\\end{bmatrix},\\quad\n\\mathbf{B}=\\begin{bmatrix}\nb_{11} & b_{12} & \\cdots & b_{1m} \\\\\nb_{21} & b_{22} & \\cdots & b_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n\\end{bmatrix}.\\end{split}\n\\]\nLet \\(\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^k\\) denote the row vector representing the \\(i^\\textrm{th}\\) row of the matrix \\(\\mathbf{A}\\) and let \\(\\mathbf{b}_{j} \\in \\mathbb{R}^k\\) denote the column vector from the \\(j^\\textrm{th}\\) column of the matrix \\(\\mathbf{B}\\) :\n\\[\n\\begin{split}\\mathbf{A}=\n\\begin{bmatrix}\n\\mathbf{a}^\\top_{1} \\\\\n\\mathbf{a}^\\top_{2} \\\\\n\\vdots \\\\\n\\mathbf{a}^\\top_n \\\\\n\\end{bmatrix},\n\\quad \\mathbf{B}=\\begin{bmatrix}\n\\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n\\end{bmatrix}.\\end{split}\n\\]\nTo form the matrix product \\(\\mathbf{C} \\in \\mathbb{R}^{n \\times m}\\) , we simply compute each element \\(c_{ij}\\) as the dot product between the \\(i^{\\textrm{th}}\\) row of \\(\\mathbf{A}\\) and the \\(i^{\\textrm{th}}\\) column of \\(\\mathbf{B}\\) , i.e., \\(\\mathbf{a}^\\top_i \\mathbf{b}_j\\) :\n\\[\n\\begin{split}\\mathbf{C} = \\mathbf{AB} = \\begin{bmatrix}\n\\mathbf{a}^\\top_{1} \\\\\n\\mathbf{a}^\\top_{2} \\\\\n\\vdots \\\\\n\\mathbf{a}^\\top_n \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n\\end{bmatrix}\n= \\begin{bmatrix}\n\\mathbf{a}^\\top_{1} \\mathbf{b}_1 & \\mathbf{a}^\\top_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^\\top_{1} \\mathbf{b}_m \\\\\n\\mathbf{a}^\\top_{2}\\mathbf{b}_1 & \\mathbf{a}^\\top_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^\\top_{2} \\mathbf{b}_m \\\\\n\\vdots & \\vdots & \\ddots &\\vdots\\\\\n\\mathbf{a}^\\top_{n} \\mathbf{b}_1 & \\mathbf{a}^\\top_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^\\top_{n} \\mathbf{b}_m\n\\end{bmatrix}.\\end{split}\n\\]\nWe can think of the matrix–matrix multiplication \\(\\mathbf{AB}\\) as performing \\(m\\) matrix–vector products or \\(m \\times n\\) dot products and stitching the results together to form an \\(m \\times n\\) matrix. In the following snippet, we perform matrix multiplication on A and B. Here, A is a matrix with two rows and three columns, and B is a matrix with three rows and four columns. After multiplication, we obtain a matrix with two rows and four columns.\n\nComputing similarity / distance matrix\nNote that, given two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), matrix-matrix products can also be used to compute the similarity between each row \\(\\mathbf{a}^\\top_i\\) of matrix \\(\\mathbf{A}\\) and each row \\(\\mathbf{b}^\\top_j\\) of matrix \\(\\mathbf{B}\\). This is because the matrix-matrix product \\(\\mathbf{A}\\mathbf{B}\\) will contain the dot products of each pair of rows in \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\).\n\nsimilarity_matrix = bow_unit.dot(bow_unit.T)\n\nsimilarity_matrix.shape\n\n(584, 584)\n\n\n\nsimilarity_matrix.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n\n\n\n\n0\n1.000000\n0.568191\n0.453216\n0.565809\n0.612787\n0.608156\n0.377769\n0.631309\n0.793969\n0.642410\n...\n0.516106\n0.612624\n0.660911\n0.490580\n0.845518\n0.573549\n0.648564\n0.629598\n0.594468\n0.570452\n\n\n1\n0.568191\n1.000000\n0.508206\n0.928580\n0.687137\n0.681944\n0.423604\n0.707906\n0.551139\n0.720354\n...\n0.573755\n0.686954\n0.741100\n0.546166\n0.801864\n0.643139\n0.721001\n0.705988\n0.652357\n0.639666\n\n\n2\n0.453216\n0.508206\n1.000000\n0.506075\n0.548093\n0.707857\n0.778316\n0.621438\n0.538883\n0.648861\n...\n0.457088\n0.547948\n0.650512\n0.443105\n0.593732\n0.512998\n0.574392\n0.605515\n0.520351\n0.609064\n\n\n3\n0.565809\n0.928580\n0.506075\n1.000000\n0.685614\n0.679085\n0.421828\n0.704938\n0.548828\n0.717334\n...\n0.571349\n0.684074\n0.737993\n0.543876\n0.798502\n0.640442\n0.717978\n0.703027\n0.649622\n0.638039\n\n\n4\n0.612787\n0.687137\n0.548093\n0.685614\n1.000000\n0.735468\n0.273748\n0.559585\n0.714100\n0.599091\n...\n0.926494\n0.825530\n0.806139\n0.933257\n0.802775\n0.945544\n0.931874\n0.913601\n0.928473\n0.690556\n\n\n\n\n5 rows × 584 columns\n\n\n\n\nfrom matplotlib import pyplot as plt\nplt.imshow(similarity_matrix, cmap='Greens')\nplt.colorbar();\nplt.title(\"Similarity Matrix: Each cell contains cosine \\nsimilarity between two messages\");\nplt.xlabel(\"Message Index\");\nplt.ylabel(\"Message Index\");\n\n\n\n\n\n\n\n\nNote how 1. the similarity matrix is symmetric, i.e. \\(sim_{ij} = sim_{ji}\\) and 2. the diagonal elements are all 1, i.e. \\(sim_{ii} = 1\\).\n\ndistance_matrix = 1 - bow_unit.dot(bow_unit.T)\n\nfrom matplotlib import pyplot as plt\nplt.imshow(distance_matrix, cmap='Reds')\nplt.colorbar();\nplt.title(\"Distance Matrix: Each cell contains cosine \\ndistance between two messages\");\nplt.xlabel(\"Message Index\");\nplt.ylabel(\"Message Index\");",
    "crumbs": [
      "Home",
      "Matrices"
    ]
  },
  {
    "objectID": "linearalgebra/nearestneighbor.html",
    "href": "linearalgebra/nearestneighbor.html",
    "title": "Nearest Neighbor",
    "section": "",
    "text": "Nearest Neighbor is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions). It is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until function evaluation.\n# import libraries\nfrom matplotlib import pyplot as plt \nimport pandas as pd\n\nplt.style.use('dark_background')\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\nurl  = 'https://raw.githubusercontent.com/fahadsultan/csc343/refs/heads/main/data/uscities.csv'\ndata = pd.read_csv(url)\nus_mainland = data[(data['state_id'] != 'HI') & \\\n                   (data['state_id'] != 'AK') & \\\n                   (data['state_id'] != 'PR')]\n\nsc = us_mainland[us_mainland['state_id'] == 'SC']\nga = us_mainland[us_mainland['state_id'] == 'GA']\n\nunknown1 = (33.04366363086289, -79.34728514760124)\nunknown2 = (33.69266640894652, -82.22939618795743)\nunknown3 = (32.9806084015696,  -81.46763167425789)\nunknown4 = (34.84821943641973, -84.2073074091929)\n\nax.scatter(sc['lng'], sc['lat'], label='South Carolina', color='red',  marker='x');\nax.scatter(ga['lng'], ga['lat'], label='Georgia',        color='blue', marker='^');\nax.scatter(unknown1[1], unknown1[0], s=40, label='Unknowns', color='lightgreen');\nax.scatter(unknown2[1], unknown2[0], s=40, color='lightgreen');\nax.scatter(unknown3[1], unknown3[0], s=40, color='lightgreen');\nax.scatter(unknown4[1], unknown4[0], s=40, color='lightgreen');\nax.legend(fontsize=12);\nThe algorithm is composed of two stages:\nThe algorithm is based on the assumption that the data points that are close to each other are similar. The similarity is calculated using a distance function, such as Euclidean distance, Manhattan distance, Minkowski distance, etc.\nThe algorithm is simple and easy to implement, but it is computationally expensive, especially when the training data is large. It is also sensitive to the curse of dimensionality, which means that the algorithm’s performance deteriorates as the number of dimensions increases.",
    "crumbs": [
      "Home",
      "Nearest Neighbor"
    ]
  },
  {
    "objectID": "linearalgebra/nearestneighbor.html#pseudocode",
    "href": "linearalgebra/nearestneighbor.html#pseudocode",
    "title": "Nearest Neighbor",
    "section": "Pseudocode",
    "text": "Pseudocode\nThe pseudocode for the nearest neighbor algorithm is as follows:\n\nFor each test data point:\n\nCalculate the distance between the test data point and all training data points.\nSort the distances in ascending order.\nSelect the nearest data points.\nDetermine the class of the nearest data points.\nAssign the test data point to the nearest neighbor class.",
    "crumbs": [
      "Home",
      "Nearest Neighbor"
    ]
  },
  {
    "objectID": "linearalgebra/nearestneighbor.html#k-nn-algorithm",
    "href": "linearalgebra/nearestneighbor.html#k-nn-algorithm",
    "title": "Nearest Neighbor",
    "section": "k-NN Algorithm",
    "text": "k-NN Algorithm\nThe k-NN algorithm is an extension of the nearest neighbor algorithm. Instead of returning the most similar data point, the algorithm returns the k most similar data points. The class of the test data is determined by the majority class of the k most similar data points.\nFor example, in the figure below, the test sample (green dot) should be classified either to blue squares or to red triangles. If k = 3 (solid line circle) it is assigned to the red triangles because there are 2 triangles and only 1 square inside the inner circle. If k = 5 (dashed line circle) it is assigned to the blue squares (3 squares vs. 2 triangles inside the outer circle).\n\n\n\n\n\nThe k-NN algorithm is more robust than the nearest neighbor algorithm, as it reduces the noise in the data. However, it is computationally more expensive, as it requires storing and comparing more data points.\n\n\n\nThe k-NN algorithm is a simple and effective algorithm for classification and regression tasks. It is widely used in various fields, such as pattern recognition, image processing, and data mining.",
    "crumbs": [
      "Home",
      "Nearest Neighbor"
    ]
  },
  {
    "objectID": "calculus/em.html",
    "href": "calculus/em.html",
    "title": "Expectation Maximization",
    "section": "",
    "text": "Clustering is the most well-known unsupervised learning technique. The goal of clustering is to discover groups in observations. The groups are called clusters.\nThe data points in the same cluster are similar to each other, compared to points in different clusters, which are relatively dissimilar.\nThere are many clustering algorithms. In this notebook, we will focus on two of them: 1. One that requires the number of clusters (\\(k\\)) to be specified: K-means. 2. And another that does NOT require the number of clusters to be specified: DBSCAN.\nTo compare the performance of the clustering algorithms, in the code below we will use the same six datasets capturing a wide variety of patterns and structures.\n\nimport pandas as pd \nfrom matplotlib import pyplot as plt\nimport seaborn as sns \n\nurl = \"https://raw.githubusercontent.com/fahadsultan/csc272/main/data/clusters/\"\n\nfnames = [\"aniso\", \"blobs\", \"no_structure\", \"noisy_circles\", \"noisy_moons\", \"varied\"]\n\ndatasets = {}\n\nfig, axs = plt.subplots(1, len(fnames), figsize=(17, 3))\nfor i, fname in enumerate(fnames):\n    df = pd.read_csv(url + fname + \".csv\", index_col=0)\n    df.columns = ['x1', 'x2']\n    ax = sns.scatterplot(data=df, x='x1', y='x2', ax=axs[i]);\n    ax.set(title=fname)\n    datasets[fname] = df\n\n\n\n\n\n\n\n\nNote that the data sets are not labeled. Also note that unsupervised learning algorithms do not work only with 2-dimensional data but with data of any dimensionality. Here we use 2-dimensional data only to be able to visualize the results.\n\nK-means\nThe k-means algorithm is a simple and popular clustering algorithm. It is an iterative algorithm that partitions the data points into a pre-specified \\(k\\) number of clusters.\nThe algorithm works as follows: 1. Start: Select \\(k\\) random points as the initial centroids. 2. Update Cluster Assignments: Assign each data point to the cluster with the nearest centroid. 3. Update Cluster Centers: Update the centroids of the clusters by taking the average of the data points in each cluster. 4. Repeat steps 2 and 3 until the centroids do not change.\nThe animation below visualizes the algorithm:\n\n\n\nThe algorithm is guaranteed to converge to a result. However, the result may not be the optimal one.\nBecause of random initialization, the algorithm converges to different results on different runs. Such algorithms or processes, where there is an element of randomness but with some bounds of predictability, are called stochastic algorithms or processes.\n\nBefore Clustering\nLet’s try the k-means algorithm on the blobs dataset first. Note that the raw data has just two features (x1 and x2) but no labels.\n\nX = datasets['blobs']\nX.head()\n\n\n\n\n\n\n\n\nx1\nx2\n\n\n\n\n0\n-5.730354\n-7.583286\n\n\n1\n1.942992\n1.918875\n\n\n2\n6.829682\n1.164871\n\n\n3\n-2.901306\n7.550771\n\n\n4\n5.841093\n1.565094\n\n\n\n\n\n\n\nThe code below plots the raw data as a scatter plot.\n\nsns.scatterplot(data=X, x='x1', y='x2');\nplt.title(\"Blobs dataset\");\n\n\n\n\n\n\n\n\nNote that there are clearly three clusters in the data where the points within each cluster are closer to each other compared to points across clusters.\n\n\nClustering\nWe will use the KMeans class from the sklearn.cluster module.\nThe constructor of the KMeans class takes the number of clusters \\(k\\) as input.\nThe KMeans class has a fit() method that takes the data as input and runs the k-means algorithm on it.\nAfter we fit the model to the data, we can use the .labels_ attribute to get the discovered labels of the clusters assigned to each data point.\nBelow we add a third feature label to the data, which is the cluster label assigned to each data point by the k-means algorithm.\n\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=3, random_state=0)\nkmeans.fit(X)\nX['label'] = kmeans.labels_\n\nX.head()\n\n\n\n\n\n\n\n\nx1\nx2\nlabel\n\n\n\n\n0\n-5.730354\n-7.583286\n1\n\n\n1\n1.942992\n1.918875\n2\n\n\n2\n6.829682\n1.164871\n2\n\n\n3\n-2.901306\n7.550771\n0\n\n\n4\n5.841093\n1.565094\n2\n\n\n\n\n\n\n\n\n\nAfter Clustering\nThe code below plots the data again, but this time with the cluster labels.\n\nimport seaborn as sns \nsns.scatterplot(data=X, x='x1', y='x2', hue='label', palette='viridis');\n\n\n\n\n\n\n\n\nNote that the k-means algorithm has perfectly discovered the three blobs in the data.\n\n\nLimitations of K-means\nK-means is a simple and popular clustering algorithm. However, it has some limitations:\n\nIt requires the number of clusters \\(k\\) to be specified. If the number of clusters is not known in advance, then we need to try different values of \\(k\\) and select the one that gives the best results.\nIt is sensitive to the initial random selection of centroids. The algorithm may converge to different results on different runs.\nSince k-means is reliant on averages, it is sensitive to outliers. Outliers can significantly affect the location of the centroids and hence the clusters.\nMost importantly, k-means does not work well with clusters of different sizes and densities. It assumes that the clusters are spherical and of similar size.\n\nTo illustrate this limitation, let’s try the k-means algorithm on a dataset that does not satisfy the assumptions of the algorithm.\n\nX = datasets['noisy_circles']\nprint(X.head())\nsns.scatterplot(data=X, x='x1', y='x2');\n\nkmeans = KMeans(n_clusters=2, random_state=0)\nkmeans.fit(X)\nsns.scatterplot(data=X, x='x1', y='x2', hue=kmeans.labels_, palette='viridis');\n\n         x1        x2\n0 -0.469276  0.210118\n1 -0.164164  0.986075\n2 -0.471454  0.019974\n3 -0.670347 -0.657977\n4 -0.826468  0.594071\n\n\n\n\n\n\n\n\n\nNote how the k-means algorithm fails to discover the two clusters in the data. This is because the clusters are a) not spherical and b) of different sizes.\nSuch failures of a clustering algorithm can only be detected by either visualizing the results or computing internal cluster validation metrics such as the silhouette score.\n\nSilhouette Score\nThe Silhouette Score is calculated using the mean intra-cluster distance (\\(a\\)) and the mean nearest-cluster distance (\\(b\\)) for each sample. The Silhouette Coefficient for a sample is\n\\[\\text{Silhouette Coefficient} = \\frac{(b - a)}{\\text{max}(a, b)}\\]\nwhere \\(b\\) is the distance between a sample and the nearest cluster that the sample is not a part of.\nNote that Silhouette Coefficient is only defined if number of labels is 2 &lt;= n_labels &lt;= n_samples - 1.\nsklearn.metrics.silhouette_score function returns the mean Silhouette Coefficient over all samples. To obtain the values for each sample, use silhouette_samples.\nThe best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.\nA score of 1 indicates that the object is far away from the neighboring clusters. A score of 0 indicates that the object is close to the decision boundary between two neighboring clusters. A score of -1 indicates that the object may have been assigned to the wrong cluster.\n\nfrom sklearn.metrics import silhouette_score\n\nsilhouette_score(X, kmeans.labels_)\n\n0.355318252897544\n\n\n\n\n\n\nDBSCAN\nDBSCAN stands for Density-Based Spatial Clustering of Applications with Noise. It is a density-based clustering algorithm. It is a popular clustering algorithm because it does not require the number of clusters to be specified. It can discover clusters of arbitrary shapes. It can also identify outliers in the data.\nThe algorithm has two parameters:\n\n\\(\\epsilon\\): maximum distance between two points for them to be considered as in the same neighborhood.\n\\(m\\): minimum number of points required to form a dense region.\n\nThe algorithm works as follows:\n\nIt starts with an arbitrary point in the data set that has not been visited.\nIt finds all of the points in the neighborhood of the point, using a distance measure \\(\\epsilon\\).\nIf there are at least \\(m\\) points in the neighborhood, it starts a cluster with the initial point as its first member. It also visits all of the points in the neighborhood and adds them to the cluster.\nIf there are less than \\(m\\) points in the neighborhood, the point is labeled as noise.\nIf a point is part of a cluster, its neighborhood is also part of that cluster. Hence, all of the points in the neighborhood are added to the cluster.\nThe algorithm repeats steps 1 to 5 until all of the points have been visited.\n\n\n\n\n\nNote that despite the random initialization, DBSCAN is a deterministic algorithm. That is, it always produces the same result on the same data set.\n\nBefore Clustering\n\nX = datasets['noisy_circles']\nprint(X.head())\nsns.scatterplot(data=X, x='x1', y='x2');\n\n         x1        x2\n0 -0.469276  0.210118\n1 -0.164164  0.986075\n2 -0.471454  0.019974\n3 -0.670347 -0.657977\n4 -0.826468  0.594071\n\n\n\n\n\n\n\n\n\n\n\nClustering\nDBSCAN is implemented in the DBSCAN class from the sklearn.cluster module.\nThe constructor of the DBSCAN class takes the two parameters \\(\\epsilon\\) and \\(m\\) as input.\nSimilar to KMeans, the DBSCAN class has a fit() method that takes the data as input and runs the DBSCAN algorithm on it.\nAfter we fit the model to the data, we can use the .labels_ attribute to get the discovered labels of the clusters assigned to each data point.\n\nfrom sklearn.cluster import DBSCAN\n\ndbscan = DBSCAN(eps=0.2, min_samples=5)\ndbscan.fit(X)\nX['label'] = dbscan.labels_\n\n\n\nAfter clustering\nThe code below plots the data again, but this time with the cluster labels.\n\nprint(X.head())\n\nsns.scatterplot(data=X, x='x1', y='x2', hue='label', palette='viridis');\n\n         x1        x2  label\n0 -0.469276  0.210118      0\n1 -0.164164  0.986075      1\n2 -0.471454  0.019974      0\n3 -0.670347 -0.657977      1\n4 -0.826468  0.594071      1\n\n\n\n\n\n\n\n\n\nJust because DBSCAN does better than k-means on the circles dataset, it does not mean that DBSCAN is always better than k-means. Each clustering algorithm has its own strengths and weaknesses.\n\n\n\nComparing DBSCAN and K-means\n\nfnames = [\"aniso\", \"blobs\", \"no_structure\", \"noisy_circles\", \"noisy_moons\", \"varied\"]\n\ndatasets = {}\n\nfig, axs = plt.subplots(3, len(fnames), figsize=(17, 10))\n\nfor i, fname in enumerate(fnames):\n    df = pd.read_csv(url + fname + \".csv\", index_col=0)\n    df.columns = ['x1', 'x2']\n    kmeans = KMeans(n_clusters=3, random_state=0)\n    kmeans.fit(df)\n    kmeans_labels = kmeans.labels_\n    dbscan = DBSCAN(eps=0.2, min_samples=5)\n    dbscan.fit(df)\n    dbscan_labels = dbscan.labels_\n    \n    ax = sns.scatterplot(data=df, x='x1', y='x2', ax=axs[0][i]);\n    ax.set(title=fname)\n    ax = sns.scatterplot(data=df, x='x1', y='x2', hue=kmeans_labels, ax=axs[1][i],  palette='viridis');\n    ax = sns.scatterplot(data=df, x='x1', y='x2', hue=dbscan_labels, ax=axs[2][i],  palette='viridis');\n\n    axs[0][0].set(title=fname, ylabel='No Clustering')\n    axs[1][0].set(ylabel='KMeans (k=3)')\n    axs[2][0].set(ylabel='DBSCAN (eps=0.2, min_samples=5)')\n\n\n\n\n\n\n\n\nNote that the two algorithms work better on different datasets.\nFurthermore, the parameters of the two algorithms (\\(k\\) for nearest neighbor and \\(\\epsilon\\) and \\(m\\) for DBSCAN) need to be tuned to get the best results for an individual datasets.\n\n\nLimitations of Clustering\nNote that not all data sets are suitable for clustering. Some data sets do not have a well-defined cluster structure.\nFor example, below we try the k-means algorithm on the sentiments dataset. We know that the data set has three classes: positive, negative, and neutral. However, the k-means algorithm fails to discover the three classes. This is because the data set does not have a well-defined cluster structure.\n\nimport pandas as pd\ndata = pd.read_csv('https://raw.githubusercontent.com/fahadsultan/csc272/main/data/chat_dataset.csv')\ndata.head()\n\n\n\n\n\n\n\n\nmessage\nsentiment\n\n\n\n\n0\nI really enjoyed the movie\npositive\n\n\n1\nThe food was terrible\nnegative\n\n\n2\nI'm not sure how I feel about this\nneutral\n\n\n3\nThe service was excellent\npositive\n\n\n4\nI had a bad experience\nnegative\n\n\n\n\n\n\n\n\nvocab = ' '.join(data['message'].values).lower().split()\nvocab = list(set(vocab))\n\nbow = pd.DataFrame(0, columns=vocab, index=data.index)\nfor word in vocab:\n    bow[word] = data['message'].apply(lambda x: x.lower().split().count(word))\n\nkmeans = KMeans(n_clusters=3, random_state=0)\n\ndef l2_norm(x):\n    return (sum(x**2))**(1/2)\n\nbow = bow.apply(lambda x: x/l2_norm(x), axis=1)\nkmeans.fit(bow)\n\ndata['label'] = kmeans.labels_\n\n\ndata['label'].value_counts()\n\n1    201\n2    199\n0    184\nName: label, dtype: int64\n\n\n\ndata.groupby(['label', 'sentiment']).size()\n\nlabel  sentiment\n0      negative      50\n       neutral       72\n       positive      62\n1      negative      69\n       neutral       70\n       positive      62\n2      negative      28\n       neutral      117\n       positive      54\ndtype: int64\n\n\n\n\nRelationship between Clusters and Labels\nPlease take caution in comparing the discovered clusters with any available labels for a dataset.\nIn clustering, the label ‘values’ are arbitrary. For example, if we have a dataset with three classes, we can label them as 0, 1, and 2 or as 1, 2, and 3 or as 100, 200, and 300.\n\nfrom sklearn.datasets import load_iris\n\ndata = load_iris(as_frame=True)\nX    = data['data']\n\nkmeans = KMeans(n_clusters=3, random_state=0)\nkmeans.fit(X)\ncluster_labels = kmeans.labels_\n\ny    = data['target']\n\nsum(cluster_labels == y)/len(y)\n\n0.24\n\n\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\nax = sns.scatterplot(data=X, \\\n                x='sepal length (cm)', \\\n                y='sepal width (cm)', \\\n                hue=y, \\\n                palette='viridis', ax=axs[0]);\nax.set(title=\"True labels\");\n\nax = sns.scatterplot(data=X, \\\n                x='sepal length (cm)', \\\n                y='sepal width (cm)', \\\n                hue=cluster_labels, \\\n                palette='viridis', ax=axs[1]);\nax.set(title=\"Cluster labels\");\n\n\n\n\n\n\n\n\n\n\nSilhouette Score to identify \\(k\\), \\(\\epsilon\\) and min_samples\n\nX = datasets['noisy_circles']\nax = sns.scatterplot(data=X, x='x1', y='x2');\nax.set(title=\"Noisy Circles\");\n\nplt.figure();\nX = datasets['blobs']\nax = sns.scatterplot(data=X, x='x1', y='x2');\nax.set(title=\"Blobs\");\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting Silhouette score to find k\n\nsscores = {'noisy_circles':[], 'blobs':[]}\nks = [2, 3, 4, 5, 10, 15]\nurl = \"https://raw.githubusercontent.com/fahadsultan/csc272/main/data/clusters/\"\n\nfor name in ['noisy_circles', 'blobs']:\n\n    X = pd.read_csv(url + name + \".csv\", index_col=0)\n\n    for k in ks:\n        kmeans = KMeans(n_clusters=k, random_state=0)\n        kmeans.fit(X)\n        score = silhouette_score(X, kmeans.labels_)\n        sscores[name].append(score)\n\nax = sns.lineplot(x=ks, y=sscores['noisy_circles'], marker='s');\nax = sns.lineplot(x=ks, y=sscores['blobs'], marker='s');\nax.set(xlabel='k', ylabel='silhouette_score');\n\nplt.grid()\nplt.legend(['noisy_circles', 'blobs']);\nplt.title('Because K-Means does not work for `noisy circles` data, \\nSilhouette Score never reaches close to 1 for any `k`');\n\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n\n\nepsilons = [0.1, 0.2, 0.5, 1, 2, 5]\n\nfig, axs = plt.subplots(1, len(epsilons), figsize=(17, 3))\n\n\nX = datasets['blobs']\nsscores = []\nfor i, e in enumerate(epsilons):\n    dbscan = DBSCAN(eps=e, min_samples=2)\n    dbscan.fit(X[['x1', 'x2']])\n    score = silhouette_score(X, dbscan.labels_)\n    sscores.append(score)\n    # sns.scatterplot(data=X, x='x1', y='x2', hue=dbscan.labels_)\n    axs[i].scatter(X['x1'], X['x2'], c=dbscan.labels_)\n    axs[i].set_title(\"epsilon = \"+ str(e))\n\nplt.figure();\nax = sns.lineplot(x=epsilons, y=sscores, marker='s');\nax.set(xlabel='eps', ylabel='Silhouette Score');\n\nplt.grid()\nplt.legend(['noisy_circles']);\nplt.title('Because K-Means does not work for `noisy circles` data, \\nSilhouette Score never reaches close to 1 for any `k`');\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsscores = {'noisy_circles':[], 'blobs':[]}\nepsilons = [0.2, 0.3, 0.4, 0.5, 1.0, 1.5]\n# epsilons = [0.1, 0.15, 0.2]\n\nX = datasets['blobs']\n# del X['label']\nfor e in epsilons:\n    dbscan = DBSCAN(eps=e, min_samples=2)\n    dbscan.fit(X)\n    print(name, e, len(set(dbscan.labels_)))\n    score = silhouette_score(X, dbscan.labels_)\n    sscores[name].append(score)\n\n# ax = sns.lineplot(x=epsilons, y=sscores['noisy_circles'], marker='s');\nax = sns.lineplot(x=epsilons, y=sscores['blobs'], marker='s');\nax.set(xlabel='eps', ylabel='Silhouette Score');\n\nplt.grid()\nplt.legend(['noisy_circles', 'blobs']);\nplt.title('For Circles dataset, Silhouette Score never reaches close to 1 for either KMeans or DBSCAN');"
  },
  {
    "objectID": "calculus/calculus0.html",
    "href": "calculus/calculus0.html",
    "title": "CALCULUS",
    "section": "",
    "text": "CALCULUS"
  },
  {
    "objectID": "calculus/52_differential.html",
    "href": "calculus/52_differential.html",
    "title": "Differential Calculus",
    "section": "",
    "text": "Mean squared error is also known as the L2 loss function. It is defined as follows:\n\\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\]\nwhere \\(y_i\\) is the true value of the \\(i\\)th example, \\(\\hat{y}_i\\) is the predicted value of the \\(i\\)th example, and \\(n\\) is the number of examples.\nOther common loss functions include the L1 loss function, which is defined as follows:\n\\[ \\text{L1} = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i| \\]\nThe L1 loss function is also known as the mean absolute error (MAE)\nThe L1 loss function is less sensitive to outliers than the L2 loss function. For example, if we have the following two vectors:\n\\[ \\mathbf{y} = [1, 2, 3, 4, 5] \\]\n\\[ \\mathbf{\\hat{y}} = [1, 2, 3, 4, 100] \\]\nThe MSE between these two vectors is 1, while the MAE is 18.4. The MSE is more sensitive to outliers because it squares the difference between the true and predicted values. The L1 loss function is less sensitive to outliers because it takes the absolute value of the difference between the true and predicted values."
  },
  {
    "objectID": "calculus/52_differential.html#cost-and-objective-functions",
    "href": "calculus/52_differential.html#cost-and-objective-functions",
    "title": "Differential Calculus",
    "section": "",
    "text": "Mean squared error is also known as the L2 loss function. It is defined as follows:\n\\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\]\nwhere \\(y_i\\) is the true value of the \\(i\\)th example, \\(\\hat{y}_i\\) is the predicted value of the \\(i\\)th example, and \\(n\\) is the number of examples.\nOther common loss functions include the L1 loss function, which is defined as follows:\n\\[ \\text{L1} = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i| \\]\nThe L1 loss function is also known as the mean absolute error (MAE)\nThe L1 loss function is less sensitive to outliers than the L2 loss function. For example, if we have the following two vectors:\n\\[ \\mathbf{y} = [1, 2, 3, 4, 5] \\]\n\\[ \\mathbf{\\hat{y}} = [1, 2, 3, 4, 100] \\]\nThe MSE between these two vectors is 1, while the MAE is 18.4. The MSE is more sensitive to outliers because it squares the difference between the true and predicted values. The L1 loss function is less sensitive to outliers because it takes the absolute value of the difference between the true and predicted values."
  },
  {
    "objectID": "calculus/52_differential.html#optimization",
    "href": "calculus/52_differential.html#optimization",
    "title": "Differential Calculus",
    "section": "Optimization",
    "text": "Optimization\nOptimization is the process of finding the minimum (or maximum) of a function that depends on some inputs, called design variables. In machine learning, we often want to find the minimum of a loss function, which is a function that measures how bad our model is. For example, in linear regression, we want to find the parameters that minimize the mean squared error (MSE) between the predictions of our model and the true values."
  },
  {
    "objectID": "stats/summary.html",
    "href": "stats/summary.html",
    "title": "Descriptive Statistics",
    "section": "",
    "text": "In data science, we often want to compute summary statistics. pandas provides a number of built-in methods for this purpose.\nFor example, we can use the .mean(), .median() and .std() methods to compute the mean, median, and standard deviation of a column, respectively.\nimport pandas as pd \n\nurl = \"https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv\"\n\nelections = pd.read_csv(url)\n\nelections.head()\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789",
    "crumbs": [
      "Home",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "stats/summary.html#central-tendency",
    "href": "stats/summary.html#central-tendency",
    "title": "Descriptive Statistics",
    "section": "Central Tendency",
    "text": "Central Tendency\nThe mean, median, and mode are three measures of central tendency. They are used to describe the center of a data set.\n\n\n\n\nMean\nThe mean is the average of all the values in a data set. It is calculated by adding up all the values and then dividing by the number of values.\n\\[ \\text{Mean} = \\frac{\\sum_{i=1}^{n} x_i}{n} \\]\nwhere \\(x_i\\) is the \\(i\\)-th value in the data set and \\(n\\) is the number of values.\n\nelections['%'].mean(), sum(elections['%'])/len(elections)\n\n(27.470350372043967, 27.470350372043967)\n\n\n\n\nMedian\nThe median is the middle value in a data set when the values are ordered from smallest to largest. If there is an even number of values, the median is the average of the two middle values.\n\\[ \\text{Median} = \\begin{cases} x_{(n+1)/2} & \\text{if $n$ is odd} \\\\ \\frac{x_{n/2} + x_{n/2+1}}{2} & \\text{if $n$ is even} \\end{cases} \\]\nwhere \\(x_{(n+1)/2}\\) is the middle value when \\(n\\) is odd, and \\(x_{n/2}\\) and \\(x_{n/2+1}\\) are the two middle values when \\(n\\) is even.\n\nelections['%'].median(), elections['%'].quantile(0.5)\n\n(37.67789306, 37.67789306)\n\n\n\n\nMode\nThe mode is the value that appears most frequently in a data set. A data set can have one mode, more than one mode, or no mode at all.\n\\[ \\text{Mode} = \\text{value that appears most frequently} \\]\n\nelections['Party'].mode()\n\n0    Democratic\nName: Party, dtype: object\n\n\n\nelections['Party'].value_counts().idxmax()\n\n'Democratic'",
    "crumbs": [
      "Home",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "stats/summary.html#dispersion",
    "href": "stats/summary.html#dispersion",
    "title": "Descriptive Statistics",
    "section": "Dispersion",
    "text": "Dispersion\nDispersion refers to the spread of values in a data set. Measures of dispersion include the range, variance, and standard deviation.\n\n\n\n\nRange\nThe range is the difference between the largest and smallest values in a data set.\n\\[ \\text{Range} = \\text{Largest value} - \\text{Smallest value} \\]\n\n\nVariance\nThe variance is a measure of how spread out the values in a data set are. It is calculated by taking the average of the squared differences between each value and the mean.\n\\[ \\text{Variance} = \\frac{\\sum_{i=1}^{n} (x_i - \\text{Mean})^2}{n} \\]\nwhere \\(x_i\\) is the \\(i\\)-th value in the data set, \\(\\text{Mean}\\) is the mean of the data set, and \\(n\\) is the number of values.\n\n\nStandard Deviation\nThe standard deviation is the square root of the variance. It is a measure of how spread out the values in a data set are relative to the mean.\n\\[ \\text{Standard Deviation} = \\sqrt{\\text{Variance}} \\]\n\n\nSkewness\nSkewness is a measure of the asymmetry of a distribution. It can be positive, negative, or zero.\nPositive skewness: The distribution is skewed to the right, with a long tail on the right side.\nNegative skewness: The distribution is skewed to the left, with a long tail on the left side.\nZero skewness: The distribution is symmetric.\nThe skewness of a distribution can be computed using the .skew() method.\n\\[ \\text{Skewness} = \\frac{\\sum_{i=1}^{n} (x_i - \\mu)^3}{n \\sigma^3} \\]\nwhere \\(x_i\\) is the \\(i\\)-th value in the data set, $ $ is the mean of the data set, \\(n\\) is the number of values, and $ $ is the standard deviation of the data set.\nSimilarly, we can use the .max() and .min() methods to compute the maximum and minimum values of a Series or DataFrame.\n\nelections['%'].max(), elections['%'].min()\n\n(61.34470329, 0.098088334)\n\n\nThe .sum() method computes the sum of all the values in a Series or DataFrame.\n\n\n\nThe .describe() method computes summary statistics for a Series or DataFrame. It computes the mean, standard deviation, minimum, maximum, and the quantiles of the data.\n\nelections['%'].describe()\n\ncount    182.000000\nmean      27.470350\nstd       22.968034\nmin        0.098088\n25%        1.219996\n50%       37.677893\n75%       48.354977\nmax       61.344703\nName: %, dtype: float64\n\n\n\nelections.describe()\n\n\n\n\n\n\n\n\nYear\nPopular vote\n%\n\n\n\n\ncount\n182.000000\n1.820000e+02\n182.000000\n\n\nmean\n1934.087912\n1.235364e+07\n27.470350\n\n\nstd\n57.048908\n1.907715e+07\n22.968034\n\n\nmin\n1824.000000\n1.007150e+05\n0.098088\n\n\n25%\n1889.000000\n3.876395e+05\n1.219996\n\n\n50%\n1936.000000\n1.709375e+06\n37.677893\n\n\n75%\n1988.000000\n1.897775e+07\n48.354977\n\n\nmax\n2020.000000\n8.126892e+07\n61.344703\n\n\n\n\n\n\n\n\n\n.describe()\nIf many statistics are required from a DataFrame (minimum value, maximum value, mean value, etc.), then .describe() can be used to compute all of them at once.\n\nelections.describe()\n\n\n\n\n\n\n\n\nYear\nPopular vote\n%\n\n\n\n\ncount\n182.000000\n1.820000e+02\n182.000000\n\n\nmean\n1934.087912\n1.235364e+07\n27.470350\n\n\nstd\n57.048908\n1.907715e+07\n22.968034\n\n\nmin\n1824.000000\n1.007150e+05\n0.098088\n\n\n25%\n1889.000000\n3.876395e+05\n1.219996\n\n\n50%\n1936.000000\n1.709375e+06\n37.677893\n\n\n75%\n1988.000000\n1.897775e+07\n48.354977\n\n\nmax\n2020.000000\n8.126892e+07\n61.344703\n\n\n\n\n\n\n\nA different set of statistics will be reported if .describe() is called on a Series.\n\nelections[\"Party\"].describe()\n\ncount            182\nunique            36\ntop       Democratic\nfreq              47\nName: Party, dtype: object\n\n\n\nelections[\"Popular vote\"].describe().astype(int)\n\ncount         182\nmean     12353635\nstd      19077149\nmin        100715\n25%        387639\n50%       1709375\n75%      18977751\nmax      81268924\nName: Popular vote, dtype: int64\n\n\n\nx = elections['%']\ny = elections['Popular vote']\n\ncov = sum((x-x.mean()) * (y-y.mean())) / len(x)\n\nround(cov)\n\n243614836\n\n\n\nx.cov(y), round(x.cov(y, ddof=0))\n\n(244960774.11030602, 243614836)",
    "crumbs": [
      "Home",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "stats/summary.html#percentile-and-quantile",
    "href": "stats/summary.html#percentile-and-quantile",
    "title": "Descriptive Statistics",
    "section": "Percentile and Quantile",
    "text": "Percentile and Quantile\nPercentiles and quantiles are measures of position in a data set. They divide the data set into equal parts.\n\nPercentile\nA percentile is a value below which a given percentage of the data falls. For example, the 25th percentile is the value below which 25% of the data falls.\n\n\nQuantile\nA quantile is a value below which a given fraction of the data falls. For example, the 0.25 quantile is the value below which 25% of the data falls.\nThe .quantile() method can be used to compute the quantiles of a Series or DataFrame.\n\nelections.quantile(0.25)\n\nYear              1889.000000\nPopular vote    387639.500000\n%                    1.219996\nName: 0.25, dtype: float64",
    "crumbs": [
      "Home",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "stats/correlations.html",
    "href": "stats/correlations.html",
    "title": "Correlations",
    "section": "",
    "text": "Covariance is a measure of the relationship between two random variables. It is similar to correlation, but it is not normalized.\n\\[ \\text{Cov}(X, Y) = \\frac{\\sum_{i=1}^{n} (x_i - \\mu_x)(y_i - \\mu_y)}{n} \\]\nwhere \\(x_i\\) and \\(y_i\\) are the \\(i\\)-th values of the two variables, $ _x$ and $ _y$ are the means of the two variables, and \\(n\\) is the number of values.\nThe covariance can be positive, negative, or zero. A positive covariance indicates that the two variables tend to increase or decrease together, while a negative covariance indicates that one variable tends to increase as the other decreases.",
    "crumbs": [
      "Home",
      "Correlations"
    ]
  },
  {
    "objectID": "stats/correlations.html#covariance",
    "href": "stats/correlations.html#covariance",
    "title": "Correlations",
    "section": "",
    "text": "Covariance is a measure of the relationship between two random variables. It is similar to correlation, but it is not normalized.\n\\[ \\text{Cov}(X, Y) = \\frac{\\sum_{i=1}^{n} (x_i - \\mu_x)(y_i - \\mu_y)}{n} \\]\nwhere \\(x_i\\) and \\(y_i\\) are the \\(i\\)-th values of the two variables, $ _x$ and $ _y$ are the means of the two variables, and \\(n\\) is the number of values.\nThe covariance can be positive, negative, or zero. A positive covariance indicates that the two variables tend to increase or decrease together, while a negative covariance indicates that one variable tends to increase as the other decreases.",
    "crumbs": [
      "Home",
      "Correlations"
    ]
  },
  {
    "objectID": "stats/correlations.html#correlations",
    "href": "stats/correlations.html#correlations",
    "title": "Correlations",
    "section": "Correlations",
    "text": "Correlations\nCorrelation is a statistical measure that describes the relationship between two variables. It can be positive, negative, or zero.\nPositive correlation: If one variable increases, the other variable also increases.\nNegative correlation: If one variable increases, the other variable decreases.\nZero correlation: There is no relationship between the two variables.\nThe correlation coefficient ranges from -1 to 1. A value of 1 indicates a perfect positive correlation, a value of -1 indicates a perfect negative correlation, and a value of 0 indicates no correlation.\nThere are several methods to compute the correlation between two variables. The two most common methods are the Pearson correlation coefficient and the Spearman correlation\n\nPearson Correlation Coefficient\nThe Pearson correlation coefficient measures the linear relationship between two variables. It ranges from -1 to 1.\n\\[ r = \\frac{\\sum_{i=1}^{n} (x_i - \\mu_x)(y_i - \\mu_y)}{\\sqrt{\\sum_{i=1}^{n} (x_i -  \\mu_x)^2} \\sqrt{\\sum_{i=1}^{n} (y_i -  \\mu_y)^2}} \\]\nwhere \\(x_i\\) and \\(y_i\\) are the \\(i\\)-th values of the two variables, $ _x$ and $ _y$ are the means of the two variables, and \\(n\\) is the number of values.\n\nimport pandas as pd \n\nurl = \"https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv\"\n\nelections = pd.read_csv(url)\n\nelections.head()\n\nx = elections['Popular vote']\ny = elections['%']\n\nr = x.cov(y) / (x.std() * y.std())\n\nr\n\n0.559061061317942\n\n\nYou can also compute the correlation between two columns of a DataFrame using the .corr() method.\n\nx.corr(y, method='pearson')\n\n0.559061061317942\n\n\n\n\nSpearman Correlation\nThe Spearman correlation coefficient measures the monotonic relationship between two variables. It ranges from -1 to 1.\n\\[ r = 1 - \\frac{6 \\sum_{i=1}^{n} d_i^2}{n(n^2 - 1)} \\]\nwhere \\(d_i\\) is the difference between the ranks of the two variables and \\(n\\) is the number of values.\n\nx.corr(y, method='spearman')\n\n0.7432486904455022\n\n\n\n\n.corr()\nThe .corr() method computes the correlation between columns in a DataFrame. By default, it computes the Pearson correlation coefficient, but the method parameter can be used to specify the method to use.",
    "crumbs": [
      "Home",
      "Correlations"
    ]
  },
  {
    "objectID": "pandas/write_data.html",
    "href": "pandas/write_data.html",
    "title": "Writing Data",
    "section": "",
    "text": "pandas can also write data to a variety of file formats, including CSV, Excel, and SQL databases. The following code cell writes the elections dataset to a CSV file named elections.csv.\n\n\n\n\n\nTo write a DataFrame to a CSV file, use the df.to_csv() function. The first input to df.to_csv() is the filename or filepath that you want to write to.\npd.to_csv('elections_new.csv')\nOther important parameters of the df.to_csv() function are:\n\nsep: (default: sep=',') specifies the separator used to separate columns. Default is , which means the columns are separated by a comma.\nheader: (default: header=True) specifies whether to write the header row. Default is True which means the header row is written. If you don’t want to write the header row, then header=False should be used.\nindex: (default: index=True) specifies whether to write the index column. Default is True which means the index column is written. If you don’t want to write the index column, then index=False should be used. data.to_csv(‘elections.csv’)",
    "crumbs": [
      "Home",
      "Writing Data"
    ]
  },
  {
    "objectID": "pandas/write_data.html#to_csv",
    "href": "pandas/write_data.html#to_csv",
    "title": "Writing Data",
    "section": "",
    "text": "pandas can also write data to a variety of file formats, including CSV, Excel, and SQL databases. The following code cell writes the elections dataset to a CSV file named elections.csv.\n\n\n\n\n\nTo write a DataFrame to a CSV file, use the df.to_csv() function. The first input to df.to_csv() is the filename or filepath that you want to write to.\npd.to_csv('elections_new.csv')\nOther important parameters of the df.to_csv() function are:\n\nsep: (default: sep=',') specifies the separator used to separate columns. Default is , which means the columns are separated by a comma.\nheader: (default: header=True) specifies whether to write the header row. Default is True which means the header row is written. If you don’t want to write the header row, then header=False should be used.\nindex: (default: index=True) specifies whether to write the index column. Default is True which means the index column is written. If you don’t want to write the index column, then index=False should be used. data.to_csv(‘elections.csv’)",
    "crumbs": [
      "Home",
      "Writing Data"
    ]
  },
  {
    "objectID": "pandas/write_data.html#serialization",
    "href": "pandas/write_data.html#serialization",
    "title": "Writing Data",
    "section": "Serialization",
    "text": "Serialization\nSerialization is the process of converting a Python object into a byte stream. This byte stream can be stored on disk or sent over a network.\nThe pickle.dump() function is used to serialize a Python object. It takes two arguments: the object to serialize and a file object to write the byte stream to.\nimport pickle\n\ndata = {'name': 'Alice', 'age': 25}\n\nwith open('data.pickle', 'wb') as f:\n    pickle.dump(data, f)\nIn this example, we serialize a dictionary containing a person’s name and age to a file called data.pickle.",
    "crumbs": [
      "Home",
      "Writing Data"
    ]
  },
  {
    "objectID": "pandas/write_data.html#deserialization",
    "href": "pandas/write_data.html#deserialization",
    "title": "Writing Data",
    "section": "Deserialization",
    "text": "Deserialization\nDeserialization is the process of converting a byte stream back into a Python object.\nThe pickle.load() function is used to deserialize a Python object. It takes a file object containing the byte stream as an argument and returns the deserialized object.\n\nwith open('data.pickle', 'rb') as f:\n    data = pickle.load(f)\n\nprint(data)\nIn this example, we deserialize the byte stream from the data.pickle file back into a Python object and print it.",
    "crumbs": [
      "Home",
      "Writing Data"
    ]
  },
  {
    "objectID": "pandas/write_data.html#security",
    "href": "pandas/write_data.html#security",
    "title": "Writing Data",
    "section": "Security",
    "text": "Security\nIt is important to note that the pickle module is not secure. Deserializing untrusted data can lead to security vulnerabilities, as malicious code can be executed during deserialization. It is recommended to only deserialize data from trusted sources.",
    "crumbs": [
      "Home",
      "Writing Data"
    ]
  },
  {
    "objectID": "pandas/extraction.html",
    "href": "pandas/extraction.html",
    "title": "Extracting and Dropping values",
    "section": "",
    "text": "To extract a subset of values, we can use .loc[] or .iloc[] with row and column indices and labels respectively.\nThe .loc[] method is used to access a group of rows and columns by labels or a boolean array.",
    "crumbs": [
      "Home",
      "Extracting and Dropping values"
    ]
  },
  {
    "objectID": "pandas/extraction.html#dropping-rows-and-columns",
    "href": "pandas/extraction.html#dropping-rows-and-columns",
    "title": "Extracting and Dropping values",
    "section": "Dropping rows and columns",
    "text": "Dropping rows and columns\nTo drop rows and columns in a DataFrame, we can use the drop() method.\nFor example, to drop the first row from the election DataFrame, we can use the following code:\n\nelections.head()\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n\n\n\n\n\n\nelections.drop(columns=['Popular vote'])\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\nwin\n54.574789\n\n\n...\n...\n...\n...\n...\n...\n\n\n177\n2016\nJill Stein\nGreen\nloss\n1.073699\n\n\n178\n2020\nJoseph Biden\nDemocratic\nwin\n51.311515\n\n\n179\n2020\nDonald Trump\nRepublican\nloss\n46.858542\n\n\n180\n2020\nJo Jorgensen\nLibertarian\nloss\n1.177979\n\n\n181\n2020\nHoward Hawkins\nGreen\nloss\n0.255731\n\n\n\n\n182 rows × 5 columns\n\n\n\n\n# Drop the first row\nelections.drop(index=0)\n\n# Drop the first two rows\nelections.drop(index=[0, 1])\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n5\n1832\nHenry Clay\nNational Republican\n484205\nloss\n37.603628\n\n\n6\n1832\nWilliam Wirt\nAnti-Masonic\n100715\nloss\n7.821583\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n177\n2016\nJill Stein\nGreen\n1457226\nloss\n1.073699\n\n\n178\n2020\nJoseph Biden\nDemocratic\n81268924\nwin\n51.311515\n\n\n179\n2020\nDonald Trump\nRepublican\n74216154\nloss\n46.858542\n\n\n180\n2020\nJo Jorgensen\nLibertarian\n1865724\nloss\n1.177979\n\n\n181\n2020\nHoward Hawkins\nGreen\n405035\nloss\n0.255731\n\n\n\n\n180 rows × 6 columns",
    "crumbs": [
      "Home",
      "Extracting and Dropping values"
    ]
  },
  {
    "objectID": "pandas/filtering.html",
    "href": "pandas/filtering.html",
    "title": "Filtering: subset of rows",
    "section": "",
    "text": "Extracting a subset of rows from a DataFrame is called filtering.\nWe can filter rows based on a boolean condition, similar to conditional statements (e.g., if, else) in Python.\nFor example, to filter rows of candidates who ran for elections since 2010, we can use the following code:\nimport pandas as pd \n\nurl = \"https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv\"\n\nelections = pd.read_csv(url)",
    "crumbs": [
      "Home",
      "Filtering: subset of rows"
    ]
  },
  {
    "objectID": "pandas/filtering.html#first-k-rows",
    "href": "pandas/filtering.html#first-k-rows",
    "title": "Filtering: subset of rows",
    "section": "First k rows",
    "text": "First k rows\nThe .head() method is used to view the first few rows of a DataFrame. By default, it returns the first 5 rows.\n\nelections.head()\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789",
    "crumbs": [
      "Home",
      "Filtering: subset of rows"
    ]
  },
  {
    "objectID": "pandas/filtering.html#last-k-rows",
    "href": "pandas/filtering.html#last-k-rows",
    "title": "Filtering: subset of rows",
    "section": "Last k rows",
    "text": "Last k rows\nThe .tail() method is used to view the last few rows of a DataFrame. By default, it returns the last 5 rows.\n\nelections.tail()\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n177\n2016\nJill Stein\nGreen\n1457226\nloss\n1.073699\n\n\n178\n2020\nJoseph Biden\nDemocratic\n81268924\nwin\n51.311515\n\n\n179\n2020\nDonald Trump\nRepublican\n74216154\nloss\n46.858542\n\n\n180\n2020\nJo Jorgensen\nLibertarian\n1865724\nloss\n1.177979\n\n\n181\n2020\nHoward Hawkins\nGreen\n405035\nloss\n0.255731",
    "crumbs": [
      "Home",
      "Filtering: subset of rows"
    ]
  },
  {
    "objectID": "pandas/filtering.html#random-k-rows",
    "href": "pandas/filtering.html#random-k-rows",
    "title": "Filtering: subset of rows",
    "section": "Random k rows",
    "text": "Random k rows\nThe .sample() method is used to view a random sample of rows from a DataFrame. By default, it returns a single random row.\n\nelections.sample()\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n162\n2008\nBarack Obama\nDemocratic\n69498516\nwin\n53.02351\n\n\n\n\n\n\n\n.sample() method can also be used to return multiple random rows by specifying the number of rows to return using the n parameter.\n\nelections.sample(7)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n111\n1960\nJohn Kennedy\nDemocratic\n34220984\nwin\n50.082561\n\n\n20\n1856\nJames Buchanan\nDemocratic\n1835140\nwin\n45.306080\n\n\n67\n1912\nEugene W. Chafin\nProhibition\n208156\nloss\n1.386325\n\n\n50\n1896\nJohn M. Palmer\nNational Democratic\n134645\nloss\n0.969566\n\n\n32\n1872\nUlysses Grant\nRepublican\n3597439\nwin\n55.928594\n\n\n101\n1948\nHenry A. Wallace\nProgressive\n1157328\nloss\n2.374144\n\n\n14\n1848\nLewis Cass\nDemocratic\n1223460\nloss\n42.552229",
    "crumbs": [
      "Home",
      "Filtering: subset of rows"
    ]
  },
  {
    "objectID": "pandas/filtering.html#filtering-by-position",
    "href": "pandas/filtering.html#filtering-by-position",
    "title": "Filtering: subset of rows",
    "section": "Filtering by position",
    "text": "Filtering by position\nThe [] selection operator is the most baffling of all, yet the most commonly used. It only takes a single argument: A slice of row numbers\nSay we wanted the first four rows of our elections DataFrame.\n\nelections[0:4]\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073",
    "crumbs": [
      "Home",
      "Filtering: subset of rows"
    ]
  },
  {
    "objectID": "pandas/filtering.html#filtering-by-condition",
    "href": "pandas/filtering.html#filtering-by-condition",
    "title": "Filtering: subset of rows",
    "section": "Filtering by condition",
    "text": "Filtering by condition\n\nStep 1. A Filtering Condition\nPerhaps the most interesting (and useful) method of selecting data from a Series is with a filtering condition.\nFirst, we apply a boolean condition to the Series. This create a new Series of boolean values.\n\nseries = pd.Series({'a': 1, 'b': 2, 'c': 3, 'd': 4})\nseries &gt; 2\n\na    False\nb    False\nc     True\nd     True\ndtype: bool\n\n\n\n\n\nWe then use this boolean condition to index into our original Series. pandas will select only the entries in the original Series that satisfy the condition.\n\ncondition1 = series &gt; 2\ncondition2 = elections['Year'] &gt; 2010\n\n\n\nStep 2. Applying the Condition\nThe second step is to apply the condition (pd.Series of boolean values, equal to the length of the DataFrame) to the DataFrame using the [] operator.\n\nseries[condition1]\n\nc    3\nd    4\ndtype: int64\n\n\n\nelections[condition2]\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n168\n2012\nBarack Obama\nDemocratic\n65915795\nwin\n51.258484\n\n\n169\n2012\nGary Johnson\nLibertarian\n1275971\nloss\n0.992241\n\n\n170\n2012\nJill Stein\nGreen\n469627\nloss\n0.365199\n\n\n171\n2012\nMitt Romney\nRepublican\n60933504\nloss\n47.384076\n\n\n172\n2016\nDarrell Castle\nConstitution\n203091\nloss\n0.149640\n\n\n173\n2016\nDonald Trump\nRepublican\n62984828\nwin\n46.407862\n\n\n174\n2016\nEvan McMullin\nIndependent\n732273\nloss\n0.539546\n\n\n175\n2016\nGary Johnson\nLibertarian\n4489235\nloss\n3.307714\n\n\n176\n2016\nHillary Clinton\nDemocratic\n65853514\nloss\n48.521539\n\n\n177\n2016\nJill Stein\nGreen\n1457226\nloss\n1.073699\n\n\n178\n2020\nJoseph Biden\nDemocratic\n81268924\nwin\n51.311515\n\n\n179\n2020\nDonald Trump\nRepublican\n74216154\nloss\n46.858542\n\n\n180\n2020\nJo Jorgensen\nLibertarian\n1865724\nloss\n1.177979\n\n\n181\n2020\nHoward Hawkins\nGreen\n405035\nloss\n0.255731\n\n\n\n\n\n\n\nTo filter rows based on multiple conditions, we can use the & operator for AND and the | operator for OR.\nFor example, to filter rows of candidates who won the elections with less than 50% of the votes, we can use the following code:\n\ncondition = (elections['Result'] == 'win') & (elections['%'] &lt; 50)\n\nelections[condition]\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n16\n1848\nZachary Taylor\nWhig\n1360235\nwin\n47.309296\n\n\n20\n1856\nJames Buchanan\nDemocratic\n1835140\nwin\n45.306080\n\n\n23\n1860\nAbraham Lincoln\nRepublican\n1855993\nwin\n39.699408\n\n\n33\n1876\nRutherford Hayes\nRepublican\n4034142\nwin\n48.471624\n\n\n36\n1880\nJames Garfield\nRepublican\n4453337\nwin\n48.369234\n\n\n39\n1884\nGrover Cleveland\nDemocratic\n4914482\nwin\n48.884933\n\n\n43\n1888\nBenjamin Harrison\nRepublican\n5443633\nwin\n47.858041\n\n\n47\n1892\nGrover Cleveland\nDemocratic\n5553898\nwin\n46.121393\n\n\n70\n1912\nWoodrow Wilson\nDemocratic\n6296284\nwin\n41.933422\n\n\n74\n1916\nWoodrow Wilson\nDemocratic\n9126868\nwin\n49.367987\n\n\n100\n1948\nHarry Truman\nDemocratic\n24179347\nwin\n49.601536\n\n\n117\n1968\nRichard Nixon\nRepublican\n31783783\nwin\n43.565246\n\n\n140\n1992\nBill Clinton\nDemocratic\n44909806\nwin\n43.118485\n\n\n144\n1996\nBill Clinton\nDemocratic\n47400125\nwin\n49.296938\n\n\n152\n2000\nGeorge W. Bush\nRepublican\n50456002\nwin\n47.974666\n\n\n173\n2016\nDonald Trump\nRepublican\n62984828\nwin\n46.407862\n\n\n\n\n\n\n\n\n\n.drop_duplicates()\nIf we have a DataFrame with many repeated rows, then .drop_duplicates() can be used to remove the repeated rows.\nWhere .unique() only works with individual columns (Series) and returns an array of unique values, .drop_duplicates() can be used with multiple columns (DataFrame) and returns a DataFrame with the repeated rows removed.\n\nelections[['Party']].drop_duplicates()\n\n\n\n\n\n\n\n\nParty\n\n\n\n\n0\nDemocratic-Republican\n\n\n2\nDemocratic\n\n\n3\nNational Republican\n\n\n6\nAnti-Masonic\n\n\n7\nWhig\n\n\n15\nFree Soil\n\n\n21\nRepublican\n\n\n22\nAmerican\n\n\n24\nConstitutional Union\n\n\n25\nSouthern Democratic\n\n\n26\nNorthern Democratic\n\n\n27\nNational Union\n\n\n31\nLiberal Republican\n\n\n35\nGreenback\n\n\n38\nAnti-Monopoly\n\n\n41\nProhibition\n\n\n42\nUnion Labor\n\n\n48\nPopulist\n\n\n50\nNational Democratic\n\n\n58\nSocialist\n\n\n68\nProgressive\n\n\n78\nFarmer–Labor\n\n\n89\nCommunist\n\n\n93\nUnion\n\n\n103\nDixiecrat\n\n\n110\nStates' Rights\n\n\n115\nAmerican Independent\n\n\n121\nIndependent\n\n\n125\nLibertarian\n\n\n127\nCitizens\n\n\n136\nNew Alliance\n\n\n147\nTaxpayers\n\n\n148\nNatural Law\n\n\n149\nGreen\n\n\n150\nReform\n\n\n160\nConstitution",
    "crumbs": [
      "Home",
      "Filtering: subset of rows"
    ]
  },
  {
    "objectID": "pandas/concat_merge.html",
    "href": "pandas/concat_merge.html",
    "title": "Concatenation and Merging",
    "section": "",
    "text": "Another way to combine DataFrames is to concatenate them. Concatenation is a bit different from joining. When we join two DataFrames, we are combining them horizontally – that is, we are adding new columns to an existing DataFrame. Concatenation, on the other hand, is generally a vertical operation – we are adding new rows to an existing DataFrame.\n\n\n\n\n\n\n\n\n\npd.concat is the pandas method used to concatenate DataFrames together. It takes as input a list of DataFrames to be concatenated and returns a new DataFrame containing all of the rows from each input DataFrame in the input list.\nLet’s say we wanted to concatenate data from two different years in babynames. We can do so using the pd.concat method.\n\nimport pandas as pd \nurl_template = \"https://raw.githubusercontent.com/fahadsultan/csc272/main/data/names/yob%s.txt\"\n\ndata_list = []\nfor year in range(1880, 2023):\n    url = url_template % year\n    data = pd.read_csv(url, header=None, names=['name', 'sex', 'count'])\n    data['year'] = year\n    data_list.append(data)\nall_data = pd.concat(data_list)\nall_data\n\n\n\n\n\n\n\n\nname\nsex\ncount\nyear\n\n\n\n\n0\nMary\nF\n7065\n1880\n\n\n1\nAnna\nF\n2604\n1880\n\n\n2\nEmma\nF\n2003\n1880\n\n\n3\nElizabeth\nF\n1939\n1880\n\n\n4\nMinnie\nF\n1746\n1880\n\n\n...\n...\n...\n...\n...\n\n\n31910\nZuberi\nM\n5\n2022\n\n\n31911\nZydn\nM\n5\n2022\n\n\n31912\nZylon\nM\n5\n2022\n\n\n31913\nZymeer\nM\n5\n2022\n\n\n31914\nZymeire\nM\n5\n2022\n\n\n\n\n2085158 rows × 4 columns",
    "crumbs": [
      "Home",
      "Concatenation and Merging"
    ]
  },
  {
    "objectID": "pandas/concat_merge.html#concatenating-dataframes",
    "href": "pandas/concat_merge.html#concatenating-dataframes",
    "title": "Concatenation and Merging",
    "section": "",
    "text": "Another way to combine DataFrames is to concatenate them. Concatenation is a bit different from joining. When we join two DataFrames, we are combining them horizontally – that is, we are adding new columns to an existing DataFrame. Concatenation, on the other hand, is generally a vertical operation – we are adding new rows to an existing DataFrame.\n\n\n\n\n\n\n\n\n\npd.concat is the pandas method used to concatenate DataFrames together. It takes as input a list of DataFrames to be concatenated and returns a new DataFrame containing all of the rows from each input DataFrame in the input list.\nLet’s say we wanted to concatenate data from two different years in babynames. We can do so using the pd.concat method.\n\nimport pandas as pd \nurl_template = \"https://raw.githubusercontent.com/fahadsultan/csc272/main/data/names/yob%s.txt\"\n\ndata_list = []\nfor year in range(1880, 2023):\n    url = url_template % year\n    data = pd.read_csv(url, header=None, names=['name', 'sex', 'count'])\n    data['year'] = year\n    data_list.append(data)\nall_data = pd.concat(data_list)\nall_data\n\n\n\n\n\n\n\n\nname\nsex\ncount\nyear\n\n\n\n\n0\nMary\nF\n7065\n1880\n\n\n1\nAnna\nF\n2604\n1880\n\n\n2\nEmma\nF\n2003\n1880\n\n\n3\nElizabeth\nF\n1939\n1880\n\n\n4\nMinnie\nF\n1746\n1880\n\n\n...\n...\n...\n...\n...\n\n\n31910\nZuberi\nM\n5\n2022\n\n\n31911\nZydn\nM\n5\n2022\n\n\n31912\nZylon\nM\n5\n2022\n\n\n31913\nZymeer\nM\n5\n2022\n\n\n31914\nZymeire\nM\n5\n2022\n\n\n\n\n2085158 rows × 4 columns",
    "crumbs": [
      "Home",
      "Concatenation and Merging"
    ]
  },
  {
    "objectID": "pandas/concat_merge.html#merging-dataframes",
    "href": "pandas/concat_merge.html#merging-dataframes",
    "title": "Concatenation and Merging",
    "section": "Merging DataFrames",
    "text": "Merging DataFrames\nWhen working on data science projects, we’re unlikely to have absolutely all the data we want contained in a single DataFrame – a real-world data scientist needs to grapple with data coming from multiple sources. If we have access to multiple datasets with related information, we can merge two or more tables into a single DataFrame.\n\n\n\n\n\nTo put this into practice, we’ll revisit the elections dataset.\n\nelections.head(5)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n\n\n\n\n\nSay we want to understand the popularity of the names of each presidential candidate in 2020. To do this, we’ll need the combined data of names and elections.\n\nWe’ll start by creating a new column containing the first name of each presidential candidate. This will help us join each name in elections to the corresponding name data in names.\n\nelections[\"First Name\"] = elections[\"Candidate\"].apply(lambda x: x.split()[0])\nelections.head(5)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\nFirst Name\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\nAndrew\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\nJohn\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\nAndrew\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\nJohn\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\nAndrew\n\n\n\n\n\n\n\n\n# Here, we'll only consider `names` data from 2020\nnames_2020 = names[names[\"Year\"]==2020]\nnames_2020.head()\n\n\n\n\n\n\n\n\nName\nSex\nCount\nYear\nFirst Letter\n\n\n\n\n0\nOlivia\nF\n17641\n2020\nO\n\n\n1\nEmma\nF\n15656\n2020\nE\n\n\n2\nAva\nF\n13160\n2020\nA\n\n\n3\nCharlotte\nF\n13065\n2020\nC\n\n\n4\nSophia\nF\n13036\n2020\nS\n\n\n\n\n\n\n\nNow, we’re ready to merge the two tables. pd.merge is the pandas method used to merge DataFrames together.\n\nmerged = pd.merge(left = elections, right = names_2020, \\\n                  left_on = \"First Name\", right_on = \"Name\")\nmerged.head()\n# Notice that pandas automatically specifies `Year_x` and `Year_y` \n# when both merged DataFrames have the same column name to avoid confusion\n\n\n\n\n\n\n\n\nYear_x\nCandidate\nParty\nPopular vote\nResult\n%\nFirst Name\nName\nSex\nCount\nYear_y\nFirst Letter\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\nAndrew\nAndrew\nF\n12\n2020\nA\n\n\n1\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\nAndrew\nAndrew\nM\n6036\n2020\nA\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\nAndrew\nAndrew\nF\n12\n2020\nA\n\n\n3\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\nAndrew\nAndrew\nM\n6036\n2020\nA\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\nAndrew\nAndrew\nF\n12\n2020\nA\n\n\n\n\n\n\n\nLet’s take a closer look at the parameters:\n\nleft and right parameters are used to specify the DataFrames to be merge.\nleft_on and right_on parameters are assigned to the string names of the columns to be used when performing the merge. These two on parameters tell pandas what values should act as pairing keys to determine which rows to merge across the DataFrames. We’ll talk more about this idea of a pairing key next lecture.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTypes of Merges\nThere are five types of merges in pandas. The type of merge is determined by the how parameter in the pd.merge method. The five types of merges are:\n\n\n\n\n\n\nInner Merge (Default) how='inner': This is the default type of merge in pandas. It returns only the rows that have matching values in both DataFrames.\n\n\n\n\n\n\n\n\n\n\n\nOuter Merge how='outer': This merge returns all rows from both DataFrames. If a row has no match in the other DataFrame, the missing values are filled with NaN.\n\n\n\n\n\n\n\n\n\n\n\nLeft Merge how='left': This merge returns all rows from the left DataFrame and the matched rows from the right DataFrame. The result is NaN for the right DataFrame if there is no match.\n\n\n\n\n\n\n\n\n\n\n\nRight Merge how='right': This merge returns all rows from the right DataFrame and the matched rows from the left DataFrame. The result is NaN for the left DataFrame if there is no match.\n\n\n\n\n\n\n\n\n\n\n\nCross Merge how='cross': This merge returns the Cartesian product of the two DataFrames. That is, it returns all possible combinations of rows from both DataFrames.",
    "crumbs": [
      "Home",
      "Concatenation and Merging"
    ]
  },
  {
    "objectID": "pandas/grouping.html",
    "href": "pandas/grouping.html",
    "title": "Aggregation",
    "section": "",
    "text": "Up until this point, we have been working with individual rows of DataFrames. As data scientists, we often wish to investigate trends across a larger subset of our data. For example, we may want to compute some summary statistic (the mean, median, sum, etc.) for a group of rows in our DataFrame. To do this, we’ll use pandas GroupBy objects.\n\n\n\n\n\n\n\n\n\nA groupby operation involves some combination of splitting a DataFrame into grouped subframes, applying a function, and combining the results.\nFor some arbitrary DataFrame df below, the code df.groupby(\"year\").sum() does the following:\n\nSplits the DataFrame into sub-DataFrames with rows belonging to the same year.\nApplies the sum function to each column of each sub-DataFrame.\nCombines the results of sum into a single DataFrame, indexed by year.\n\n\n\n\nLet’s say we had baby names for all years in a single DataFrame names\n\nimport urllib.request\nimport os.path\nimport pandas as pd \n\n# Download data from the web directly\ndata_url = \"https://www.ssa.gov/oact/babynames/names.zip\"\nlocal_filename = \"../data/names.zip\"\nif not os.path.exists(local_filename): # if the data exists don't download again\n    with urllib.request.urlopen(data_url) as resp, open(local_filename, 'wb') as f:\n        f.write(resp.read())\n\n        \n# Load data without unzipping the file\nimport zipfile\nnames = [] \nwith zipfile.ZipFile(local_filename, \"r\") as zf:\n    data_files = [f for f in zf.filelist if f.filename[-3:] == \"txt\"]\n    def extract_year_from_filename(fn):\n        return int(fn[3:7])\n    for f in data_files:\n        year = extract_year_from_filename(f.filename)\n        with zf.open(f) as fp:\n            df = pd.read_csv(fp, names=[\"Name\", \"Sex\", \"Count\"])\n            df[\"Year\"] = year\n            names.append(df)\nnames = pd.concat(names)\n\nnames\n\n\n\n\n\n\n\n\nName\nSex\nCount\nYear\n\n\n\n\n0\nMary\nF\n7065\n1880\n\n\n1\nAnna\nF\n2604\n1880\n\n\n2\nEmma\nF\n2003\n1880\n\n\n3\nElizabeth\nF\n1939\n1880\n\n\n4\nMinnie\nF\n1746\n1880\n\n\n...\n...\n...\n...\n...\n\n\n31677\nZyell\nM\n5\n2023\n\n\n31678\nZyen\nM\n5\n2023\n\n\n31679\nZymirr\nM\n5\n2023\n\n\n31680\nZyquan\nM\n5\n2023\n\n\n31681\nZyrin\nM\n5\n2023\n\n\n\n\n2117219 rows × 4 columns\n\n\n\n\nnames.to_csv(\"../data/names.csv\", index=False)\n\nNow, if we wanted to aggregate all rows in names for a given year, we would need names.groupby(\"Year\")\n\nnames.groupby(\"Year\")\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f7a30b64bb0&gt;\n\n\nWhat does this strange output mean? Calling .groupby has generated a GroupBy object. You can imagine this as a set of “mini” sub-DataFrames, where each subframe contains all of the rows from names that correspond to a particular year.\nThe diagram below shows a simplified view of names to help illustrate this idea.\n\n\n\nWe can’t work with a GroupBy object directly – that is why you saw that strange output earlier, rather than a standard view of a DataFrame. To actually manipulate values within these “mini” DataFrames, we’ll need to call an aggregation method. This is a method that tells pandas how to aggregate the values within the GroupBy object. Once the aggregation is applied, pandas will return a normal (now grouped) DataFrame.\nAggregation functions (.min(), .max(), .mean(), .sum(), etc.) are the most common way to work with GroupBy objects. These functions are applied to each column of a “mini” grouped DataFrame. We end up with a new DataFrame with one aggregated row per subframe. Let’s see this in action by finding the sum of all counts for each year in names – this is equivalent to finding the number of babies born in each year.\n\nnames.groupby(\"Year\").sum().head(5)\n\n\n\n\n\n\n\n\nCount\n\n\nYear\n\n\n\n\n\n1880\n201484\n\n\n1881\n192690\n\n\n1882\n221533\n\n\n1883\n216944\n\n\n1884\n243461\n\n\n\n\n\n\n\nWe can relate this back to the diagram we used above. Remember that the diagram uses a simplified version of names, which is why we see smaller values for the summed counts.\n\n\n\n\nCalling .agg has condensed each subframe back into a single row. This gives us our final output: a DataFrame that is now indexed by \"Year\", with a single row for each unique year in the original names DataFrame.\nYou may be wondering: where did the \"State\", \"Sex\", and \"Name\" columns go? Logically, it doesn’t make sense to sum the string data in these columns (how would we add “Mary” + “Ann”?). Because of this, pandas will simply omit these columns when it performs the aggregation on the DataFrame. Since this happens implicitly, without the user specifying that these columns should be ignored, it’s easy to run into troubling situations where columns are removed without the programmer noticing. It is better coding practice to select only the columns we care about before performing the aggregation.\n\n# Same result, but now we explicitly tell pandas to only consider the \"Count\" column when summing\nnames.groupby(\"Year\")[[\"Count\"]].sum().head(5)\n\n\n\n\n\n\n\n\nCount\n\n\nYear\n\n\n\n\n\n1880\n201484\n\n\n1881\n192690\n\n\n1882\n221533\n\n\n1883\n216944\n\n\n1884\n243461\n\n\n\n\n\n\n\nThere are many different aggregations that can be applied to the grouped data. The primary requirement is that an aggregation function must:\n\nTake in a Series of data (a single column of the grouped subframe)\nReturn a single value that aggregates this Series\n\nBecause of this fairly broad requirement, pandas offers many ways of computing an aggregation.\nIn-built Python operations – such as sum, max, and min – are automatically recognized by pandas.\n\n# What is the maximum count for each name in any year?\nnames.groupby(\"Name\")[[\"Count\"]].max().head()\n\n\n\n\n\n\n\n\nCount\n\n\nName\n\n\n\n\n\nAaban\n16\n\n\nAabha\n9\n\n\nAabid\n6\n\n\nAabidah\n5\n\n\nAabir\n5\n\n\n\n\n\n\n\n\n# What is the minimum count for each name in any year?\nnames.groupby(\"Name\")[[\"Count\"]].min().head()\n\n\n\n\n\n\n\n\nCount\n\n\nName\n\n\n\n\n\nAaban\n5\n\n\nAabha\n5\n\n\nAabid\n5\n\n\nAabidah\n5\n\n\nAabir\n5\n\n\n\n\n\n\n\n\n# What is the average count for each name across all years?\nnames.groupby(\"Name\")[[\"Count\"]].mean().head()\n\n\n\n\n\n\n\n\nCount\n\n\nName\n\n\n\n\n\nAaban\n10.000000\n\n\nAabha\n6.375000\n\n\nAabid\n5.333333\n\n\nAabidah\n5.000000\n\n\nAabir\n5.000000\n\n\n\n\n\n\n\npandas also offers a number of in-built functions for aggregation. Some examples include:\n\n.sum()\n.max()\n.min()\n.mean()\n.first()\n.last()\n\nThe latter two entries in this list – \"first\" and \"last\" – are unique to pandas. They return the first or last entry in a subframe column. Why might this be useful? Consider a case where multiple columns in a group share identical information. To represent this information in the grouped output, we can simply grab the first or last entry, which we know will be identical to all other entries.\nLet’s illustrate this with an example. Say we add a new column to names that contains the first letter of each name.\n\n# Imagine we had an additional column, \"First Letter\". We'll explain this code next week\nnames[\"First Letter\"] = names[\"Name\"].apply(lambda x: x[0])\n\n# We construct a simplified DataFrame containing just a subset of columns\nnames_new = names[[\"Name\", \"First Letter\", \"Year\"]]\nnames_new.head()\n\n\n\n\n\n\n\n\nName\nFirst Letter\nYear\n\n\n\n\n0\nMary\nM\n1880\n\n\n1\nAnna\nA\n1880\n\n\n2\nEmma\nE\n1880\n\n\n3\nElizabeth\nE\n1880\n\n\n4\nMinnie\nM\n1880\n\n\n\n\n\n\n\nIf we form groups for each name in the dataset, \"First Letter\" will be the same for all members of the group. This means that if we simply select the first entry for \"First Letter\" in the group, we’ll represent all data in that group.\nWe can use a dictionary to apply different aggregation functions to each column during grouping.\n\n\n\n\n\nnames_new.groupby(\"Name\").agg({\"First Letter\":\"first\", \"Year\":\"max\"}).head()\n\n\n\n\n\n\n\n\nFirst Letter\nYear\n\n\nName\n\n\n\n\n\n\nAaban\nA\n2019\n\n\nAabha\nA\n2021\n\n\nAabid\nA\n2018\n\n\nAabidah\nA\n2018\n\n\nAabir\nA\n2018\n\n\n\n\n\n\n\nWe can also define aggregation functions of our own! This can be done using either a def or lambda statement. Again, the condition for a custom aggregation function is that it must take in a Series and output a single scalar value.\n\ndef ratio_to_peak(series):\n    return series.iloc[-1]/max(series)\n\nnames.groupby(\"Name\")[[\"Year\", \"Count\"]].apply(ratio_to_peak)\n\n\n---------------------------------------------------------------------------\n\nNameError                                 Traceback (most recent call last)\n\nInput In [2], in &lt;cell line: 4&gt;()\n\n      1 def ratio_to_peak(series):\n\n      2     return series.iloc[-1]/max(series)\n\n----&gt; 4 names.groupby(\"Name\")[[\"Year\", \"Count\"]].apply(ratio_to_peak)\n\n\n\nNameError: name 'names' is not defined\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nlambda functions are a special type of function that can be defined in a single line. They are also often refered to as “anonymous” functions because these functions don’t have a name. They are useful for simple functions that are not used elsewhere in your code.\n\n\n\n# Alternatively, using lambda\nnames.groupby(\"Name\")[[\"Year\", \"Count\"]].agg(lambda s: s.iloc[-1]/max(s))\n\n\n\n\n\n\n\n\nYear\nCount\n\n\nName\n\n\n\n\n\n\nAaban\n1.0\n0.375000\n\n\nAabha\n1.0\n0.555556\n\n\nAabid\n1.0\n1.000000\n\n\nAabidah\n1.0\n1.000000\n\n\nAabir\n1.0\n1.000000\n\n\n...\n...\n...\n\n\nZyvion\n1.0\n1.000000\n\n\nZyvon\n1.0\n1.000000\n\n\nZyyanna\n1.0\n1.000000\n\n\nZyyon\n1.0\n1.000000\n\n\nZzyzx\n1.0\n1.000000\n\n\n\n\n101338 rows × 2 columns\n\n\n\n\n\nWe’ll work with the elections DataFrame again.\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv\"\nelections = pd.read_csv(url)\nelections.head(5)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n\n\n\n\n\nWhat if we wish to aggregate our DataFrame using a non-standard function – for example, a function of our own design? We can do so by combining .agg with lambda expressions.\nLet’s first consider a puzzle to jog our memory. We will attempt to find the Candidate from each Party with the highest % of votes.\nA naive approach may be to group by the Party column and aggregate by the maximum.\n\nelections.groupby(\"Party\").agg(max).head(10)\n\n\n\n\n\n\n\n\nYear\nCandidate\nPopular vote\nResult\n%\n\n\nParty\n\n\n\n\n\n\n\n\n\nAmerican\n1976\nThomas J. Anderson\n873053\nloss\n21.554001\n\n\nAmerican Independent\n1976\nLester Maddox\n9901118\nloss\n13.571218\n\n\nAnti-Masonic\n1832\nWilliam Wirt\n100715\nloss\n7.821583\n\n\nAnti-Monopoly\n1884\nBenjamin Butler\n134294\nloss\n1.335838\n\n\nCitizens\n1980\nBarry Commoner\n233052\nloss\n0.270182\n\n\nCommunist\n1932\nWilliam Z. Foster\n103307\nloss\n0.261069\n\n\nConstitution\n2016\nMichael Peroutka\n203091\nloss\n0.152398\n\n\nConstitutional Union\n1860\nJohn Bell\n590901\nloss\n12.639283\n\n\nDemocratic\n2020\nWoodrow Wilson\n81268924\nwin\n61.344703\n\n\nDemocratic-Republican\n1824\nJohn Quincy Adams\n151271\nwin\n57.210122\n\n\n\n\n\n\n\nThis approach is clearly wrong – the DataFrame claims that Woodrow Wilson won the presidency in 2020.\nWhy is this happening? Here, the max aggregation function is taken over every column independently. Among Democrats, max is computing:\n\nThe most recent Year a Democratic candidate ran for president (2020)\nThe Candidate with the alphabetically “largest” name (“Woodrow Wilson”)\nThe Result with the alphabetically “largest” outcome (“win”)\n\nInstead, let’s try a different approach. We will:\n\nSort the DataFrame so that rows are in descending order of %\nGroup by Party and select the first row of each sub-DataFrame\n\nWhile it may seem unintuitive, sorting elections by descending order of % is extremely helpful. If we then group by Party, the first row of each groupby object will contain information about the Candidate with the highest voter %.\n\nelections_sorted_by_percent = elections.sort_values(\"%\", ascending=False)\nelections_sorted_by_percent.head(5)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n114\n1964\nLyndon Johnson\nDemocratic\n43127041\nwin\n61.344703\n\n\n91\n1936\nFranklin Roosevelt\nDemocratic\n27752648\nwin\n60.978107\n\n\n120\n1972\nRichard Nixon\nRepublican\n47168710\nwin\n60.907806\n\n\n79\n1920\nWarren Harding\nRepublican\n16144093\nwin\n60.574501\n\n\n133\n1984\nRonald Reagan\nRepublican\n54455472\nwin\n59.023326\n\n\n\n\n\n\n\n\nelections_sorted_by_percent.groupby(\"Party\").agg(lambda x : x.iloc[0]).head(10)\n\n# Equivalent to the below code\n# elections_sorted_by_percent.groupby(\"Party\").agg('first').head(10)\n\n\n\n\n\n\n\n\nYear\nCandidate\nPopular vote\nResult\n%\n\n\nParty\n\n\n\n\n\n\n\n\n\nAmerican\n1856\nMillard Fillmore\n873053\nloss\n21.554001\n\n\nAmerican Independent\n1968\nGeorge Wallace\n9901118\nloss\n13.571218\n\n\nAnti-Masonic\n1832\nWilliam Wirt\n100715\nloss\n7.821583\n\n\nAnti-Monopoly\n1884\nBenjamin Butler\n134294\nloss\n1.335838\n\n\nCitizens\n1980\nBarry Commoner\n233052\nloss\n0.270182\n\n\nCommunist\n1932\nWilliam Z. Foster\n103307\nloss\n0.261069\n\n\nConstitution\n2008\nChuck Baldwin\n199750\nloss\n0.152398\n\n\nConstitutional Union\n1860\nJohn Bell\n590901\nloss\n12.639283\n\n\nDemocratic\n1964\nLyndon Johnson\n43127041\nwin\n61.344703\n\n\nDemocratic-Republican\n1824\nAndrew Jackson\n151271\nloss\n57.210122\n\n\n\n\n\n\n\nHere’s an illustration of the process:\n\n\n\nNotice how our code correctly determines that Lyndon Johnson from the Democratic Party has the highest voter %.\nMore generally, lambda functions are used to design custom aggregation functions that aren’t pre-defined by Python. The input parameter x to the lambda function is a GroupBy object. Therefore, it should make sense why lambda x : x.iloc[0] selects the first row in each groupby object.\nIn fact, there’s a few different ways to approach this problem. Each approach has different tradeoffs in terms of readability, performance, memory consumption, complexity, etc. We’ve given a few examples below.\nNote: Understanding these alternative solutions is not required. They are given to demonstrate the vast number of problem-solving approaches in pandas.\n\n# Using the idxmax function\nbest_per_party = elections.loc[elections.groupby('Party')['%'].idxmax()]\nbest_per_party.head(5)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n22\n1856\nMillard Fillmore\nAmerican\n873053\nloss\n21.554001\n\n\n115\n1968\nGeorge Wallace\nAmerican Independent\n9901118\nloss\n13.571218\n\n\n6\n1832\nWilliam Wirt\nAnti-Masonic\n100715\nloss\n7.821583\n\n\n38\n1884\nBenjamin Butler\nAnti-Monopoly\n134294\nloss\n1.335838\n\n\n127\n1980\nBarry Commoner\nCitizens\n233052\nloss\n0.270182\n\n\n\n\n\n\n\n\n# Using the .drop_duplicates function\nbest_per_party2 = elections.sort_values('%').drop_duplicates(['Party'], keep='last')\nbest_per_party2.head(5)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n148\n1996\nJohn Hagelin\nNatural Law\n113670\nloss\n0.118219\n\n\n164\n2008\nChuck Baldwin\nConstitution\n199750\nloss\n0.152398\n\n\n110\n1956\nT. Coleman Andrews\nStates' Rights\n107929\nloss\n0.174883\n\n\n147\n1996\nHoward Phillips\nTaxpayers\n184656\nloss\n0.192045\n\n\n136\n1988\nLenora Fulani\nNew Alliance\n217221\nloss\n0.237804\n\n\n\n\n\n\n\n\n\n\nThere are many aggregation methods we can use with .agg. Some useful options are:\n\n.mean: creates a new DataFrame with the mean value of each group\n.sum: creates a new DataFrame with the sum of each group\n.max and .min: creates a new DataFrame with the maximum/minimum value of each group\n.first and .last: creates a new DataFrame with the first/last row in each group\n.size: creates a new Series with the number of entries in each group\n.count: creates a new DataFrame with the number of entries, excluding missing values.\n\nNote the slight difference between .size() and .count(): while .size() returns a Series and counts the number of entries including the missing values, .count() returns a DataFrame and counts the number of entries in each column excluding missing values. Here’s an example:\n\ndf = pd.DataFrame({'letter':['A','A','B','C','C','C'], \n                   'num':[1,2,3,4,None,4], \n                   'state':[None, 'tx', 'fl', 'hi', None, 'ak']})\ndf\n\n\n\n\n\n\n\n\nletter\nnum\nstate\n\n\n\n\n0\nA\n1.0\nNone\n\n\n1\nA\n2.0\ntx\n\n\n2\nB\n3.0\nfl\n\n\n3\nC\n4.0\nhi\n\n\n4\nC\nNaN\nNone\n\n\n5\nC\n4.0\nak\n\n\n\n\n\n\n\n\ndf.groupby(\"letter\").size()\n\nletter\nA    2\nB    1\nC    3\ndtype: int64\n\n\n\ndf.groupby(\"letter\").count()\n\n\n\n\n\n\n\n\nnum\nstate\n\n\nletter\n\n\n\n\n\n\nA\n2\n1\n\n\nB\n1\n1\n\n\nC\n2\n2\n\n\n\n\n\n\n\nYou might recall that the value_counts() function in the previous note does something similar. It turns out value_counts() and groupby.size() are the same, except value_counts() sorts the resulting Series in descending order automatically.\n\ndf[\"letter\"].value_counts()\n\nC    3\nA    2\nB    1\nName: letter, dtype: int64\n\n\nhese (and other) aggregation functions are so common that pandas allows for writing shorthand. Instead of explicitly stating the use of .agg, we can call the function directly on the GroupBy object.\nFor example, the following are equivalent:\n\nelections.groupby(\"Candidate\").agg(mean)\nelections.groupby(\"Candidate\").mean()\n\nThere are many other methods that pandas supports. You can check them out on the pandas documentation.\n\n\n\n\n\n\nelections.groupby(\"Year\").filter(lambda sf: sf[\"%\"].max() &lt; 45).head(9)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n23\n1860\nAbraham Lincoln\nRepublican\n1855993\nwin\n39.699408\n\n\n24\n1860\nJohn Bell\nConstitutional Union\n590901\nloss\n12.639283\n\n\n25\n1860\nJohn C. Breckinridge\nSouthern Democratic\n848019\nloss\n18.138998\n\n\n26\n1860\nStephen A. Douglas\nNorthern Democratic\n1380202\nloss\n29.522311\n\n\n66\n1912\nEugene V. Debs\nSocialist\n901551\nloss\n6.004354\n\n\n67\n1912\nEugene W. Chafin\nProhibition\n208156\nloss\n1.386325\n\n\n68\n1912\nTheodore Roosevelt\nProgressive\n4122721\nloss\n27.457433\n\n\n69\n1912\nWilliam Taft\nRepublican\n3486242\nloss\n23.218466\n\n\n70\n1912\nWoodrow Wilson\nDemocratic\n6296284\nwin\n41.933422\n\n\n\n\n\n\n\nWhat’s going on here? In this example, we’ve defined our filtering function, \\(\\text{f}\\), to be lambda sf: sf[\"%\"].max() &lt; 45. This filtering function will find the maximum \"%\" value among all entries in the grouped sub-DataFrame, which we call sf. If the maximum value is less than 45, then the filter function will return True and all rows in that grouped sub-DataFrame will appear in the final output DataFrame.\nExamine the DataFrame above. Notice how, in this preview of the first 9 rows, all entries from the years 1860 and 1912 appear. This means that in 1860 and 1912, no candidate in that year won more than 45% of the total vote.\nYou may ask: how is the groupby.filter procedure different to the boolean filtering we’ve seen previously? Boolean filtering considers individual rows when applying a boolean condition. For example, the code elections[elections[\"%\"] &lt; 45] will check the \"%\" value of every single row in elections; if it is less than 45, then that row will be kept in the output. groupby.filter, in contrast, applies a boolean condition across all rows in a group. If not all rows in that group satisfy the condition specified by the filter, the entire group will be discarded in the output.",
    "crumbs": [
      "Home",
      "Aggregation"
    ]
  },
  {
    "objectID": "pandas/grouping.html#groupby",
    "href": "pandas/grouping.html#groupby",
    "title": "Aggregation",
    "section": "",
    "text": "Up until this point, we have been working with individual rows of DataFrames. As data scientists, we often wish to investigate trends across a larger subset of our data. For example, we may want to compute some summary statistic (the mean, median, sum, etc.) for a group of rows in our DataFrame. To do this, we’ll use pandas GroupBy objects.\n\n\n\n\n\n\n\n\n\nA groupby operation involves some combination of splitting a DataFrame into grouped subframes, applying a function, and combining the results.\nFor some arbitrary DataFrame df below, the code df.groupby(\"year\").sum() does the following:\n\nSplits the DataFrame into sub-DataFrames with rows belonging to the same year.\nApplies the sum function to each column of each sub-DataFrame.\nCombines the results of sum into a single DataFrame, indexed by year.\n\n\n\n\nLet’s say we had baby names for all years in a single DataFrame names\n\nimport urllib.request\nimport os.path\nimport pandas as pd \n\n# Download data from the web directly\ndata_url = \"https://www.ssa.gov/oact/babynames/names.zip\"\nlocal_filename = \"../data/names.zip\"\nif not os.path.exists(local_filename): # if the data exists don't download again\n    with urllib.request.urlopen(data_url) as resp, open(local_filename, 'wb') as f:\n        f.write(resp.read())\n\n        \n# Load data without unzipping the file\nimport zipfile\nnames = [] \nwith zipfile.ZipFile(local_filename, \"r\") as zf:\n    data_files = [f for f in zf.filelist if f.filename[-3:] == \"txt\"]\n    def extract_year_from_filename(fn):\n        return int(fn[3:7])\n    for f in data_files:\n        year = extract_year_from_filename(f.filename)\n        with zf.open(f) as fp:\n            df = pd.read_csv(fp, names=[\"Name\", \"Sex\", \"Count\"])\n            df[\"Year\"] = year\n            names.append(df)\nnames = pd.concat(names)\n\nnames\n\n\n\n\n\n\n\n\nName\nSex\nCount\nYear\n\n\n\n\n0\nMary\nF\n7065\n1880\n\n\n1\nAnna\nF\n2604\n1880\n\n\n2\nEmma\nF\n2003\n1880\n\n\n3\nElizabeth\nF\n1939\n1880\n\n\n4\nMinnie\nF\n1746\n1880\n\n\n...\n...\n...\n...\n...\n\n\n31677\nZyell\nM\n5\n2023\n\n\n31678\nZyen\nM\n5\n2023\n\n\n31679\nZymirr\nM\n5\n2023\n\n\n31680\nZyquan\nM\n5\n2023\n\n\n31681\nZyrin\nM\n5\n2023\n\n\n\n\n2117219 rows × 4 columns\n\n\n\n\nnames.to_csv(\"../data/names.csv\", index=False)\n\nNow, if we wanted to aggregate all rows in names for a given year, we would need names.groupby(\"Year\")\n\nnames.groupby(\"Year\")\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f7a30b64bb0&gt;\n\n\nWhat does this strange output mean? Calling .groupby has generated a GroupBy object. You can imagine this as a set of “mini” sub-DataFrames, where each subframe contains all of the rows from names that correspond to a particular year.\nThe diagram below shows a simplified view of names to help illustrate this idea.\n\n\n\nWe can’t work with a GroupBy object directly – that is why you saw that strange output earlier, rather than a standard view of a DataFrame. To actually manipulate values within these “mini” DataFrames, we’ll need to call an aggregation method. This is a method that tells pandas how to aggregate the values within the GroupBy object. Once the aggregation is applied, pandas will return a normal (now grouped) DataFrame.\nAggregation functions (.min(), .max(), .mean(), .sum(), etc.) are the most common way to work with GroupBy objects. These functions are applied to each column of a “mini” grouped DataFrame. We end up with a new DataFrame with one aggregated row per subframe. Let’s see this in action by finding the sum of all counts for each year in names – this is equivalent to finding the number of babies born in each year.\n\nnames.groupby(\"Year\").sum().head(5)\n\n\n\n\n\n\n\n\nCount\n\n\nYear\n\n\n\n\n\n1880\n201484\n\n\n1881\n192690\n\n\n1882\n221533\n\n\n1883\n216944\n\n\n1884\n243461\n\n\n\n\n\n\n\nWe can relate this back to the diagram we used above. Remember that the diagram uses a simplified version of names, which is why we see smaller values for the summed counts.\n\n\n\n\nCalling .agg has condensed each subframe back into a single row. This gives us our final output: a DataFrame that is now indexed by \"Year\", with a single row for each unique year in the original names DataFrame.\nYou may be wondering: where did the \"State\", \"Sex\", and \"Name\" columns go? Logically, it doesn’t make sense to sum the string data in these columns (how would we add “Mary” + “Ann”?). Because of this, pandas will simply omit these columns when it performs the aggregation on the DataFrame. Since this happens implicitly, without the user specifying that these columns should be ignored, it’s easy to run into troubling situations where columns are removed without the programmer noticing. It is better coding practice to select only the columns we care about before performing the aggregation.\n\n# Same result, but now we explicitly tell pandas to only consider the \"Count\" column when summing\nnames.groupby(\"Year\")[[\"Count\"]].sum().head(5)\n\n\n\n\n\n\n\n\nCount\n\n\nYear\n\n\n\n\n\n1880\n201484\n\n\n1881\n192690\n\n\n1882\n221533\n\n\n1883\n216944\n\n\n1884\n243461\n\n\n\n\n\n\n\nThere are many different aggregations that can be applied to the grouped data. The primary requirement is that an aggregation function must:\n\nTake in a Series of data (a single column of the grouped subframe)\nReturn a single value that aggregates this Series\n\nBecause of this fairly broad requirement, pandas offers many ways of computing an aggregation.\nIn-built Python operations – such as sum, max, and min – are automatically recognized by pandas.\n\n# What is the maximum count for each name in any year?\nnames.groupby(\"Name\")[[\"Count\"]].max().head()\n\n\n\n\n\n\n\n\nCount\n\n\nName\n\n\n\n\n\nAaban\n16\n\n\nAabha\n9\n\n\nAabid\n6\n\n\nAabidah\n5\n\n\nAabir\n5\n\n\n\n\n\n\n\n\n# What is the minimum count for each name in any year?\nnames.groupby(\"Name\")[[\"Count\"]].min().head()\n\n\n\n\n\n\n\n\nCount\n\n\nName\n\n\n\n\n\nAaban\n5\n\n\nAabha\n5\n\n\nAabid\n5\n\n\nAabidah\n5\n\n\nAabir\n5\n\n\n\n\n\n\n\n\n# What is the average count for each name across all years?\nnames.groupby(\"Name\")[[\"Count\"]].mean().head()\n\n\n\n\n\n\n\n\nCount\n\n\nName\n\n\n\n\n\nAaban\n10.000000\n\n\nAabha\n6.375000\n\n\nAabid\n5.333333\n\n\nAabidah\n5.000000\n\n\nAabir\n5.000000\n\n\n\n\n\n\n\npandas also offers a number of in-built functions for aggregation. Some examples include:\n\n.sum()\n.max()\n.min()\n.mean()\n.first()\n.last()\n\nThe latter two entries in this list – \"first\" and \"last\" – are unique to pandas. They return the first or last entry in a subframe column. Why might this be useful? Consider a case where multiple columns in a group share identical information. To represent this information in the grouped output, we can simply grab the first or last entry, which we know will be identical to all other entries.\nLet’s illustrate this with an example. Say we add a new column to names that contains the first letter of each name.\n\n# Imagine we had an additional column, \"First Letter\". We'll explain this code next week\nnames[\"First Letter\"] = names[\"Name\"].apply(lambda x: x[0])\n\n# We construct a simplified DataFrame containing just a subset of columns\nnames_new = names[[\"Name\", \"First Letter\", \"Year\"]]\nnames_new.head()\n\n\n\n\n\n\n\n\nName\nFirst Letter\nYear\n\n\n\n\n0\nMary\nM\n1880\n\n\n1\nAnna\nA\n1880\n\n\n2\nEmma\nE\n1880\n\n\n3\nElizabeth\nE\n1880\n\n\n4\nMinnie\nM\n1880\n\n\n\n\n\n\n\nIf we form groups for each name in the dataset, \"First Letter\" will be the same for all members of the group. This means that if we simply select the first entry for \"First Letter\" in the group, we’ll represent all data in that group.\nWe can use a dictionary to apply different aggregation functions to each column during grouping.\n\n\n\n\n\nnames_new.groupby(\"Name\").agg({\"First Letter\":\"first\", \"Year\":\"max\"}).head()\n\n\n\n\n\n\n\n\nFirst Letter\nYear\n\n\nName\n\n\n\n\n\n\nAaban\nA\n2019\n\n\nAabha\nA\n2021\n\n\nAabid\nA\n2018\n\n\nAabidah\nA\n2018\n\n\nAabir\nA\n2018\n\n\n\n\n\n\n\nWe can also define aggregation functions of our own! This can be done using either a def or lambda statement. Again, the condition for a custom aggregation function is that it must take in a Series and output a single scalar value.\n\ndef ratio_to_peak(series):\n    return series.iloc[-1]/max(series)\n\nnames.groupby(\"Name\")[[\"Year\", \"Count\"]].apply(ratio_to_peak)\n\n\n---------------------------------------------------------------------------\n\nNameError                                 Traceback (most recent call last)\n\nInput In [2], in &lt;cell line: 4&gt;()\n\n      1 def ratio_to_peak(series):\n\n      2     return series.iloc[-1]/max(series)\n\n----&gt; 4 names.groupby(\"Name\")[[\"Year\", \"Count\"]].apply(ratio_to_peak)\n\n\n\nNameError: name 'names' is not defined\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nlambda functions are a special type of function that can be defined in a single line. They are also often refered to as “anonymous” functions because these functions don’t have a name. They are useful for simple functions that are not used elsewhere in your code.\n\n\n\n# Alternatively, using lambda\nnames.groupby(\"Name\")[[\"Year\", \"Count\"]].agg(lambda s: s.iloc[-1]/max(s))\n\n\n\n\n\n\n\n\nYear\nCount\n\n\nName\n\n\n\n\n\n\nAaban\n1.0\n0.375000\n\n\nAabha\n1.0\n0.555556\n\n\nAabid\n1.0\n1.000000\n\n\nAabidah\n1.0\n1.000000\n\n\nAabir\n1.0\n1.000000\n\n\n...\n...\n...\n\n\nZyvion\n1.0\n1.000000\n\n\nZyvon\n1.0\n1.000000\n\n\nZyyanna\n1.0\n1.000000\n\n\nZyyon\n1.0\n1.000000\n\n\nZzyzx\n1.0\n1.000000\n\n\n\n\n101338 rows × 2 columns\n\n\n\n\n\nWe’ll work with the elections DataFrame again.\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv\"\nelections = pd.read_csv(url)\nelections.head(5)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n\n\n\n\n\nWhat if we wish to aggregate our DataFrame using a non-standard function – for example, a function of our own design? We can do so by combining .agg with lambda expressions.\nLet’s first consider a puzzle to jog our memory. We will attempt to find the Candidate from each Party with the highest % of votes.\nA naive approach may be to group by the Party column and aggregate by the maximum.\n\nelections.groupby(\"Party\").agg(max).head(10)\n\n\n\n\n\n\n\n\nYear\nCandidate\nPopular vote\nResult\n%\n\n\nParty\n\n\n\n\n\n\n\n\n\nAmerican\n1976\nThomas J. Anderson\n873053\nloss\n21.554001\n\n\nAmerican Independent\n1976\nLester Maddox\n9901118\nloss\n13.571218\n\n\nAnti-Masonic\n1832\nWilliam Wirt\n100715\nloss\n7.821583\n\n\nAnti-Monopoly\n1884\nBenjamin Butler\n134294\nloss\n1.335838\n\n\nCitizens\n1980\nBarry Commoner\n233052\nloss\n0.270182\n\n\nCommunist\n1932\nWilliam Z. Foster\n103307\nloss\n0.261069\n\n\nConstitution\n2016\nMichael Peroutka\n203091\nloss\n0.152398\n\n\nConstitutional Union\n1860\nJohn Bell\n590901\nloss\n12.639283\n\n\nDemocratic\n2020\nWoodrow Wilson\n81268924\nwin\n61.344703\n\n\nDemocratic-Republican\n1824\nJohn Quincy Adams\n151271\nwin\n57.210122\n\n\n\n\n\n\n\nThis approach is clearly wrong – the DataFrame claims that Woodrow Wilson won the presidency in 2020.\nWhy is this happening? Here, the max aggregation function is taken over every column independently. Among Democrats, max is computing:\n\nThe most recent Year a Democratic candidate ran for president (2020)\nThe Candidate with the alphabetically “largest” name (“Woodrow Wilson”)\nThe Result with the alphabetically “largest” outcome (“win”)\n\nInstead, let’s try a different approach. We will:\n\nSort the DataFrame so that rows are in descending order of %\nGroup by Party and select the first row of each sub-DataFrame\n\nWhile it may seem unintuitive, sorting elections by descending order of % is extremely helpful. If we then group by Party, the first row of each groupby object will contain information about the Candidate with the highest voter %.\n\nelections_sorted_by_percent = elections.sort_values(\"%\", ascending=False)\nelections_sorted_by_percent.head(5)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n114\n1964\nLyndon Johnson\nDemocratic\n43127041\nwin\n61.344703\n\n\n91\n1936\nFranklin Roosevelt\nDemocratic\n27752648\nwin\n60.978107\n\n\n120\n1972\nRichard Nixon\nRepublican\n47168710\nwin\n60.907806\n\n\n79\n1920\nWarren Harding\nRepublican\n16144093\nwin\n60.574501\n\n\n133\n1984\nRonald Reagan\nRepublican\n54455472\nwin\n59.023326\n\n\n\n\n\n\n\n\nelections_sorted_by_percent.groupby(\"Party\").agg(lambda x : x.iloc[0]).head(10)\n\n# Equivalent to the below code\n# elections_sorted_by_percent.groupby(\"Party\").agg('first').head(10)\n\n\n\n\n\n\n\n\nYear\nCandidate\nPopular vote\nResult\n%\n\n\nParty\n\n\n\n\n\n\n\n\n\nAmerican\n1856\nMillard Fillmore\n873053\nloss\n21.554001\n\n\nAmerican Independent\n1968\nGeorge Wallace\n9901118\nloss\n13.571218\n\n\nAnti-Masonic\n1832\nWilliam Wirt\n100715\nloss\n7.821583\n\n\nAnti-Monopoly\n1884\nBenjamin Butler\n134294\nloss\n1.335838\n\n\nCitizens\n1980\nBarry Commoner\n233052\nloss\n0.270182\n\n\nCommunist\n1932\nWilliam Z. Foster\n103307\nloss\n0.261069\n\n\nConstitution\n2008\nChuck Baldwin\n199750\nloss\n0.152398\n\n\nConstitutional Union\n1860\nJohn Bell\n590901\nloss\n12.639283\n\n\nDemocratic\n1964\nLyndon Johnson\n43127041\nwin\n61.344703\n\n\nDemocratic-Republican\n1824\nAndrew Jackson\n151271\nloss\n57.210122\n\n\n\n\n\n\n\nHere’s an illustration of the process:\n\n\n\nNotice how our code correctly determines that Lyndon Johnson from the Democratic Party has the highest voter %.\nMore generally, lambda functions are used to design custom aggregation functions that aren’t pre-defined by Python. The input parameter x to the lambda function is a GroupBy object. Therefore, it should make sense why lambda x : x.iloc[0] selects the first row in each groupby object.\nIn fact, there’s a few different ways to approach this problem. Each approach has different tradeoffs in terms of readability, performance, memory consumption, complexity, etc. We’ve given a few examples below.\nNote: Understanding these alternative solutions is not required. They are given to demonstrate the vast number of problem-solving approaches in pandas.\n\n# Using the idxmax function\nbest_per_party = elections.loc[elections.groupby('Party')['%'].idxmax()]\nbest_per_party.head(5)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n22\n1856\nMillard Fillmore\nAmerican\n873053\nloss\n21.554001\n\n\n115\n1968\nGeorge Wallace\nAmerican Independent\n9901118\nloss\n13.571218\n\n\n6\n1832\nWilliam Wirt\nAnti-Masonic\n100715\nloss\n7.821583\n\n\n38\n1884\nBenjamin Butler\nAnti-Monopoly\n134294\nloss\n1.335838\n\n\n127\n1980\nBarry Commoner\nCitizens\n233052\nloss\n0.270182\n\n\n\n\n\n\n\n\n# Using the .drop_duplicates function\nbest_per_party2 = elections.sort_values('%').drop_duplicates(['Party'], keep='last')\nbest_per_party2.head(5)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n148\n1996\nJohn Hagelin\nNatural Law\n113670\nloss\n0.118219\n\n\n164\n2008\nChuck Baldwin\nConstitution\n199750\nloss\n0.152398\n\n\n110\n1956\nT. Coleman Andrews\nStates' Rights\n107929\nloss\n0.174883\n\n\n147\n1996\nHoward Phillips\nTaxpayers\n184656\nloss\n0.192045\n\n\n136\n1988\nLenora Fulani\nNew Alliance\n217221\nloss\n0.237804\n\n\n\n\n\n\n\n\n\n\nThere are many aggregation methods we can use with .agg. Some useful options are:\n\n.mean: creates a new DataFrame with the mean value of each group\n.sum: creates a new DataFrame with the sum of each group\n.max and .min: creates a new DataFrame with the maximum/minimum value of each group\n.first and .last: creates a new DataFrame with the first/last row in each group\n.size: creates a new Series with the number of entries in each group\n.count: creates a new DataFrame with the number of entries, excluding missing values.\n\nNote the slight difference between .size() and .count(): while .size() returns a Series and counts the number of entries including the missing values, .count() returns a DataFrame and counts the number of entries in each column excluding missing values. Here’s an example:\n\ndf = pd.DataFrame({'letter':['A','A','B','C','C','C'], \n                   'num':[1,2,3,4,None,4], \n                   'state':[None, 'tx', 'fl', 'hi', None, 'ak']})\ndf\n\n\n\n\n\n\n\n\nletter\nnum\nstate\n\n\n\n\n0\nA\n1.0\nNone\n\n\n1\nA\n2.0\ntx\n\n\n2\nB\n3.0\nfl\n\n\n3\nC\n4.0\nhi\n\n\n4\nC\nNaN\nNone\n\n\n5\nC\n4.0\nak\n\n\n\n\n\n\n\n\ndf.groupby(\"letter\").size()\n\nletter\nA    2\nB    1\nC    3\ndtype: int64\n\n\n\ndf.groupby(\"letter\").count()\n\n\n\n\n\n\n\n\nnum\nstate\n\n\nletter\n\n\n\n\n\n\nA\n2\n1\n\n\nB\n1\n1\n\n\nC\n2\n2\n\n\n\n\n\n\n\nYou might recall that the value_counts() function in the previous note does something similar. It turns out value_counts() and groupby.size() are the same, except value_counts() sorts the resulting Series in descending order automatically.\n\ndf[\"letter\"].value_counts()\n\nC    3\nA    2\nB    1\nName: letter, dtype: int64\n\n\nhese (and other) aggregation functions are so common that pandas allows for writing shorthand. Instead of explicitly stating the use of .agg, we can call the function directly on the GroupBy object.\nFor example, the following are equivalent:\n\nelections.groupby(\"Candidate\").agg(mean)\nelections.groupby(\"Candidate\").mean()\n\nThere are many other methods that pandas supports. You can check them out on the pandas documentation.\n\n\n\n\n\n\nelections.groupby(\"Year\").filter(lambda sf: sf[\"%\"].max() &lt; 45).head(9)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n23\n1860\nAbraham Lincoln\nRepublican\n1855993\nwin\n39.699408\n\n\n24\n1860\nJohn Bell\nConstitutional Union\n590901\nloss\n12.639283\n\n\n25\n1860\nJohn C. Breckinridge\nSouthern Democratic\n848019\nloss\n18.138998\n\n\n26\n1860\nStephen A. Douglas\nNorthern Democratic\n1380202\nloss\n29.522311\n\n\n66\n1912\nEugene V. Debs\nSocialist\n901551\nloss\n6.004354\n\n\n67\n1912\nEugene W. Chafin\nProhibition\n208156\nloss\n1.386325\n\n\n68\n1912\nTheodore Roosevelt\nProgressive\n4122721\nloss\n27.457433\n\n\n69\n1912\nWilliam Taft\nRepublican\n3486242\nloss\n23.218466\n\n\n70\n1912\nWoodrow Wilson\nDemocratic\n6296284\nwin\n41.933422\n\n\n\n\n\n\n\nWhat’s going on here? In this example, we’ve defined our filtering function, \\(\\text{f}\\), to be lambda sf: sf[\"%\"].max() &lt; 45. This filtering function will find the maximum \"%\" value among all entries in the grouped sub-DataFrame, which we call sf. If the maximum value is less than 45, then the filter function will return True and all rows in that grouped sub-DataFrame will appear in the final output DataFrame.\nExamine the DataFrame above. Notice how, in this preview of the first 9 rows, all entries from the years 1860 and 1912 appear. This means that in 1860 and 1912, no candidate in that year won more than 45% of the total vote.\nYou may ask: how is the groupby.filter procedure different to the boolean filtering we’ve seen previously? Boolean filtering considers individual rows when applying a boolean condition. For example, the code elections[elections[\"%\"] &lt; 45] will check the \"%\" value of every single row in elections; if it is less than 45, then that row will be kept in the output. groupby.filter, in contrast, applies a boolean condition across all rows in a group. If not all rows in that group satisfy the condition specified by the filter, the entire group will be discarded in the output.",
    "crumbs": [
      "Home",
      "Aggregation"
    ]
  },
  {
    "objectID": "pandas/view_data.html",
    "href": "pandas/view_data.html",
    "title": "Index and Columns",
    "section": "",
    "text": "import pandas as pd \n\nurl = \"https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv\"\n\nelections = pd.read_csv(url)",
    "crumbs": [
      "Home",
      "Index and Columns"
    ]
  },
  {
    "objectID": "pandas/view_data.html#columns",
    "href": "pandas/view_data.html#columns",
    "title": "Index and Columns",
    "section": ".columns",
    "text": ".columns\nThe .columns attribute returns the column labels of the DataFrame. It is a pandas Index object.\n\nelections.head()\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n\n\n\n\n\n\nelections.columns\n\nIndex(['Year', 'Candidate', 'Party', 'Popular vote', 'Result', '%'], dtype='object')\n\n\n\ntype(elections.columns)\n\npandas.core.indexes.base.Index",
    "crumbs": [
      "Home",
      "Index and Columns"
    ]
  },
  {
    "objectID": "pandas/view_data.html#index",
    "href": "pandas/view_data.html#index",
    "title": "Index and Columns",
    "section": ".index",
    "text": ".index\n.index attribute returns the row labels of the DataFrame. It is also a pandas Index object.\n\nelections.index\n\nRangeIndex(start=0, stop=182, step=1)\n\n\n\ntype(elections.index)\n\npandas.core.indexes.range.RangeIndex",
    "crumbs": [
      "Home",
      "Index and Columns"
    ]
  },
  {
    "objectID": "pandas/view_data.html#converting-column-to-index",
    "href": "pandas/view_data.html#converting-column-to-index",
    "title": "Index and Columns",
    "section": "Converting column to index",
    "text": "Converting column to index\nThe .set_index() method is used to set the DataFrame index using existing columns.\n\nelections.set_index(\"Candidate\")\n\n\n\n\n\n\n\n\nYear\nParty\nPopular vote\nResult\n%\n\n\nCandidate\n\n\n\n\n\n\n\n\n\nAndrew Jackson\n1824\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\nJohn Quincy Adams\n1824\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\nAndrew Jackson\n1828\nDemocratic\n642806\nwin\n56.203927\n\n\nJohn Quincy Adams\n1828\nNational Republican\n500897\nloss\n43.796073\n\n\nAndrew Jackson\n1832\nDemocratic\n702735\nwin\n54.574789\n\n\n...\n...\n...\n...\n...\n...\n\n\nJill Stein\n2016\nGreen\n1457226\nloss\n1.073699\n\n\nJoseph Biden\n2020\nDemocratic\n81268924\nwin\n51.311515\n\n\nDonald Trump\n2020\nRepublican\n74216154\nloss\n46.858542\n\n\nJo Jorgensen\n2020\nLibertarian\n1865724\nloss\n1.177979\n\n\nHoward Hawkins\n2020\nGreen\n405035\nloss\n0.255731\n\n\n\n\n182 rows × 5 columns",
    "crumbs": [
      "Home",
      "Index and Columns"
    ]
  },
  {
    "objectID": "pandas/view_data.html#converting-index-to-column",
    "href": "pandas/view_data.html#converting-index-to-column",
    "title": "Index and Columns",
    "section": "Converting Index to Column",
    "text": "Converting Index to Column\nThe .reset_index() method is used to reset the index of a DataFrame. By default, the original index is stored in a new column called index.\n\nelections.reset_index()\n\n\n\n\n\n\n\n\nindex\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n177\n177\n2016\nJill Stein\nGreen\n1457226\nloss\n1.073699\n\n\n178\n178\n2020\nJoseph Biden\nDemocratic\n81268924\nwin\n51.311515\n\n\n179\n179\n2020\nDonald Trump\nRepublican\n74216154\nloss\n46.858542\n\n\n180\n180\n2020\nJo Jorgensen\nLibertarian\n1865724\nloss\n1.177979\n\n\n181\n181\n2020\nHoward Hawkins\nGreen\n405035\nloss\n0.255731\n\n\n\n\n182 rows × 7 columns",
    "crumbs": [
      "Home",
      "Index and Columns"
    ]
  },
  {
    "objectID": "pandas/view_data.html#creating-new-columns",
    "href": "pandas/view_data.html#creating-new-columns",
    "title": "Index and Columns",
    "section": "Creating New Columns",
    "text": "Creating New Columns\nCreating new columns in a DataFrame is a common task when working with data. In this notebook, we will see how to create new columns in a DataFrame based on existing columns or other values.\n\n\n\nNew columns can be created by assigning a value to a new column name. For example, to create a new column named new_column with a constant value 10, we can use the following code:\n\nelections['constant'] = 10\n\nelections.head()\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\nconstant\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n10\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n10\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n10\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n10\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n10\n\n\n\n\n\n\n\n\nIf we want to create a new column based on an existing column, we can refer to the existing column by its name, within the square brackets, on the right side of the assignment operator. For example, to create a new column named new_column with the values of the existing column column1, we can use the following code:\n\n\nelections['total_voters'] = ((elections['Popular vote']* 100) / elections['%']).astype(int)\n\nelections.head()\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\nconstant\ntotal_voters\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n10\n264413\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n10\n264412\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n10\n1143702\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n10\n1143703\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n10\n1287655\n\n\n\n\n\n\n\nNote that the new column will have the same length as the DataFrame and the calculations are element-wise. That is, the value of the new column at row i will be calculated based on the value of the existing column at row i.\n\n\n\nThese element-wise operations are vectorized and are very efficient.",
    "crumbs": [
      "Home",
      "Index and Columns"
    ]
  },
  {
    "objectID": "pandas/view_data.html#renaming-columns",
    "href": "pandas/view_data.html#renaming-columns",
    "title": "Index and Columns",
    "section": "Renaming Columns",
    "text": "Renaming Columns\nThe .rename() method is used to rename the columns or index labels of a DataFrame.\n\nelections.rename(columns={\"Candidate\":\"Name\"})\n\n\n\n\n\n\n\n\nYear\nName\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n177\n2016\nJill Stein\nGreen\n1457226\nloss\n1.073699\n\n\n178\n2020\nJoseph Biden\nDemocratic\n81268924\nwin\n51.311515\n\n\n179\n2020\nDonald Trump\nRepublican\n74216154\nloss\n46.858542\n\n\n180\n2020\nJo Jorgensen\nLibertarian\n1865724\nloss\n1.177979\n\n\n181\n2020\nHoward Hawkins\nGreen\n405035\nloss\n0.255731\n\n\n\n\n182 rows × 6 columns",
    "crumbs": [
      "Home",
      "Index and Columns"
    ]
  },
  {
    "objectID": "pandas/view_data.html#sorting-data-by-column-value",
    "href": "pandas/view_data.html#sorting-data-by-column-value",
    "title": "Index and Columns",
    "section": "Sorting Data by Column Value",
    "text": "Sorting Data by Column Value\nOrdering a DataFrame can be useful for isolating extreme values. For example, the first 5 entries of a row sorted in descending order (that is, from highest to lowest) are the largest 5 values. .sort_values allows us to order a DataFrame or Series by a specified column. We can choose to either receive the rows in ascending order (default) or descending order.\n\n# Sort the \"Count\" column from highest to lowest\nelections.sort_values(by = \"%\", ascending=False).head()\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n114\n1964\nLyndon Johnson\nDemocratic\n43127041\nwin\n61.344703\n\n\n91\n1936\nFranklin Roosevelt\nDemocratic\n27752648\nwin\n60.978107\n\n\n120\n1972\nRichard Nixon\nRepublican\n47168710\nwin\n60.907806\n\n\n79\n1920\nWarren Harding\nRepublican\n16144093\nwin\n60.574501\n\n\n133\n1984\nRonald Reagan\nRepublican\n54455472\nwin\n59.023326\n\n\n\n\n\n\n\nWe do not need to explicitly specify the column used for sorting when calling .value_counts() on a Series. We can still specify the ordering paradigm – that is, whether values are sorted in ascending or descending order.\n\n# Sort the \"Name\" Series alphabetically\nelections[\"Candidate\"].sort_values(ascending=True).head()\n\n75     Aaron S. Watkins\n27      Abraham Lincoln\n23      Abraham Lincoln\n108     Adlai Stevenson\n105     Adlai Stevenson\nName: Candidate, dtype: object",
    "crumbs": [
      "Home",
      "Index and Columns"
    ]
  },
  {
    "objectID": "pandas/linalg.html",
    "href": "pandas/linalg.html",
    "title": "Scalars, Vectors and Matrices",
    "section": "",
    "text": "Most everyday mathematics consists of manipulating numbers one at a time. Formally, we call these values scalars.\nFor example, the temperature in Greenville is a balmy \\(72\\) degrees Fahrenheit. If you wanted to convert the temperature to Celsius you would evaluate the expression \\(c = \\frac{5}{9}(f - 32)\\), setting \\(f = 72\\). In this equation, the values \\(5\\), \\(9\\), and \\(32\\) are constant scalars. The variables \\(c\\) and \\(f\\) in general represent unknown scalars.\nWe denote scalars by ordinary lower-cased letters (e.g. \\(x\\), \\(y\\), and \\(z\\)) and the space of all (continuous) real-valued scalars by \\(\\mathbb{R}\\). The expression \\(x \\in \\mathbb{R}\\) is a formal way to say that \\(x\\) is a real-valued scalar. The symbol \\(\\in\\) (pronounced “in”) denotes membership in a set. For example, \\(x, y \\in {0, 1}\\) indicates that \\(x\\) and \\(y\\) are variables that can only take on values of \\(0\\) or \\(1\\).\nScalars in Python are represented by numeric types such as int and float.\nx = 3\ny = 2\n\nprint(\"x+y:\", x+y, \"x-y:\", x-y, \"x*y:\", x*y, \"x/y:\", x/y, \"x**y:\", x**y)",
    "crumbs": [
      "Home",
      "Scalars, Vectors and Matrices"
    ]
  },
  {
    "objectID": "pandas/linalg.html#scalars",
    "href": "pandas/linalg.html#scalars",
    "title": "Scalars, Vectors and Matrices",
    "section": "",
    "text": "Most everyday mathematics consists of manipulating numbers one at a time. Formally, we call these values scalars.\nFor example, the temperature in Greenville is a balmy \\(72\\) degrees Fahrenheit. If you wanted to convert the temperature to Celsius you would evaluate the expression \\(c = \\frac{5}{9}(f - 32)\\), setting \\(f = 72\\). In this equation, the values \\(5\\), \\(9\\), and \\(32\\) are constant scalars. The variables \\(c\\) and \\(f\\) in general represent unknown scalars.\nWe denote scalars by ordinary lower-cased letters (e.g. \\(x\\), \\(y\\), and \\(z\\)) and the space of all (continuous) real-valued scalars by \\(\\mathbb{R}\\). The expression \\(x \\in \\mathbb{R}\\) is a formal way to say that \\(x\\) is a real-valued scalar. The symbol \\(\\in\\) (pronounced “in”) denotes membership in a set. For example, \\(x, y \\in {0, 1}\\) indicates that \\(x\\) and \\(y\\) are variables that can only take on values of \\(0\\) or \\(1\\).\nScalars in Python are represented by numeric types such as int and float.\nx = 3\ny = 2\n\nprint(\"x+y:\", x+y, \"x-y:\", x-y, \"x*y:\", x*y, \"x/y:\", x/y, \"x**y:\", x**y)",
    "crumbs": [
      "Home",
      "Scalars, Vectors and Matrices"
    ]
  },
  {
    "objectID": "pandas/linalg.html#dimensions-and-shapes",
    "href": "pandas/linalg.html#dimensions-and-shapes",
    "title": "Scalars, Vectors and Matrices",
    "section": "Dimensions and Shapes",
    "text": "Dimensions and Shapes\nDimensionality, in the context of data, refers to the number of axes or directions in which data can be represented. The most common dimensions are 0, 1, 2, and n.\nScalars (0-dimensional data; values) are single numbers. They can be integers, real numbers, or complex numbers. Scalars are the simplest objects in linear algebra. In Python, we can represent scalars using the built-in int and float data types. For example, 3 and 3.0 are both scalars.\nVectors (1-dimensional data, collection of values) are one-dimensional arrays of scalars. They are used to represent quantities that have both magnitude and direction. In native Python, we can represent vectors using lists or tuples. For example, [1, 2, 3] is a vector.\n\n\n\n\n\nMatrices (2-dimensional data, collection of vectors) are two-dimensional arrays of scalars. They are used to represent linear transformations from one vector space to another. In native Python, we can represent matrices using lists of lists. For example, [[1, 2], [3, 4]] is a matrix.\nTensors (n-dimensional data, collection of matrices) are n-dimensional arrays of scalars. They are used to represent multi-dimensional data.",
    "crumbs": [
      "Home",
      "Scalars, Vectors and Matrices"
    ]
  },
  {
    "objectID": "pandas/linalg.html#tabular-2-dimensional-data",
    "href": "pandas/linalg.html#tabular-2-dimensional-data",
    "title": "Scalars, Vectors and Matrices",
    "section": "Tabular (2-dimensional) Data",
    "text": "Tabular (2-dimensional) Data\nTables are one of the most common ways to organize data. This is in large part due to the simplicity and flexibility of tables. Tables allow us to represent each observation, or instance of collecting data from an individual, as its own row. We can record distinct characteristics, or features, of each observation in separate columns.\n\n\n\n\nTo see this in action, we’ll explore the elections dataset, which stores information about political candidates who ran for president of the United States in various years.\nThe first few rows of elections dataset in CSV format are as follows:\nYear,Candidate,Party,Popular vote,Result,%\\n\n1824,Andrew Jackson,Democratic-Republican,151271,loss,57.21012204\\n\n1824,John Quincy Adams,Democratic-Republican,113142,win,42.78987796\\n\n1828,Andrew Jackson,Democratic,642806,win,56.20392707\\n\n1828,John Quincy Adams,National Republican,500897,loss,43.79607293\\n\n1832,Andrew Jackson,Democratic,702735,win,54.57478905\\n\nThis dataset is stored in Comma Separated Values (CSV) format. CSV files due to their simplicity and readability are one of the most common ways to store tabular data. Each line in a CSV file (file extension: .csv) represents a row in the table. In other words, each row is separated by a newline character \\n. Within each row, each column is separated by a comma ,, hence the name Comma Separated Values.",
    "crumbs": [
      "Home",
      "Scalars, Vectors and Matrices"
    ]
  },
  {
    "objectID": "pandas/linalg.html#dataframe-series-and-index",
    "href": "pandas/linalg.html#dataframe-series-and-index",
    "title": "Scalars, Vectors and Matrices",
    "section": "DataFrame, Series and Index",
    "text": "DataFrame, Series and Index\nThere are three fundamental data structures in pandas:\n\nSeries: 1D labeled array data; best thought of as columnar data\nDataFrame: 2D tabular data with rows and columns\nIndex: A sequence of row/column labels\n\nDataFrames, Series, and Indices can be represented visually in the following diagram, which considers the first few rows of the elections dataset.\n\n\n\n\nNotice how the DataFrame is a two-dimensional object – it contains both rows and columns. The Series above is a singular column of this DataFrame, namely, the Result column. Both contain an Index, or a shared list of row labels (here, the integers from 0 to 4, inclusive).",
    "crumbs": [
      "Home",
      "Scalars, Vectors and Matrices"
    ]
  },
  {
    "objectID": "pandas/linalg.html#shape-attribute",
    "href": "pandas/linalg.html#shape-attribute",
    "title": "Scalars, Vectors and Matrices",
    "section": ".shape attribute",
    "text": ".shape attribute\n.shape is an attribute of a DataFrame that returns a tuple representing the dimensions of the DataFrame.\n\nelections.shape\n\n(182, 6)\n\n\nThe first element of the tuple is the number of rows, and the second element is the number of columns.",
    "crumbs": [
      "Home",
      "Scalars, Vectors and Matrices"
    ]
  },
  {
    "objectID": "pandas/pivot.html",
    "href": "pandas/pivot.html",
    "title": "Pivot table",
    "section": "",
    "text": "We know now that .groupby gives us the ability to group and aggregate data across our DataFrame. The examples above formed groups using just one column in the DataFrame. It’s possible to group by multiple columns at once by passing in a list of column names to .groupby.\nLet’s consider the names dataset. In this problem, we will find the total number of baby names associated with each sex for each year. To do this, we’ll group by both the \"Year\" and \"Sex\" columns.\n\nnames.head()\n\n\n\n\n\n\n\n\nName\nSex\nCount\nYear\nFirst Letter\n\n\n\n\n0\nMary\nF\n7065\n1880\nM\n\n\n1\nAnna\nF\n2604\n1880\nA\n\n\n2\nEmma\nF\n2003\n1880\nE\n\n\n3\nElizabeth\nF\n1939\n1880\nE\n\n\n4\nMinnie\nF\n1746\n1880\nM\n\n\n\n\n\n\n\n\n# Find the total number of baby names associated with each sex for each year in the data\nnames.groupby([\"Year\", \"Sex\"])[[\"Count\"]].sum().head(6)\n\n\n\n\n\n\n\n\n\nCount\n\n\nYear\nSex\n\n\n\n\n\n1880\nF\n90994\n\n\nM\n110490\n\n\n1881\nF\n91953\n\n\nM\n100737\n\n\n1882\nF\n107847\n\n\nM\n113686\n\n\n\n\n\n\n\nNotice that both \"Year\" and \"Sex\" serve as the index of the DataFrame (they are both rendered in bold). We’ve created a multi-index DataFrame where two different index values, the year and sex, are used to uniquely identify each row.\nThis isn’t the most intuitive way of representing this data – and, because multi-indexed DataFrames have multiple dimensions in their index, they can often be difficult to use.\nAnother strategy to aggregate across two columns is to create a pivot table. One set of values is used to create the index of the pivot table; another set is used to define the column names. The values contained in each cell of the table correspond to the aggregated data for each index-column pair.\nThe best way to understand pivot tables is to see one in action. Let’s return to our original goal of summing the total number of names associated with each combination of year and sex. We’ll call the pandas .pivot_table method to create a new table.\n\n# The `pivot_table` method is used to generate a Pandas pivot table\nnames.pivot_table(\n    index = \"Year\", \n    columns = \"Sex\", \n    values = \"Count\", \n    aggfunc = sum).head(5)\n\n\n\n\n\n\n\nSex\nF\nM\n\n\nYear\n\n\n\n\n\n\n1880\n90994\n110490\n\n\n1881\n91953\n100737\n\n\n1882\n107847\n113686\n\n\n1883\n112319\n104625\n\n\n1884\n129019\n114442\n\n\n\n\n\n\n\nLooks a lot better! Now, our DataFrame is structured with clear index-column combinations. Each entry in the pivot table represents the summed count of names for a given combination of \"Year\" and \"Sex\".\nLet’s take a closer look at the code implemented above.\n\nindex = \"Year\" specifies the column name in the original DataFrame that should be used as the index of the pivot table\ncolumns = \"Sex\" specifies the column name in the original DataFrame that should be used to generate the columns of the pivot table\nvalues = \"Count\" indicates what values from the original DataFrame should be used to populate the entry for each index-column combination\naggfunc = sum tells pandas what function to use when aggregating the data specified by values. Here, we are summing the name counts for each pair of \"Year\" and \"Sex\"\n\n\n\n\n\nWe can even include multiple values in the index or columns of our pivot tables.\n\nnames_pivot = names.pivot_table(\n    index=\"Year\",     # the rows (turned into index)\n    columns=\"Sex\",    # the column values\n    values=[\"Count\", \"Name\"], \n    aggfunc=max,   # group operation\n)\nnames_pivot.head(6)\n\n\n\n\n\n\n\n\nCount\nName\n\n\nSex\nF\nM\nF\nM\n\n\nYear\n\n\n\n\n\n\n\n\n1880\n7065\n9655\nZula\nZeke\n\n\n1881\n6919\n8769\nZula\nZeb\n\n\n1882\n8148\n9557\nZula\nZed\n\n\n1883\n8012\n8894\nZula\nZeno\n\n\n1884\n9217\n9388\nZula\nZollie\n\n\n1885\n9128\n8756\nZula\nZollie",
    "crumbs": [
      "Home",
      "Pivot table"
    ]
  }
]