[
  {
    "objectID": "pandas/pandas2.html",
    "href": "pandas/pandas2.html",
    "title": "Data Manipulation and Wrangling",
    "section": "",
    "text": "Let’s read in the same elections data from the previous exercise and do some data manipulation and wrangling using pandas.\nimport pandas as pd \nurl = \"https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv\"\nelections = pd.read_csv(url)",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Data Manipulation and Wrangling"
    ]
  },
  {
    "objectID": "pandas/pandas2.html#data-alignment",
    "href": "pandas/pandas2.html#data-alignment",
    "title": "Data Manipulation and Wrangling",
    "section": "Data Alignment",
    "text": "Data Alignment\npandas can make it much simpler to work with objects that have different indexes. For example, when you add objects, if any index pairs are not the same, the respective index in the result will be the union of the index pairs. Let’s look at an example:\n\ns1 = pd.Series([7.3, -2.5, 3.4, 1.5], index=[\"a\", \"c\", \"d\", \"e\"])\n\ns2 = pd.Series([-2.1, 3.6, -1.5, 4, 3.1], index=[\"a\", \"c\", \"e\", \"f\", \"g\"])\n\ns1, s2, s1 + s2\n\n(a    7.3\n c   -2.5\n d    3.4\n e    1.5\n dtype: float64,\n a   -2.1\n c    3.6\n e   -1.5\n f    4.0\n g    3.1\n dtype: float64,\n a    5.2\n c    1.1\n d    NaN\n e    0.0\n f    NaN\n g    NaN\n dtype: float64)\n\n\nThe internal data alignment introduces missing values in the label locations that don’t overlap. Missing values will then propagate in further arithmetic computations.\nIn the case of DataFrame, alignment is performed on both rows and columns:\n\ndf1 = pd.DataFrame({\"A\": [1, 2], \"B\":[3, 4]})\ndf2 = pd.DataFrame({\"B\": [5, 6], \"D\":[7, 8]})\ndf1 + df2",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Data Manipulation and Wrangling"
    ]
  },
  {
    "objectID": "pandas/pandas2.html#math-operations",
    "href": "pandas/pandas2.html#math-operations",
    "title": "Data Manipulation and Wrangling",
    "section": "Math Operations",
    "text": "Math Operations\nIn native Python, we have a number of operators that we can use to manipulate data. Most, if not all, of these operators can be used with Pandas Series and DataFrames and are applied element-wise in parallel. A summary of the operators supported by Pandas is shown below:\n\n\n\n\n\n\n\n\n\nCategory\nOperators\nSupported by Pandas\nComments\n\n\n\n\nArithmetic\n+, -, *, /, %, //, **\n✅\nAssuming comparable shapes (equal length)\n\n\nAssignment\n=, +=, -=, *=, /=, %=, //=, **=\n✅\nAssuming comparable shapes\n\n\nComparison\n==, !=, &gt;, &lt;, &gt;=, &lt;=\n✅\nAssuming comparable shapes\n\n\nLogical\nand, or, not\n❌\nUse &, \\|, ~ instead\n\n\nIdentity\nis, is not\n✅\nAssuming comparable data type/structure\n\n\nMembership\nin, not in\n❌\nUse isin() method instead\n\n\nBitwise\n&, \\|, ^, ~, &lt;&lt;, &gt;&gt;\n❌\n\n\n\n\nThe most significant difference is that logical operators and, or, and not are NOT used with Pandas Series and DataFrames. Instead, we use &, |, and ~ respectively.\nMembership operators in and not in are also not used with Pandas Series and DataFrames. Instead, we use the isin() method.",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Data Manipulation and Wrangling"
    ]
  },
  {
    "objectID": "pandas/pandas2.html#creating-new-columns",
    "href": "pandas/pandas2.html#creating-new-columns",
    "title": "Data Manipulation and Wrangling",
    "section": "Creating new columns",
    "text": "Creating new columns\nCreating new columns in a DataFrame is a common task when working with data. In this notebook, we will see how to create new columns in a DataFrame based on existing columns or other values.\n\n\n\nNew columns can be created by assigning a value to a new column name. For example, to create a new column named new_column with a constant value 10, we can use the following code:\n\nelections['constant'] = 10\n\nelections.head()\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\nconstant\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n10\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n10\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n10\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n10\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n10\n\n\n\n\n\n\n\n\nIf we want to create a new column based on an existing column, we can refer to the existing column by its name, within the square brackets, on the right side of the assignment operator. For example, to create a new column named new_column with the values of the existing column column1, we can use the following code:\n\n\nelections['total_voters'] = ((elections['Popular vote']* 100) / elections['%']).astype(int)\n\nelections.head()\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\nconstant\ntotal_voters\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n10\n264413\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n10\n264412\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n10\n1143702\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n10\n1143703\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n10\n1287655\n\n\n\n\n\n\n\nNote that the new column will have the same length as the DataFrame and the calculations are element-wise. That is, the value of the new column at row i will be calculated based on the value of the existing column at row i.\n\n\n\nThese element-wise operations are vectorized and are very efficient.",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Data Manipulation and Wrangling"
    ]
  },
  {
    "objectID": "pandas/pandas2.html#applyf-axis01",
    "href": "pandas/pandas2.html#applyf-axis01",
    "title": "Data Manipulation and Wrangling",
    "section": ".apply(f, axis=0/1)",
    "text": ".apply(f, axis=0/1)\nA frequent operation in pandas is applying a function on to either each column or row of a DataFrame.\nDataFrame’s apply method does exactly this.\n\n\n\nLet’s say we wanted to count the number of unique values that each column takes on. We can use .apply to answer that question:\n\ndef count_unique(col):\n    return len(set(col))\n\nelections.apply(count_unique, axis=\"index\") # function is passed an individual column\n\nYear             50\nCandidate       132\nParty            36\nPopular vote    182\nResult            2\n%               182\ndtype: int64\n\n\n\nColumn-wise: axis=0 (default)\ndata.apply(f, axis=0) applies the function f to each column of the DataFrame data.\n\n\n\nFor example, if we wanted to find the number of unique values in each column of a DataFrame data, we could use the following code:\n\ndef count_unique(column):\n    return len(column.unique())\n\nelections.apply(count_unique, axis=0)\n\nYear             50\nCandidate       132\nParty            36\nPopular vote    182\nResult            2\n%               182\ndtype: int64\n\n\n\n\nRow-wise: axis=1\ndata.apply(f, axis=1) applies the function f to each row of the DataFrame data.\n\n\n\nFor instance, let’s say we wanted to count the total number of voters in an election.\nWe can use .apply to answer that question using the following formula:\n\\[ \\text{total} \\times \\frac{\\%}{100} = \\text{Popular vote} \\]\n\ndef compute_total(row):\n    return int(row['Popular vote']*100/row['%'])\n\nelections.apply(compute_total, axis=1)\n\n0         264413\n1         264412\n2        1143702\n3        1143703\n4        1287655\n         ...    \n177    135720167\n178    158383403\n179    158383403\n180    158383401\n181    158383402\nLength: 182, dtype: int64",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Data Manipulation and Wrangling"
    ]
  },
  {
    "objectID": "pandas/pandas2.html#summary-statistics",
    "href": "pandas/pandas2.html#summary-statistics",
    "title": "Data Manipulation and Wrangling",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nIn data science, we often want to compute summary statistics. pandas provides a number of built-in methods for this purpose.\n\n\n\nFor example, we can use the .mean(), .median() and .std() methods to compute the mean, median, and standard deviation of a column, respectively.\n\nelections['%'].mean(), elections['%'].median(), elections['%'].std()\n\n(27.470350372043967, 37.67789306, 22.96803399144419)\n\n\nSimilarly, we can use the .max() and .min() methods to compute the maximum and minimum values of a Series or DataFrame.\n\nelections['%'].max(), elections['%'].min()\n\n(61.34470329, 0.098088334)\n\n\nThe .sum() method computes the sum of all the values in a Series or DataFrame.\n\n\n\nThe .describe() method computes summary statistics for a Series or DataFrame. It computes the mean, standard deviation, minimum, maximum, and the quantiles of the data.\n\nelections['%'].describe()\n\ncount    182.000000\nmean      27.470350\nstd       22.968034\nmin        0.098088\n25%        1.219996\n50%       37.677893\n75%       48.354977\nmax       61.344703\nName: %, dtype: float64\n\n\n\nelections.describe()\n\n\n\n\n\n\n\n\nYear\nPopular vote\n%\n\n\n\n\ncount\n182.000000\n1.820000e+02\n182.000000\n\n\nmean\n1934.087912\n1.235364e+07\n27.470350\n\n\nstd\n57.048908\n1.907715e+07\n22.968034\n\n\nmin\n1824.000000\n1.007150e+05\n0.098088\n\n\n25%\n1889.000000\n3.876395e+05\n1.219996\n\n\n50%\n1936.000000\n1.709375e+06\n37.677893\n\n\n75%\n1988.000000\n1.897775e+07\n48.354977\n\n\nmax\n2020.000000\n8.126892e+07\n61.344703",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Data Manipulation and Wrangling"
    ]
  },
  {
    "objectID": "pandas/pandas2.html#other-handy-utility-functions",
    "href": "pandas/pandas2.html#other-handy-utility-functions",
    "title": "Data Manipulation and Wrangling",
    "section": "Other Handy Utility Functions",
    "text": "Other Handy Utility Functions\npandas contains an extensive library of functions that can help shorten the process of setting and getting information from its data structures. In the following section, we will give overviews of each of the main utility functions that will help us in in this course.\nDiscussing all functionality offered by pandas could take an entire semester! We will walk you through the most commonly-used functions, and encourage you to explore and experiment on your own.\n\n.shape\n.size\n.dtypes\n.astype()\n.describe()\n.sample()\n.value_counts()\n.unique()\n.sort_values()\n\nThe pandas documentation will be a valuable resource.\n\n.astype()\nCast a pandas object to a specified dtype\n\nelections['%'].astype(int)\n\n0      57\n1      42\n2      56\n3      43\n4      54\n       ..\n177     1\n178    51\n179    46\n180     1\n181     0\nName: %, Length: 182, dtype: int64\n\n\n\n\n.describe()\nIf many statistics are required from a DataFrame (minimum value, maximum value, mean value, etc.), then .describe() can be used to compute all of them at once.\n\nelections.describe()\n\n\n\n\n\n\n\n\nYear\nPopular vote\n%\n\n\n\n\ncount\n182.000000\n1.820000e+02\n182.000000\n\n\nmean\n1934.087912\n1.235364e+07\n27.470350\n\n\nstd\n57.048908\n1.907715e+07\n22.968034\n\n\nmin\n1824.000000\n1.007150e+05\n0.098088\n\n\n25%\n1889.000000\n3.876395e+05\n1.219996\n\n\n50%\n1936.000000\n1.709375e+06\n37.677893\n\n\n75%\n1988.000000\n1.897775e+07\n48.354977\n\n\nmax\n2020.000000\n8.126892e+07\n61.344703\n\n\n\n\n\n\n\nA different set of statistics will be reported if .describe() is called on a Series.\n\nelections[\"Party\"].describe()\n\ncount            182\nunique            36\ntop       Democratic\nfreq              47\nName: Party, dtype: object\n\n\n\nelections[\"Popular vote\"].describe().astype(int)\n\ncount         182\nmean     12353635\nstd      19077149\nmin        100715\n25%        387639\n50%       1709375\n75%      18977751\nmax      81268924\nName: Popular vote, dtype: int64\n\n\n\n\n.sample()\nAs we will see later in the semester, random processes are at the heart of many data science techniques (for example, train-test splits, bootstrapping, and cross-validation). .sample() lets us quickly select random entries (a row if called from a DataFrame, or a value if called from a Series).\nBy default, .sample() selects entries without replacement. Pass in the argument replace=True to sample with replacement.\n\n# Sample a single row\nelections.sample()\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n135\n1988\nGeorge H. W. Bush\nRepublican\n48886597\nwin\n53.518845\n\n\n\n\n\n\n\n\n# Sample 5 random rows\nelections.sample(5)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n155\n2000\nRalph Nader\nGreen\n2882955\nloss\n2.741176\n\n\n134\n1984\nWalter Mondale\nDemocratic\n37577352\nloss\n40.729429\n\n\n39\n1884\nGrover Cleveland\nDemocratic\n4914482\nwin\n48.884933\n\n\n84\n1928\nHerbert Hoover\nRepublican\n21427123\nwin\n58.368524\n\n\n177\n2016\nJill Stein\nGreen\n1457226\nloss\n1.073699\n\n\n\n\n\n\n\n\n# Randomly sample 4 names from the year 2000, with replacement\nelections[elections[\"Result\"] == \"win\"].sample(4, replace = True)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n53\n1896\nWilliam McKinley\nRepublican\n7112138\nwin\n51.213817\n\n\n131\n1980\nRonald Reagan\nRepublican\n43903230\nwin\n50.897944\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n168\n2012\nBarack Obama\nDemocratic\n65915795\nwin\n51.258484\n\n\n\n\n\n\n\n\n\n.value_counts()\nThe Series.value_counts() methods counts the number of occurrence of each unique value in a Series. In other words, it counts the number of times each unique value appears. This is often useful for determining the most or least common entries in a Series.\nIn the example below, we can determine the name with the most years in which at least one person has taken that name by counting the number of times each name appears in the \"Name\" column of elections.\n\nelections[\"Party\"].value_counts().head()\n\nDemocratic     47\nRepublican     41\nLibertarian    12\nProhibition    11\nSocialist      10\nName: Party, dtype: int64\n\n\n\n\n.unique()\nIf we have a Series with many repeated values, then .unique() can be used to identify only the unique values. Here we return an array of all the names in elections.\n\nelections[\"Result\"].unique()\n\narray(['loss', 'win'], dtype=object)\n\n\n\n\n.drop_duplicates()\nIf we have a DataFrame with many repeated rows, then .drop_duplicates() can be used to remove the repeated rows.\nWhere .unique() only works with individual columns (Series) and returns an array of unique values, .drop_duplicates() can be used with multiple columns (DataFrame) and returns a DataFrame with the repeated rows removed.\n\nelections[['Candidate', 'Party']].drop_duplicates()\n\n\n\n\n\n\n\n\nCandidate\nParty\n\n\n\n\n0\nAndrew Jackson\nDemocratic-Republican\n\n\n1\nJohn Quincy Adams\nDemocratic-Republican\n\n\n2\nAndrew Jackson\nDemocratic\n\n\n3\nJohn Quincy Adams\nNational Republican\n\n\n5\nHenry Clay\nNational Republican\n\n\n...\n...\n...\n\n\n174\nEvan McMullin\nIndependent\n\n\n176\nHillary Clinton\nDemocratic\n\n\n178\nJoseph Biden\nDemocratic\n\n\n180\nJo Jorgensen\nLibertarian\n\n\n181\nHoward Hawkins\nGreen\n\n\n\n\n141 rows × 2 columns\n\n\n\n\n\n.sort_values()\nOrdering a DataFrame can be useful for isolating extreme values. For example, the first 5 entries of a row sorted in descending order (that is, from highest to lowest) are the largest 5 values. .sort_values allows us to order a DataFrame or Series by a specified column. We can choose to either receive the rows in ascending order (default) or descending order.\n\n# Sort the \"Count\" column from highest to lowest\nelections.sort_values(by = \"%\", ascending=False).head()\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n114\n1964\nLyndon Johnson\nDemocratic\n43127041\nwin\n61.344703\n\n\n91\n1936\nFranklin Roosevelt\nDemocratic\n27752648\nwin\n60.978107\n\n\n120\n1972\nRichard Nixon\nRepublican\n47168710\nwin\n60.907806\n\n\n79\n1920\nWarren Harding\nRepublican\n16144093\nwin\n60.574501\n\n\n133\n1984\nRonald Reagan\nRepublican\n54455472\nwin\n59.023326\n\n\n\n\n\n\n\nWe do not need to explicitly specify the column used for sorting when calling .value_counts() on a Series. We can still specify the ordering paradigm – that is, whether values are sorted in ascending or descending order.\n\n# Sort the \"Name\" Series alphabetically\nelections[\"Candidate\"].sort_values(ascending=True).head()\n\n75     Aaron S. Watkins\n27      Abraham Lincoln\n23      Abraham Lincoln\n108     Adlai Stevenson\n105     Adlai Stevenson\nName: Candidate, dtype: object\n\n\n\n\n.set_index()\nThe .set_index() method is used to set the DataFrame index using existing columns.\n\nelections.set_index(\"Candidate\")\n\n\n\n\n\n\n\n\nYear\nParty\nPopular vote\nResult\n%\n\n\nCandidate\n\n\n\n\n\n\n\n\n\nAndrew Jackson\n1824\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\nJohn Quincy Adams\n1824\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\nAndrew Jackson\n1828\nDemocratic\n642806\nwin\n56.203927\n\n\nJohn Quincy Adams\n1828\nNational Republican\n500897\nloss\n43.796073\n\n\nAndrew Jackson\n1832\nDemocratic\n702735\nwin\n54.574789\n\n\n...\n...\n...\n...\n...\n...\n\n\nJill Stein\n2016\nGreen\n1457226\nloss\n1.073699\n\n\nJoseph Biden\n2020\nDemocratic\n81268924\nwin\n51.311515\n\n\nDonald Trump\n2020\nRepublican\n74216154\nloss\n46.858542\n\n\nJo Jorgensen\n2020\nLibertarian\n1865724\nloss\n1.177979\n\n\nHoward Hawkins\n2020\nGreen\n405035\nloss\n0.255731\n\n\n\n\n182 rows × 5 columns\n\n\n\n\n\n.reset_index()\nThe .reset_index() method is used to reset the index of a DataFrame. By default, the original index is stored in a new column called index.\n\nelections.reset_index()\n\n\n\n\n\n\n\n\nindex\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n177\n177\n2016\nJill Stein\nGreen\n1457226\nloss\n1.073699\n\n\n178\n178\n2020\nJoseph Biden\nDemocratic\n81268924\nwin\n51.311515\n\n\n179\n179\n2020\nDonald Trump\nRepublican\n74216154\nloss\n46.858542\n\n\n180\n180\n2020\nJo Jorgensen\nLibertarian\n1865724\nloss\n1.177979\n\n\n181\n181\n2020\nHoward Hawkins\nGreen\n405035\nloss\n0.255731\n\n\n\n\n182 rows × 7 columns\n\n\n\n\n\n.rename()\nThe .rename() method is used to rename the columns or index labels of a DataFrame.\n\nelections.rename(columns={\"Candidate\":\"Name\"})\n\n\n\n\n\n\n\n\nYear\nName\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n177\n2016\nJill Stein\nGreen\n1457226\nloss\n1.073699\n\n\n178\n2020\nJoseph Biden\nDemocratic\n81268924\nwin\n51.311515\n\n\n179\n2020\nDonald Trump\nRepublican\n74216154\nloss\n46.858542\n\n\n180\n2020\nJo Jorgensen\nLibertarian\n1865724\nloss\n1.177979\n\n\n181\n2020\nHoward Hawkins\nGreen\n405035\nloss\n0.255731\n\n\n\n\n182 rows × 6 columns",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Data Manipulation and Wrangling"
    ]
  },
  {
    "objectID": "pandas/pandas0.html",
    "href": "pandas/pandas0.html",
    "title": "VECTORIZED OPERATIONS",
    "section": "",
    "text": "VECTORIZED OPERATIONS\nVector based programming (also known as array based programming) is a programming paradigm that uses operations on arrays to execute tasks. This is in contrast to scalar based programming where operations are performed on individual elements of an array.\n\n \n\nVectorization operates at the level of individual instructions sent to a processor within each node. For instance, in the illustration shown here, the instruction is to add 5 to a column of numbers and copy the results to a new column B. With vectorization, all the data elements in that column are transformed simultaneously, i.e. the instruction to add 5 is applied to multiple pieces of data at the same time. This paradigm is sometimes referred to as Single Instruction Multiple Data (or SIMD).\nWe can think of vectorization as subdividing the work into smaller chunks that can be handled independently by different computational units at the same time.\nThis is orders of magnitude faster than the conventional sequential model where each piece of data is handled one after the other in sequence.\nWith vectorization, performing the same operation on a modern intel CPU is 16 times faster than the sequential mode. The performance gains on GPUs with thousands of computational cores is even greater. However, despite these remarkable performance benefits, most analytical code out there is written in the slower sequential mode. This is not a surprise, since until about a decade ago, CPU and GPU hardware could not really support vectorization for data analysis. So most implementations had to be sequential.\nThe last 10 years, however, have seen the rise of new technologies like CUDA from NVidia and advanced vector extensions from Intel that have dramatically shifted our ability to apply vectorization. Because of the power of vectorization, some traditional vendors now make claims about including vectorization in their offerings. But shifting to this new vectorized paradigm is not easy, since all of your code needs to be written from scratch to utilize these capabilities. Unlike Kinetica, these traditional databases incorporate only partial vectorization, limited to specific operations such as filters or some aggregations and continue to use outdated and less performant modes of computation for other analytical tasks. These solutions also only leverage vectorization on the CPUs.",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS"
    ]
  },
  {
    "objectID": "pandas/pandas1.html",
    "href": "pandas/pandas1.html",
    "title": "Preliminaries",
    "section": "",
    "text": "Pandas is a powerful Python library that is widely used in data science and data analysis. It provides data structures and functions that make working with tabular data easy and intuitive.\nIt is generally accepted in the data science community as the industry- and academia-standard tool for manipulating tabular data.",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Preliminaries"
    ]
  },
  {
    "objectID": "pandas/pandas1.html#dimensionality-of-data",
    "href": "pandas/pandas1.html#dimensionality-of-data",
    "title": "Preliminaries",
    "section": "Dimensionality of Data",
    "text": "Dimensionality of Data\nDimensionality, in the context of data, refers to the number of axes or directions in which data can be represented. The most common dimensions are 0, 1, 2, and n.\nScalars (0-dimensional data; values) are single numbers. They can be integers, real numbers, or complex numbers. Scalars are the simplest objects in linear algebra. In Python, we can represent scalars using the built-in int and float data types. For example, 3 and 3.0 are both scalars.\nVectors (1-dimensional data, collection of values) are one-dimensional arrays of scalars. They are used to represent quantities that have both magnitude and direction. In native Python, we can represent vectors using lists or tuples. For example, [1, 2, 3] is a vector.\n\n\n\n\n\nMatrices (2-dimensional data, collection of vectors) are two-dimensional arrays of scalars. They are used to represent linear transformations from one vector space to another. In native Python, we can represent matrices using lists of lists. For example, [[1, 2], [3, 4]] is a matrix.\nTensors (n-dimensional data, collection of matrices) are n-dimensional arrays of scalars. They are used to represent multi-dimensional data.",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Preliminaries"
    ]
  },
  {
    "objectID": "pandas/pandas1.html#tabular-2-dimensional-data",
    "href": "pandas/pandas1.html#tabular-2-dimensional-data",
    "title": "Preliminaries",
    "section": "Tabular (2-dimensional) Data",
    "text": "Tabular (2-dimensional) Data\nTables are one of the most common ways to organize data. This is in large part due to the simplicity and flexibility of tables. Tables allow us to represent each observation, or instance of collecting data from an individual, as its own row. We can record distinct characteristics, or features, of each observation in separate columns.\n\n\n\n\nTo see this in action, we’ll explore the elections dataset, which stores information about political candidates who ran for president of the United States in various years.\nThe first few rows of elections dataset in CSV format are as follows:\noujxpu csv Year,Candidate,Party,Popular vote,Result,%\\n 1824,Andrew Jackson,Democratic-Republican,151271,loss,57.21012204\\n 1824,John Quincy Adams,Democratic-Republican,113142,win,42.78987796\\n 1828,Andrew Jackson,Democratic,642806,win,56.20392707\\n 1828,John Quincy Adams,National Republican,500897,loss,43.79607293\\n 1832,Andrew Jackson,Democratic,702735,win,54.57478905\\n\nThis dataset is stored in Comma Separated Values (CSV) format. CSV files due to their simplicity and readability are one of the most common ways to store tabular data. Each line in a CSV file (file extension: .csv) represents a row in the table. In other words, each row is separated by a newline character \\n. Within each row, each column is separated by a comma ,, hence the name Comma Separated Values.",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Preliminaries"
    ]
  },
  {
    "objectID": "pandas/pandas1.html#reading-data",
    "href": "pandas/pandas1.html#reading-data",
    "title": "Preliminaries",
    "section": "Reading Data",
    "text": "Reading Data\nTo begin our studies in pandas, we must first import the library into our Python environment using import pandas as pd statement. pd is a common alias for pandas. The import statement will allow us to use pandas data structures and methods in our code.\n\nCSV files can be in pandas using read_csv. The following code cell imports pandas as pd, the conventional alias for Pandas and then reads the elections.csv file.\n\n# `pd` is the conventional alias for Pandas\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv\"\nelections = pd.read_csv(url)\nelections\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n177\n2016\nJill Stein\nGreen\n1457226\nloss\n1.073699\n\n\n178\n2020\nJoseph Biden\nDemocratic\n81268924\nwin\n51.311515\n\n\n179\n2020\nDonald Trump\nRepublican\n74216154\nloss\n46.858542\n\n\n180\n2020\nJo Jorgensen\nLibertarian\n1865724\nloss\n1.177979\n\n\n181\n2020\nHoward Hawkins\nGreen\n405035\nloss\n0.255731\n\n\n\n\n182 rows × 6 columns\n\n\n\nLet’s dissect the code above.\n\nWe first import the pandas library into our Python environment, using the alias pd.  import pandas as pd\nThere are a number of ways to read data into a DataFrame. In this course, our datasets are typically stored in a CSV (comma-seperated values) file format. We can import a CSV file into a DataFrame by passing the data path as an argument to the following pandas function.  pd.read_csv(\"data/elections.csv\")\n\nThis code stores our DataFrame object in the elections variable. We see that our elections DataFrame has 182 rows and 6 columns (Year, Candidate, Party, Popular Vote, Result, %). Each row represents a single record – in our example, a presedential candidate from some particular year. Each column represents a single attribute, or feature of the record.\nIn the example above, we constructed a DataFrame object using data from a CSV file. As we’ll explore in the next section, we can also create a DataFrame with data of our own.\nIn the elections dataset, each row represents one instance of a candidate running for president in a particular year. For example, the first row represents Andrew Jackson running for president in the year 1824. Each column represents one characteristic piece of information about each presidential candidate. For example, the column named Result stores whether or not the candidate won the election.\nSome relevant arguments for read_csv are:\n\nfilepath_or_buffer: The path to the CSV file.\nsep: The character that separates the values in the CSV file. By default, this is a comma ,.\nheader: The row number to use as the column names. By default, this is 0, which means the first row is used as the column names.\nindex_col: The column to use as the row labels of the DataFrame. By default, this is None, which means that the row labels are integers starting from 0.\nerror_bad_lines: If True, the parser will skip lines with too many fields rather than raising an error. By default, this is False.",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Preliminaries"
    ]
  },
  {
    "objectID": "pandas/pandas1.html#head-method",
    "href": "pandas/pandas1.html#head-method",
    "title": "Preliminaries",
    "section": ".head() method",
    "text": ".head() method\n.head() is a method of a DataFrame that returns the first n rows of a DataFrame. By default, n is 5. This is useful when you want to quickly check the contents of a DataFrame.\n\nelections.head()\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n\n\n\n\n\nSimilarly, calling df.tail(n) allows us to extract the last n rows of the DataFrame.\n\nelections.tail(3)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n179\n2020\nDonald Trump\nRepublican\n74216154\nloss\n46.858542\n\n\n180\n2020\nJo Jorgensen\nLibertarian\n1865724\nloss\n1.177979\n\n\n181\n2020\nHoward Hawkins\nGreen\n405035\nloss\n0.255731",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Preliminaries"
    ]
  },
  {
    "objectID": "pandas/pandas1.html#shape-attribute",
    "href": "pandas/pandas1.html#shape-attribute",
    "title": "Preliminaries",
    "section": ".shape attribute",
    "text": ".shape attribute\n.shape is an attribute of a DataFrame that returns a tuple representing the dimensions of the DataFrame.\n\nelections.shape\n\n(182, 6)\n\n\nThe first element of the tuple is the number of rows, and the second element is the number of columns.",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Preliminaries"
    ]
  },
  {
    "objectID": "pandas/pandas1.html#dtypes-attribute",
    "href": "pandas/pandas1.html#dtypes-attribute",
    "title": "Preliminaries",
    "section": ".dtypes attribute",
    "text": ".dtypes attribute\n.dtypes is an attribute of a DataFrame that returns the data type of each column. The data types are returned as a Series with the column names as the index labels.\n\nelections.dtypes\n\nYear              int64\nCandidate        object\nParty            object\nPopular vote      int64\nResult           object\n%               float64\ndtype: object\n\n\nIn pandas, object is the data type used for string columns, while int64 and float64 are used for integer and floating-point columns, respectively.",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Preliminaries"
    ]
  },
  {
    "objectID": "pandas/pandas1.html#writing-data",
    "href": "pandas/pandas1.html#writing-data",
    "title": "Preliminaries",
    "section": "Writing Data",
    "text": "Writing Data\npandas can also write data to a variety of file formats, including CSV, Excel, and SQL databases. The following code cell writes the elections dataset to a CSV file named elections.csv.\n\npd.to_csv('elections_new.csv')",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Preliminaries"
    ]
  },
  {
    "objectID": "pandas/pandas1.html#dataframe-series-and-index",
    "href": "pandas/pandas1.html#dataframe-series-and-index",
    "title": "Preliminaries",
    "section": "DataFrame, Series and Index",
    "text": "DataFrame, Series and Index\nThere are three fundamental data structures in pandas:\n\nSeries: 1D labeled array data; best thought of as columnar data\nDataFrame: 2D tabular data with rows and columns\nIndex: A sequence of row/column labels\n\nDataFrames, Series, and Indices can be represented visually in the following diagram, which considers the first few rows of the elections dataset.\n\n\n\n\nNotice how the DataFrame is a two-dimensional object – it contains both rows and columns. The Series above is a singular column of this DataFrame, namely, the Result column. Both contain an Index, or a shared list of row labels (here, the integers from 0 to 4, inclusive).",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Preliminaries"
    ]
  },
  {
    "objectID": "calendar.html",
    "href": "calendar.html",
    "title": "Calendar",
    "section": "",
    "text": "Calendar\n\n\n\n\n\n\nCaution\n\n\n\nPlease note that this is a tentative plan and is subject to change.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\nTOPIC\nREADING\nMEETING 1\nMEETING 2\nWRITTEN ASSIGNMENT\nPROGRAMMING ASSIGNMENT\n\n\n\n\n1\nIntroduction\nSkiena Ch.1\n\n\n\n\n\n\n2\nPandas 1\nMcKinney Ch.5, 8, 10\n\n\n\n\n\n\n3\nPandas 2\nMcKinney Ch.5, 8, 10\n\n\n\n\n\n\n4\nPandas 3\nMcKinney Ch.5, 8, 10\n\n\n\n\n\n\n\n\nProject phase 1 due\nEXAM 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\nTOPIC\nREADING\nMEETING 1\nMEETING 2\nWRITTEN ASSIGNMENT\nPROGRAMMING ASSIGNMENT\n\n\n\n\n5\nEncoding and Representation\nMcKinney Ch.7\n\n\n\n\n\n\n6\nData Visualization\nMcKinney Ch.7\n\n\n\n\n\n\n7\nProbability + Stats\nSkiena Ch.2\n\n\n\n\n\n\n8\nNaive Bayes\nSLP 3rd edition Ch.4\n\n\n\n\n\n\n\n\nProject phase 2 due\nEXAM 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\nTOPIC\nREADING\nMEETING 1\nMEETING 2\nWRITTEN ASSIGNMENT\nPROGRAMMING ASSIGNMENT\n\n\n\n\n9\nNearest Neighbor\nSkiena Ch. 8 and Ch. 10\n\n\n\n\n\n\n10\nClustering\nSkiena Ch.10 (10.5)\n\n\n\n\n\n\n11\nLinear Models 1\nSkiena Ch.9 (9.1-9.5)\n\n\n\n\n\n\n12\nLinear Models 2\nSkiena Ch.9 (9.1-9.5)\n\n\n\n\n\n\n\n\nProject phase 3 due\nFINAL EXAM"
  },
  {
    "objectID": "calculus/linearregression.html",
    "href": "calculus/linearregression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Linear Regression"
  },
  {
    "objectID": "linearalgebra/linalg0.html",
    "href": "linearalgebra/linalg0.html",
    "title": "LINEAR ALGEBRA",
    "section": "",
    "text": "LINEAR ALGEBRA"
  },
  {
    "objectID": "project/project.html",
    "href": "project/project.html",
    "title": "Course Project",
    "section": "",
    "text": "In this course, you are expected to plan, execute, and present the results of a semester-long group project of your choosing.\nTake the time to investigate multiple options during the proposal phase. You should have gotten some preliminary results by then, enough to provide confidence you will be able to successfully complete the project."
  },
  {
    "objectID": "project/project.html#groups",
    "href": "project/project.html#groups",
    "title": "Course Project",
    "section": "Groups",
    "text": "Groups\nWorking alone on the project is strongly discouraged. Group sizes should range from two to three students. It is possible for multiple groups working on the same data set. However, such groups must work independently and are not allowed to share code or results. Each dataset leaves enough room to pursue different directions so I expect to see variety among the submissions from each group.\n\nTo ensure consistent progress, there is a project deadline roughly every month, before each of the four exams. This will allow you to apply the techniques you learn in class to your project in a timely manner."
  },
  {
    "objectID": "project/project.html#data-science-lifecycle",
    "href": "project/project.html#data-science-lifecycle",
    "title": "Course Project",
    "section": "Data Science Lifecycle",
    "text": "Data Science Lifecycle\nThe steps of your project are to mirror the general data science lifecycle pipeline.\nThe data science lifecycle is a high-level overview of the data science workflow. It’s a cycle of stages that a data scientist should explore as they conduct a thorough analysis of a data-driven problem.\nThere are many variations of the key ideas present in the data science lifecycle. Here, we visualize the stages of the lifecycle using a flow diagram."
  },
  {
    "objectID": "project/project.html#ask-a-question-and-obtain-data",
    "href": "project/project.html#ask-a-question-and-obtain-data",
    "title": "Course Project",
    "section": "1. Ask a Question and Obtain Data",
    "text": "1. Ask a Question and Obtain Data\n **Due before Exam 1**\nWhether by curiosity or necessity, data scientists will constantly ask questions. For example, in the business world, data scientists may be interested in predicting the profit generated by a certain investment. In the field of medicine, they may ask whether some patients are more likely than others to benefit from a treatment.\nPosing questions is one of the primary ways the data science lifecycle begins. It helps to fully define the question. Here are some things you should ask yourself before framing a question.\n\nWhat do we want to know?\n\nA question that is too ambiguous may lead to confusion.\n\nWhat problems are we trying to solve?\n\nThe goal of asking a question should be clear in order to justify your efforts to stakeholders.\n\nWhat are the hypotheses we want to test?\n\nThis gives a clear perspective from which to analyze final results.\n\nWhat are the metrics for our success?\n\nThis gives a clear point to know when to finish the project.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n| 1. Ask a Question | 2. Obtain Data |\nThe second entry point to the lifecycle is by obtaining data. A careful analysis of any problem requires the use of data. Data may be readily available to us, or we may have to embark on a process to collect it. When doing so, its crucial to ask the following:\n\nWhat data do we have and what data do we need?\n\nDefine the units of the data (people, cities, points in time, etc.) and what features to measure.\n\nHow will we sample more data?\n\nScrape the web, collect manually, etc.\n\nIs our data representative of the population we want to study?\n\nIf our data is not representative of our population of interest, then we can come to incorrect conclusions.\n\n\n\n\n\n\n\n\nProject Phase 1\n\n\n\nIdentify or collect a publicly available data set that you will use for your project. A list of possible datasets and project ideas are provided here but you are allowed to propose your own project and data set. Just be mindful of the scale of the data: it should be large enough to be appropriate for usage in Machine Learning, but not so large that it is unwieldy.\nKey procedures: Data Acquisition, Data Cleaning"
  },
  {
    "objectID": "project/project.html#understand-the-data",
    "href": "project/project.html#understand-the-data",
    "title": "Course Project",
    "section": "2. Understand the Data",
    "text": "2. Understand the Data\n\n\n\n\n\n\nCaution\n\n\n\nDue before Exam 2\n\n\nRaw data itself is not inherently useful. It’s impossible to discern all the patterns and relationships between variables without carefully investigating them. Therefore, translating pure data to actionable insights is a key job of a data scientist. For example, we may choose to ask:\n\nHow is our data organized and what does it contain?\n\nKnowing what the data says about the world helps us better understand the world.\n\nDo we have relevant data?\n\nIf the data we have collected is not useful to the question at hand, then we must collected more data.\n\nWhat are the biases, anomalies, or other issues with the data?\n\nThese can lead to many false conclusions if ignored, so data scientists must always be aware of these issues.\n\nHow do we transform the data to enable effective analysis?\n\nData is not always easy to interpret at first glance, so a data scientist should reveal these hidden insights.\n\n\n\n\n\n\n\n\nProject Phase 2\n\n\n\nPerform some exploratory analysis to help you get acquainted with the data. This may include data visualization and basic preliminary analysis. Identify interesting aspects of the data set that would be useful for downstream prediction: correlations, outliers, missing values, etc.\nKey procedures: Exploratory data analysis, Data visualization."
  },
  {
    "objectID": "project/project.html#understand-the-world",
    "href": "project/project.html#understand-the-world",
    "title": "Course Project",
    "section": "3. Understand the World",
    "text": "3. Understand the World\n\n\n\n\n\n\nCaution\n\n\n\nDue before Final Exam\n\n\nAfter observing the patterns in our data, we can begin answering our question. This may require that we predict a quantity (machine learning), or measure the effect of some treatment (inference).\nFrom here, we may choose to report our results, or possibly conduct more analysis. We may not be satisfied by our findings, or our initial exploration may have brought up new questions that require a new data.\n\nWhat does the data say about the world?\n\nGiven our models, the data will lead us to certain conclusions about the real world.\n\n\nDoes it answer our questions or accurately solve the problem?\n\nIf our model and data can not accomplish our goals, then we must reform our question, model, or both.\n\n\nHow robust are our conclusions and can we trust the predictions?\n\nInaccurate models can lead to untrue conclusions.\n\n\n\n\n\n\n\n\nProject Phase 3\n\n\n\nPreprocess data to change raw feature vectors into a representation that is more suitable for the downstream analysis. This may include data cleaning, calculating derivative or second-order variables, feature extraction, and feature selection.\nImplement baseline models covered in class and report their performance.\nIdentify, implement and apply as many models as relevant from class to predict some aspect of the data. This must be a supervised learning model. This may include, model selection, and model evaluation.\nYou must compare your results to a number of baselines, including random predictor, major class classifier and/or autocorrelation model. An example table is shown below:\n\n\n\nModel\nAccuracy\nPrecision\nRecall\nF1-score\n\n\n\n\nRandom baseline\n0.5\n0.52\n0.55\n0.53\n\n\nMajority class / Autocorrelation\n0.75\n0.5\n1\n0.66\n\n\nModel 1\n0.77\n0.72\n0.74\n0.73\n\n\nModel 2\n0.85\n0.79\n0.89\n0.88\n\n\n\nDiscuss the results of your analysis. This must include stating the assumptions of the model, the limitations of the model, a thorough error analysis and future directions.\nKey procedures: Model Creation, Prediction, Inference, Model Selection, Error Analysis."
  },
  {
    "objectID": "syllabus/textbook.html",
    "href": "syllabus/textbook.html",
    "title": "Textbooks & Other Resources",
    "section": "",
    "text": "Caution\n\n\n\nPlease note that the following textbooks are NOT strictly required for this course, but they are strongly recommended for those whoprefer to have a physical reference.",
    "crumbs": [
      "Textbooks & Other Resources"
    ]
  },
  {
    "objectID": "syllabus/textbook.html#other-resources",
    "href": "syllabus/textbook.html#other-resources",
    "title": "Textbooks & Other Resources",
    "section": "Other Resources",
    "text": "Other Resources\n\nMining of Massive Datasets by Jure Leskovec, Anand Rajaraman, Jeffrey D. Ullman\nProbabilistic Programming & Bayesian Methods for Hackers by Cameron Davidson-Pilon\nIntroduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani\n\n\n\n\nJupyter Notebook User Guide\nPython Data Science Handbook by Jake VanderPlas\nPython: How to Think Like a Computer Scientist (Swarthmore Edition) by Jeffrey Elkner, Allen B. Downey, and Chris Meyers (free, open textbook)",
    "crumbs": [
      "Textbooks & Other Resources"
    ]
  },
  {
    "objectID": "syllabus/mental_health.html",
    "href": "syllabus/mental_health.html",
    "title": "Mental Health Resources",
    "section": "",
    "text": "Empowering and equipping students to manage their mental health and academic success, the Counseling Center’s stepped care model offers an array of evidence based services.\nThe resources listed below are free, confidential and accessible to all enrolled students. Go to the Counseling Center Website for details.\n\n\nFurman University Counseling Center Mental Health and Crisis Support Line – Call the Counseling Center at 864-294-3031, press #3 (confidential, available 24/7/365 from anywhere).\n\n\n\n\nHeadspace – a mindfulness app that helps decrease stress and improve focus and mind-wandering, sponsored by SGA and PHOKUS. Students may enroll using their Furman email.\nTAO Connect – a self-help platform (anonymous and confidential, 24/7) sponsored by the Counseling Center and accessible to students, faculty and staff. Enroll with a Furman email.\n\n\n\n\n\nPaladin Peer Support is a student peer mentoring organization focused on wellness and self-efficacy. Follow them on Instagram and connect for support in reaching personal well-being goals. ### Skill Building Groups and Workshops\nRotating evidence-based psycho-education and skill building groups for anxiety and emotional regulation ### Consultation and Treatment Services\nStart Strong and Finish Strong Walk-in Clinics (first and last two weeks of every semester)\nBrief individual counseling (in person and online), which may include psychiatric and nutrition consults where clinically indicated.\nSingle Session Consultations\nGroup Counseling and Skill Building Workshops\n\n\n\n\n\n\nThe Office for Spiritual Life provides individual confidential counseling for students, faculty and staff in person and online\nGroups and workshops that are theme-focused and interpersonal\nContact OSL@furman.edu, 864-294-2133, or contact a chaplain directly: vaughn.crowetipton@furman.edu, kate.taber@furman.edu.",
    "crumbs": [
      "Mental Health Resources"
    ]
  },
  {
    "objectID": "syllabus/mental_health.html#the-counseling-center",
    "href": "syllabus/mental_health.html#the-counseling-center",
    "title": "Mental Health Resources",
    "section": "",
    "text": "Empowering and equipping students to manage their mental health and academic success, the Counseling Center’s stepped care model offers an array of evidence based services.\nThe resources listed below are free, confidential and accessible to all enrolled students. Go to the Counseling Center Website for details.\n\n\nFurman University Counseling Center Mental Health and Crisis Support Line – Call the Counseling Center at 864-294-3031, press #3 (confidential, available 24/7/365 from anywhere).\n\n\n\n\nHeadspace – a mindfulness app that helps decrease stress and improve focus and mind-wandering, sponsored by SGA and PHOKUS. Students may enroll using their Furman email.\nTAO Connect – a self-help platform (anonymous and confidential, 24/7) sponsored by the Counseling Center and accessible to students, faculty and staff. Enroll with a Furman email.\n\n\n\n\n\nPaladin Peer Support is a student peer mentoring organization focused on wellness and self-efficacy. Follow them on Instagram and connect for support in reaching personal well-being goals. ### Skill Building Groups and Workshops\nRotating evidence-based psycho-education and skill building groups for anxiety and emotional regulation ### Consultation and Treatment Services\nStart Strong and Finish Strong Walk-in Clinics (first and last two weeks of every semester)\nBrief individual counseling (in person and online), which may include psychiatric and nutrition consults where clinically indicated.\nSingle Session Consultations\nGroup Counseling and Skill Building Workshops",
    "crumbs": [
      "Mental Health Resources"
    ]
  },
  {
    "objectID": "syllabus/mental_health.html#spiritual-life",
    "href": "syllabus/mental_health.html#spiritual-life",
    "title": "Mental Health Resources",
    "section": "",
    "text": "The Office for Spiritual Life provides individual confidential counseling for students, faculty and staff in person and online\nGroups and workshops that are theme-focused and interpersonal\nContact OSL@furman.edu, 864-294-2133, or contact a chaplain directly: vaughn.crowetipton@furman.edu, kate.taber@furman.edu.",
    "crumbs": [
      "Mental Health Resources"
    ]
  },
  {
    "objectID": "syllabus/appointment.html",
    "href": "syllabus/appointment.html",
    "title": "Appointment",
    "section": "",
    "text": "Appointment\nI don’t have fixed office hours this semester. Instead, I am using an appointment scheduling system Calendly to make it easier for you to find a time that works for you.\nYou can schedule an appointment with me using using this link\nI also have an Open-door Policy and am generally in my office (Riley Hall 200-D) from 10 AM - 4 PM on most weekdays. You are always welcome to drop by and chat.\nI am also ofcourse available via email."
  },
  {
    "objectID": "syllabus/github.html",
    "href": "syllabus/github.html",
    "title": "Github Classroom",
    "section": "",
    "text": "Link to CSC-223 Github Classroom"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSC-272: Data Mining",
    "section": "",
    "text": "Hi! Welcome to the CSC-272 Introduction to Data Mining course website 👋🏾\nI am excited to talk math 🔢, programming 💻 and all-things data 📊 in this course with you!\nFrom the navigation bar on the top ☝🏾 and sidebar on the left 👈🏾, you should be able to navigate to any topic relevant to the course. If that does not help, there should also be a search icon 🔍 in the top left corner ↗️\n👇🏾 Below, you can find important links and important announcements.\n\n\n\n\n\n\nOpen-Door Policy\n\n\n\nI have an Open-Door Policy and am in my office (Riley Hall 200-D) between 9 AM - 5 PM most weekdays. Please drop by with your questions or just to say hi!\nAlternatively, send me an email or schedule an appointment",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "probability/prob0.html",
    "href": "probability/prob0.html",
    "title": "PROBABILITY",
    "section": "",
    "text": "PROBABILITY"
  },
  {
    "objectID": "probability/bayestheorem.html",
    "href": "probability/bayestheorem.html",
    "title": "Bayes’ Theorem",
    "section": "",
    "text": "Bayes’ Theorem"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Syed Fahad Sultan سید فہد سلطان \nPronunciation: Saiyyudh Fahad Sool-tahn\nJust call me “Dr. Sultan” (click on the speaker for a short audio clip: 🔈)\n\n\n\n\n\nI am originally from Lahore, Pakistan and joined Furman University in Fall 2022 after earning my Ph.D. in Computer Science from State University of New York at Stony Brook.\n\n\n\n\n\nFresh out of college, I worked as a professional video game developer for a startup that later got acquired by the Japanese gaming giant DeNA. During this time, I was part of the team that built TapFish, the top-grossing game worldwide, for two weeks in 2011, on both the App Store and Google Play.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo games development\n\n\n\nUrban Sensing\n\n\n\nComputational Neuroscience\n\n\n\n\n\n\n\n\nI then went on to work at Technology for People Initiative, an applied research lab in my where I mined social media and cell phone data for proxies of socio-economic indicators that allowed more inclusive policy-making for marginalized communities. During these years, I also dabbled in data journalism and helped organize a boot camp on using data for journalists with the support of the International Center for Journalists (ICFJ) and the Knight Foundation.\n\nIn 2015, I moved to Mecca, Saudi Arabia to work for the GIS Innovation Center (now Wadi Makkah). There I worked on innovative urban sensing techniques for better crowd control during the annual pilgrimage to the city, the largest human gathering in the world every year.\nDuring my PhD, I worked at the intersection of computational neuroscience, bioinformatics and machine learning. My work focused on identifying neurological and genetic biomarkers linking type-2 diabetes with cognitive disorders such as Alzheimer’s and other dementias.\nI live in Travelers Rest with my wife and cat.\n\n\nOffice: Riley Hall 200-D\nEmail: fahad.sultan@furman.edu\nMy office hours this semester are 10 - 11:30 AM on Mondays and Wednesdays.\nI have an Open door policy. I am in my office during work hours most weekdays and my door is only closed if I am in a class or in a meeting. So please drop by.\nYou can also schedule a meeting using this link if you want to absolutely make sure that I am available.\n\n\n\n\n\n\nCourse website: https://fahadsultan.com/csc223\nThe Syllabus is available on the course website. In particular, please make sure to read the Grading, Academic Integrity and Textbook and other Resources sections carefully.\nAll of the course content will be posted on this website.\nImportant announcements will be made on both the course website homepage and in class.\nYou are to submit assignments and exams on the course Moodle page. I will also upload all of your grades there.\n\n\n\nDeclarative knowledge is knowledge about facts. It is knowledge that answers the “What is” questions. Most courses outside Computer Science are about declarative knowledge.\nIn contrast, Imperative knowledge is knowledge about how to do things. It is knowledge that answers the “How to” questions.\nWhile we will spend a non-trivial amount of time in this course on declarative knowledge, the overwhelming majority of this course will focus on imperative knowledge. Your grade in this course will be determined by your ability to apply declarative and more importantly imperative knowledge to solve problems.\n\nResearch shows that there is only one way to acquire imperative knowledge: Practice, Practice, Practice !. Practice combined with feedback is the only way to achieve mastery.\nIn this course, you will be given ample opportunities to practice along with regular feedback.\n\n\n\nApproach assignments purely as opportunities to learn, prepare for exams and to prepare for your career.\n\nIt is not worth cheating on assignments. Just come talk to me if you are struggling with an assignment. I will literally just tell you the answer.\nOn assignments, expect near maximal flexibility from me. Every assignment will be due 10 days calendar after it is posted.\nYou can schedule a time to get your assignments graded using this link.\nWritten Assignments:\nWritten assignments are to help you build a deeper understanding of algorithms and math covered in class.\nThese could simply be math problems or involve tracing algorithms and dry-runs.\nBoth handwritten or typed submissions are acceptable. Submissions, as always, on Moodle.\nProgramming Assignments:\nProgramming assignments are going to be posted at the start of the lab session each week and will be due in 10 days, unless otherwise specified.\nAll Programming assignments will be graded through an in-person code review. You are to give a walkthrough of your code and be able to answer questions about it.\nDuring these code review, you will be given feedback on how to improve your code and avoid common mistakes.\nYou should expect questions in the exams similar to assignments.\n\n\n\nI have created 10 graded items under class participation on Moodle. In class, you will be asked to answer a question or solve a problem. You will be graded on the basis of your participation. It is your responsibility to make sure you have 10 points by the end of the semester.\nThere are 10 graded items under class participation on Moodle. In class, you will be asked to answer a question or solve a problem. You will be graded on the basis of your participation. It is your responsibility to make sure you have 10 points by the end of the semester.\n\\[\\frac{24~\\text{students} \\times 10~\\text{points needed by each student}}{15~\\text{weeks} \\times 2~\\text{classes per week}} = 8~\\text{points given out class, on average}\\]\nI will give out class participation points in every class class for answering or asking a question.\nGiven the glut of information accessible online and otherwise in this day and age, meaningful interactions with your peers and teachers is essentially why you are paying your college tuition.\nPlease come to class, labs and office hours\nPlease ask questions during class\nPlease answer questions and participate in discussions during class\n\n\n\nThere will be three exams in the course, including the final. The final exam will be cumulative. Exams constitute 60% of your course grade.\nAll exams will be on computer, with a large programming component. Questions will be posted on Moodle and you will have to submit your solutions on Moodle, just like assignments.\nYou will be evaluated on your ability to apply knowledge to new problems and not just on your ability to retain and recall information.\nThe exams, more than the assignments, are going to determine your grade.\nAll exams are going to be cumulative, with focus on the topics covered since last exam.\nDiligent work on the homework and assignments will be rewarded here.\n\n\n\nEverything is tentative and subject to change\n\nThis is my first teaching this course. Any and all feedback is welcome!\nI have created an anonymous feedback poll on Moodle. Please use this to anonymously share any feedback.\nShare any changes you want me to make in the course, at any point in the semester. You can submit multiple times over the span of the semester.\nThink of it as a Complaints Box for the course.\n\n\n\n\n“Data Mining” is a term from the 1990s, back when it was an exciting and popular new field. Around 2010, people instead started to speak of “big data”. Today, the popular term is “data science”. There are some who even regard data mining as synonymous with machine learning. There is no question that some data mining appropriately uses algorithms from machine learning. However, during all this time, the concept remained the same: use the most powerful hardware, the most powerful programming systems, and the most efficient algorithms to solve problems in science, commerce, healthcare, government, the humanities, and many other fields of human endeavor.\n\n\n\nFrom the Venn Diagram, the course content is going to cover ✅ Hacking Skills and ✅ Math & Statistics in detail but not ☐ Substantive Expertise. For that missing piece, I strongly encourage you to bring in knowledge from your GERs and other Non-CS department courses into this class and the term project in particular. Nothing would make me happier than to see projects that combines CS with your other interests.\n\n\n\n“But wait, I am not a Math Person!” you say!\nThere is no such thing as a “Math Person”. I do recognize, however, that Math Anxiety is a real thing and is very common. It is a feeling of fear based on a belief that one is not good at math or that math is inherently difficult.\nPlease use this course as an opportunity to overcome your Math anxiety!\nIn this course, the code you write will be mostly math. Most modern “AI” is just that: math, in code.\nThis presents a unique opportunity for you to overcome your Math anxiety. You will be able to see the math in action, be able to visualize the results and have a conversation with it.\nTrust me, there is a tremendous amount of beauty and joy to be found in mathematics. And if beauty and joy aren’t really your thing, then let me also assure you there is a lot of money to be made these days by being good at coding math. Either way, the rewards are well worth the effort!",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#about-the-course",
    "href": "intro.html#about-the-course",
    "title": "Introduction",
    "section": "",
    "text": "Course website: https://fahadsultan.com/csc223\nThe Syllabus is available on the course website. In particular, please make sure to read the Grading, Academic Integrity and Textbook and other Resources sections carefully.\nAll of the course content will be posted on this website.\nImportant announcements will be made on both the course website homepage and in class.\nYou are to submit assignments and exams on the course Moodle page. I will also upload all of your grades there.\n\n\n\nDeclarative knowledge is knowledge about facts. It is knowledge that answers the “What is” questions. Most courses outside Computer Science are about declarative knowledge.\nIn contrast, Imperative knowledge is knowledge about how to do things. It is knowledge that answers the “How to” questions.\nWhile we will spend a non-trivial amount of time in this course on declarative knowledge, the overwhelming majority of this course will focus on imperative knowledge. Your grade in this course will be determined by your ability to apply declarative and more importantly imperative knowledge to solve problems.\n\nResearch shows that there is only one way to acquire imperative knowledge: Practice, Practice, Practice !. Practice combined with feedback is the only way to achieve mastery.\nIn this course, you will be given ample opportunities to practice along with regular feedback.\n\n\n\nApproach assignments purely as opportunities to learn, prepare for exams and to prepare for your career.\n\nIt is not worth cheating on assignments. Just come talk to me if you are struggling with an assignment. I will literally just tell you the answer.\nOn assignments, expect near maximal flexibility from me. Every assignment will be due 10 days calendar after it is posted.\nYou can schedule a time to get your assignments graded using this link.\nWritten Assignments:\nWritten assignments are to help you build a deeper understanding of algorithms and math covered in class.\nThese could simply be math problems or involve tracing algorithms and dry-runs.\nBoth handwritten or typed submissions are acceptable. Submissions, as always, on Moodle.\nProgramming Assignments:\nProgramming assignments are going to be posted at the start of the lab session each week and will be due in 10 days, unless otherwise specified.\nAll Programming assignments will be graded through an in-person code review. You are to give a walkthrough of your code and be able to answer questions about it.\nDuring these code review, you will be given feedback on how to improve your code and avoid common mistakes.\nYou should expect questions in the exams similar to assignments.\n\n\n\nI have created 10 graded items under class participation on Moodle. In class, you will be asked to answer a question or solve a problem. You will be graded on the basis of your participation. It is your responsibility to make sure you have 10 points by the end of the semester.\nThere are 10 graded items under class participation on Moodle. In class, you will be asked to answer a question or solve a problem. You will be graded on the basis of your participation. It is your responsibility to make sure you have 10 points by the end of the semester.\n\\[\\frac{24~\\text{students} \\times 10~\\text{points needed by each student}}{15~\\text{weeks} \\times 2~\\text{classes per week}} = 8~\\text{points given out class, on average}\\]\nI will give out class participation points in every class class for answering or asking a question.\nGiven the glut of information accessible online and otherwise in this day and age, meaningful interactions with your peers and teachers is essentially why you are paying your college tuition.\nPlease come to class, labs and office hours\nPlease ask questions during class\nPlease answer questions and participate in discussions during class\n\n\n\nThere will be three exams in the course, including the final. The final exam will be cumulative. Exams constitute 60% of your course grade.\nAll exams will be on computer, with a large programming component. Questions will be posted on Moodle and you will have to submit your solutions on Moodle, just like assignments.\nYou will be evaluated on your ability to apply knowledge to new problems and not just on your ability to retain and recall information.\nThe exams, more than the assignments, are going to determine your grade.\nAll exams are going to be cumulative, with focus on the topics covered since last exam.\nDiligent work on the homework and assignments will be rewarded here.\n\n\n\nEverything is tentative and subject to change\n\nThis is my first teaching this course. Any and all feedback is welcome!\nI have created an anonymous feedback poll on Moodle. Please use this to anonymously share any feedback.\nShare any changes you want me to make in the course, at any point in the semester. You can submit multiple times over the span of the semester.\nThink of it as a Complaints Box for the course.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#what-is-data-mining",
    "href": "intro.html#what-is-data-mining",
    "title": "Introduction",
    "section": "",
    "text": "“Data Mining” is a term from the 1990s, back when it was an exciting and popular new field. Around 2010, people instead started to speak of “big data”. Today, the popular term is “data science”. There are some who even regard data mining as synonymous with machine learning. There is no question that some data mining appropriately uses algorithms from machine learning. However, during all this time, the concept remained the same: use the most powerful hardware, the most powerful programming systems, and the most efficient algorithms to solve problems in science, commerce, healthcare, government, the humanities, and many other fields of human endeavor.\n\n\n\nFrom the Venn Diagram, the course content is going to cover ✅ Hacking Skills and ✅ Math & Statistics in detail but not ☐ Substantive Expertise. For that missing piece, I strongly encourage you to bring in knowledge from your GERs and other Non-CS department courses into this class and the term project in particular. Nothing would make me happier than to see projects that combines CS with your other interests.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#expect-lots-of-programming-and-lots-of-math",
    "href": "intro.html#expect-lots-of-programming-and-lots-of-math",
    "title": "Introduction",
    "section": "",
    "text": "“But wait, I am not a Math Person!” you say!\nThere is no such thing as a “Math Person”. I do recognize, however, that Math Anxiety is a real thing and is very common. It is a feeling of fear based on a belief that one is not good at math or that math is inherently difficult.\nPlease use this course as an opportunity to overcome your Math anxiety!\nIn this course, the code you write will be mostly math. Most modern “AI” is just that: math, in code.\nThis presents a unique opportunity for you to overcome your Math anxiety. You will be able to see the math in action, be able to visualize the results and have a conversation with it.\nTrust me, there is a tremendous amount of beauty and joy to be found in mathematics. And if beauty and joy aren’t really your thing, then let me also assure you there is a lot of money to be made these days by being good at coding math. Either way, the rewards are well worth the effort!",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "syllabus/grading.html",
    "href": "syllabus/grading.html",
    "title": "Grading",
    "section": "",
    "text": "Component\nPercentage\n\n\n\n\nWritten Assignments\n10%\n\n\nProgramming Assignments\n10%\n\n\nClass Participation\n10%\n\n\nExam 1\n10%\n\n\nExam 2\n15%\n\n\nFinal Exam\n25%\n\n\nProject\n20%\n\n\n\n\n\n\n(+/- at instructor’s discretion)\n\n\n\nLetter Grade\nRange\n\n\n\n\nA\n&gt; 90 %\n\n\nB\n80 - 90 %\n\n\nC\n70 - 80 %\n\n\nD\n60 - 70 %\n\n\nF\n&lt; 60 %\n\n\n\n\n\n\nIn order to pass this class, you must\n\nEarn \\(\\geq\\) 60% of the total points\nAttend \\(\\geq\\) 80% of the lectures and labs.\nSubmit \\(\\geq\\) 80% of written and programmming assignments.\nTake ALL tests and final!\nEarn \\(\\geq\\) 50% in each of the 7 components above. In other words, you cannot blow off an entire aspect of the course and pass this class!\n\nNote that this basic requirement is necessary but not sufficient to pass the class.",
    "crumbs": [
      "Grading"
    ]
  },
  {
    "objectID": "syllabus/grading.html#breakdown",
    "href": "syllabus/grading.html#breakdown",
    "title": "Grading",
    "section": "",
    "text": "Component\nPercentage\n\n\n\n\nWritten Assignments\n10%\n\n\nProgramming Assignments\n10%\n\n\nClass Participation\n10%\n\n\nExam 1\n10%\n\n\nExam 2\n15%\n\n\nFinal Exam\n25%\n\n\nProject\n20%",
    "crumbs": [
      "Grading"
    ]
  },
  {
    "objectID": "syllabus/grading.html#scale",
    "href": "syllabus/grading.html#scale",
    "title": "Grading",
    "section": "",
    "text": "(+/- at instructor’s discretion)\n\n\n\nLetter Grade\nRange\n\n\n\n\nA\n&gt; 90 %\n\n\nB\n80 - 90 %\n\n\nC\n70 - 80 %\n\n\nD\n60 - 70 %\n\n\nF\n&lt; 60 %",
    "crumbs": [
      "Grading"
    ]
  },
  {
    "objectID": "syllabus/grading.html#passing-requirements",
    "href": "syllabus/grading.html#passing-requirements",
    "title": "Grading",
    "section": "",
    "text": "In order to pass this class, you must\n\nEarn \\(\\geq\\) 60% of the total points\nAttend \\(\\geq\\) 80% of the lectures and labs.\nSubmit \\(\\geq\\) 80% of written and programmming assignments.\nTake ALL tests and final!\nEarn \\(\\geq\\) 50% in each of the 7 components above. In other words, you cannot blow off an entire aspect of the course and pass this class!\n\nNote that this basic requirement is necessary but not sufficient to pass the class.",
    "crumbs": [
      "Grading"
    ]
  },
  {
    "objectID": "syllabus/integrity.html",
    "href": "syllabus/integrity.html",
    "title": "Academic Integrity",
    "section": "",
    "text": "Academic Integrity\nAcademic Integrity standards are important to our Furman community and will be upheld in this class. Students should review the Academic Integrity Pledge and other resources available on the Academic Integrity page on the Furman website. Pay special attention to definitions of cheating, plagiarism, unacceptable collaboration, facilitating misconduct and other types of misrepresentation. All those apply in this course.\nFor programming assignments/homeworks and labs, follow the 50 foot policy in its spirit.\n\nIn this class, the grade penalty for an academic integrity violation is an F for the course. Academic Discipline procedures will be followed through the Office of the Academic Dean.",
    "crumbs": [
      "Academic Integrity"
    ]
  },
  {
    "objectID": "syllabus/about.html",
    "href": "syllabus/about.html",
    "title": "Course Description and Goals",
    "section": "",
    "text": "CSC-121 Introduction to Computer Programming\n\n\n\n\n\nCore course requirement for Data Analytics minor\nHuman Behavior (HB) General Education Requirement (GER)\nPre-requisite for CSC-372: Machine Learning with Big Data\n\n\n\n\nThis course focuses on the algorithms and computing tools fundamental to data science: the process of extracting accurate and generalizable models from data via machine learning. Topics will include the prediction of outcomes, the discovery of associations, and the identification of similar groups. Students will complete a project related to human behavior, starting with data collection and cleaning, culminating in the presentation of a model and visualization of results.\n\n\n\n\nOn successful completion of the course, the students should have the ability to identify and apply appropriate data mining and/or machine learning techniques towards solving problems of pattern recogition, learning and prediction. The course also aims to instill in students a deep sensitivity of issues of algorithmic bias and fairness in data mining.",
    "crumbs": [
      "Course Description and Goals"
    ]
  },
  {
    "objectID": "syllabus/about.html#pre-requisites",
    "href": "syllabus/about.html#pre-requisites",
    "title": "Course Description and Goals",
    "section": "",
    "text": "CSC-121 Introduction to Computer Programming",
    "crumbs": [
      "Course Description and Goals"
    ]
  },
  {
    "objectID": "syllabus/about.html#fulfills-requirements",
    "href": "syllabus/about.html#fulfills-requirements",
    "title": "Course Description and Goals",
    "section": "",
    "text": "Core course requirement for Data Analytics minor\nHuman Behavior (HB) General Education Requirement (GER)\nPre-requisite for CSC-372: Machine Learning with Big Data",
    "crumbs": [
      "Course Description and Goals"
    ]
  },
  {
    "objectID": "syllabus/about.html#course-description",
    "href": "syllabus/about.html#course-description",
    "title": "Course Description and Goals",
    "section": "",
    "text": "This course focuses on the algorithms and computing tools fundamental to data science: the process of extracting accurate and generalizable models from data via machine learning. Topics will include the prediction of outcomes, the discovery of associations, and the identification of similar groups. Students will complete a project related to human behavior, starting with data collection and cleaning, culminating in the presentation of a model and visualization of results.",
    "crumbs": [
      "Course Description and Goals"
    ]
  },
  {
    "objectID": "syllabus/about.html#course-goals",
    "href": "syllabus/about.html#course-goals",
    "title": "Course Description and Goals",
    "section": "",
    "text": "On successful completion of the course, the students should have the ability to identify and apply appropriate data mining and/or machine learning techniques towards solving problems of pattern recogition, learning and prediction. The course also aims to instill in students a deep sensitivity of issues of algorithmic bias and fairness in data mining.",
    "crumbs": [
      "Course Description and Goals"
    ]
  },
  {
    "objectID": "syllabus/title_9.html",
    "href": "syllabus/title_9.html",
    "title": "Nondiscrimination Policy and Sexual Misconduct",
    "section": "",
    "text": "Nondiscrimination Policy and Sexual Misconduct\nFurman University and its faculty are committed to supporting our students and seeking an environment that is free of bias, discrimination, and harassment. Furman does not unlawfully discriminate on the basis of race, color, national origin, sex, sexual orientation, gender identity, pregnancy, disability, age, religion, veteran status, or any other characteristic or status protected by applicable local, state, or federal law in admission, treatment, or access to, or employment in, its programs and activities.\nIf you have encountered any form of discrimination or harassment, including sexual misconduct (e.g. sexual assault, sexual harassment or gender-based harassment, sexual exploitation or intimidation, stalking, intimate partner violence), we encourage you to report this to the institution. If you wish to report such an incident of misconduct, you may contact Furman’s Title IX Coordinator, Melissa Nichols (Trone Center, Suite 215; Melissa.nichols@furman.edu; 864.294.2221).\nIf you would like to speak with someone who can advise you but maintain complete confidentiality, you can talk with a counselor, a professional in the Student Health Center or someone in the Office of Spiritual Life. If you speak with a faculty member, understand that as a “Responsible Employee” of the University, the faculty member MUST report to the University’s Title IX Coordinator what you share to help ensure that your safety and welfare are being addressed, consistent with the requirements of the law.\nAdditional information about Furman’s Sexual Misconduct Policy, how to report sexual misconduct and your rights can be found at the Furman Title IX Webpage. You do not have to go through the experience alone.",
    "crumbs": [
      "Nondiscrimination Policy and Sexual Misconduct"
    ]
  },
  {
    "objectID": "syllabus/accomodations.html",
    "href": "syllabus/accomodations.html",
    "title": "Accomodations",
    "section": "",
    "text": "Accomodations\nFurman University recognizes a student with a disability as anyone whose impairment substantially limits one or more major life activity. Students may receive a variety of services including classroom accommodations such as extended time on tests, test proctoring, note-taking assistance and access to assistive technology. However, receipt of reasonable accommodations cannot guarantee success–all students are responsible for meeting academic standards. Students with a diagnosed disability may be entitled to accommodations under the Americans with Disabilities Act (ADA).\nPlease visit Student Office for Accessibility Resources for more info.",
    "crumbs": [
      "Accomodations"
    ]
  },
  {
    "objectID": "syllabus/academic_success.html",
    "href": "syllabus/academic_success.html",
    "title": "Center for Academic Success",
    "section": "",
    "text": "Center for Academic Success\nPeer Tutors are available free of charge for many classes and may be requested by dropping by CAS (LIB 002) or on the Center for Academic Success website. Tutors are typically recommended by faculty and have performed well in the class. \nThe Writing & Media Lab (WML) is staffed by student Consultants who are trained to help you improve your writing and multimodal communication skills. The consultation process is non-directive and intended to allow students to maintain ownership of their work. In addition to helping with the nuts and bolts, WML Consultants also support you in developing your own ideas thoughtfully and critically, whether you’re writing an essay or planning a video or other multimedia project. You may drop into the WML during its regular hours (LIB 002; 9 AM to 10 PM) or visit the **Writing and Media Lab website to make an appointment online.\nProfessional Academic Assistance Staff in CAS can provide students assistance with time management, study skills, and organizational skills.\nThe Writing and ESL Specialist provides professional writing support as well as support for students whose primary language is not English.",
    "crumbs": [
      "Center for Academic Success"
    ]
  },
  {
    "objectID": "project/data.html",
    "href": "project/data.html",
    "title": "Data sets and Project ideas",
    "section": "",
    "text": "Good places to look for data sets:\n\nGoogle’s Dataset Search\nKaggle\n\nAwesome datasets\n\n\n\nFollowing data sets and ideas are only there to give you a starting point. You are free to propose a data set or project idea not listed here.\n\n\nMovies: i) Scripts data ii) Subtitles data iii) IMDB Dataset\nMusic: i) Million Song Dataset ii) Last.fm Dataset iii) Spotify Dataset iv) Lyrics data\nTV series: i) TV Series Dataset ii) Subtitles data iii) IMDB Dataset\nBooks: i) Goodreads Dataset ii) Book Reviews Dataset iii) Book Summaries Dataset\nSocio-Economic: i) S&P 500 ii) World Development Indicators\nEnvironment: i) Earth Surface Temperature ii) US Pollution Data\nSports: i) College Basketball ii) FIFA Soccer Rankings iii) Cricket\n\n\n\n\n\n\nImplement a Recommendation System using the data sets above. {bdg-primary}Unsupervised  \nPredict the Genre / Artist of the media using the data sets above. {bdg-primary}Supervised  \nPredict the Rating / Popularity / Revenue of the media using the data sets above. {bdg-primary}Regression  \nUse historical trend to predict future value of an indicator. {bdg-primary}Time Series"
  },
  {
    "objectID": "project/data.html#data-sets",
    "href": "project/data.html#data-sets",
    "title": "Data sets and Project ideas",
    "section": "",
    "text": "Following data sets and ideas are only there to give you a starting point. You are free to propose a data set or project idea not listed here.\n\n\nMovies: i) Scripts data ii) Subtitles data iii) IMDB Dataset\nMusic: i) Million Song Dataset ii) Last.fm Dataset iii) Spotify Dataset iv) Lyrics data\nTV series: i) TV Series Dataset ii) Subtitles data iii) IMDB Dataset\nBooks: i) Goodreads Dataset ii) Book Reviews Dataset iii) Book Summaries Dataset\nSocio-Economic: i) S&P 500 ii) World Development Indicators\nEnvironment: i) Earth Surface Temperature ii) US Pollution Data\nSports: i) College Basketball ii) FIFA Soccer Rankings iii) Cricket"
  },
  {
    "objectID": "project/data.html#project-ideas",
    "href": "project/data.html#project-ideas",
    "title": "Data sets and Project ideas",
    "section": "",
    "text": "Implement a Recommendation System using the data sets above. {bdg-primary}Unsupervised  \nPredict the Genre / Artist of the media using the data sets above. {bdg-primary}Supervised  \nPredict the Rating / Popularity / Revenue of the media using the data sets above. {bdg-primary}Regression  \nUse historical trend to predict future value of an indicator. {bdg-primary}Time Series"
  },
  {
    "objectID": "linearalgebra/nearestneighbor.html",
    "href": "linearalgebra/nearestneighbor.html",
    "title": "Nearest Neighbor",
    "section": "",
    "text": "Nearest Neighbor"
  },
  {
    "objectID": "calculus/calculus0.html",
    "href": "calculus/calculus0.html",
    "title": "CALCULUS",
    "section": "",
    "text": "CALCULUS"
  },
  {
    "objectID": "pandas/pandas5.html",
    "href": "pandas/pandas5.html",
    "title": "Concatenation and Merging",
    "section": "",
    "text": "Another way to combine DataFrames is to concatenate them. Concatenation is a bit different from joining. When we join two DataFrames, we are combining them horizontally – that is, we are adding new columns to an existing DataFrame. Concatenation, on the other hand, is generally a vertical operation – we are adding new rows to an existing DataFrame.\n\n\n\n\n\n\npd.concat is the pandas method used to concatenate DataFrames together. It takes as input a list of DataFrames to be concatenated and returns a new DataFrame containing all of the rows from each input DataFrame in the input list.\nLet’s say we wanted to concatenate data from two different years in babynames. We can do so using the pd.concat method.\n\nimport pandas as pd \nurl_template = \"https://raw.githubusercontent.com/fahadsultan/csc272/main/data/names/yob%s.txt\"\n\ndata_list = []\nfor year in range(1880, 2023):\n    url = url_template % year\n    data = pd.read_csv(url, header=None, names=['name', 'sex', 'count'])\n    data['year'] = year\n    data_list.append(data)\nall_data = pd.concat(data_list)\nall_data\n\n\n\n\n\n\n\n\nname\nsex\ncount\nyear\n\n\n\n\n0\nMary\nF\n7065\n1880\n\n\n1\nAnna\nF\n2604\n1880\n\n\n2\nEmma\nF\n2003\n1880\n\n\n3\nElizabeth\nF\n1939\n1880\n\n\n4\nMinnie\nF\n1746\n1880\n\n\n...\n...\n...\n...\n...\n\n\n31910\nZuberi\nM\n5\n2022\n\n\n31911\nZydn\nM\n5\n2022\n\n\n31912\nZylon\nM\n5\n2022\n\n\n31913\nZymeer\nM\n5\n2022\n\n\n31914\nZymeire\nM\n5\n2022\n\n\n\n\n2085158 rows × 4 columns",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Concatenation and Merging"
    ]
  },
  {
    "objectID": "pandas/pandas5.html#concatenating-dataframes",
    "href": "pandas/pandas5.html#concatenating-dataframes",
    "title": "Concatenation and Merging",
    "section": "",
    "text": "Another way to combine DataFrames is to concatenate them. Concatenation is a bit different from joining. When we join two DataFrames, we are combining them horizontally – that is, we are adding new columns to an existing DataFrame. Concatenation, on the other hand, is generally a vertical operation – we are adding new rows to an existing DataFrame.\n\n\n\n\n\n\npd.concat is the pandas method used to concatenate DataFrames together. It takes as input a list of DataFrames to be concatenated and returns a new DataFrame containing all of the rows from each input DataFrame in the input list.\nLet’s say we wanted to concatenate data from two different years in babynames. We can do so using the pd.concat method.\n\nimport pandas as pd \nurl_template = \"https://raw.githubusercontent.com/fahadsultan/csc272/main/data/names/yob%s.txt\"\n\ndata_list = []\nfor year in range(1880, 2023):\n    url = url_template % year\n    data = pd.read_csv(url, header=None, names=['name', 'sex', 'count'])\n    data['year'] = year\n    data_list.append(data)\nall_data = pd.concat(data_list)\nall_data\n\n\n\n\n\n\n\n\nname\nsex\ncount\nyear\n\n\n\n\n0\nMary\nF\n7065\n1880\n\n\n1\nAnna\nF\n2604\n1880\n\n\n2\nEmma\nF\n2003\n1880\n\n\n3\nElizabeth\nF\n1939\n1880\n\n\n4\nMinnie\nF\n1746\n1880\n\n\n...\n...\n...\n...\n...\n\n\n31910\nZuberi\nM\n5\n2022\n\n\n31911\nZydn\nM\n5\n2022\n\n\n31912\nZylon\nM\n5\n2022\n\n\n31913\nZymeer\nM\n5\n2022\n\n\n31914\nZymeire\nM\n5\n2022\n\n\n\n\n2085158 rows × 4 columns",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Concatenation and Merging"
    ]
  },
  {
    "objectID": "pandas/pandas5.html#merging-dataframes",
    "href": "pandas/pandas5.html#merging-dataframes",
    "title": "Concatenation and Merging",
    "section": "Merging DataFrames",
    "text": "Merging DataFrames\nWhen working on data science projects, we’re unlikely to have absolutely all the data we want contained in a single DataFrame – a real-world data scientist needs to grapple with data coming from multiple sources. If we have access to multiple datasets with related information, we can merge two or more tables into a single DataFrame.\nTo put this into practice, we’ll revisit the elections dataset.\n\nelections.head(5)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n\n\n\n\n\nSay we want to understand the popularity of the names of each presidential candidate in 2020. To do this, we’ll need the combined data of names and elections.\n\n\n\n\nWe’ll start by creating a new column containing the first name of each presidential candidate. This will help us join each name in elections to the corresponding name data in names.\n\n# This `str` operation splits each candidate's full name at each \n# blank space, then takes just the candidiate's first name\nelections[\"First Name\"] = elections[\"Candidate\"].str.split().str[0]\nelections.head(5)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\nFirst Name\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\nAndrew\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\nJohn\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\nAndrew\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\nJohn\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\nAndrew\n\n\n\n\n\n\n\n\n# Here, we'll only consider `names` data from 2020\nnames_2020 = names[names[\"Year\"]==2020]\nnames_2020.head()\n\n\n\n\n\n\n\n\nName\nSex\nCount\nYear\nFirst Letter\n\n\n\n\n0\nOlivia\nF\n17641\n2020\nO\n\n\n1\nEmma\nF\n15656\n2020\nE\n\n\n2\nAva\nF\n13160\n2020\nA\n\n\n3\nCharlotte\nF\n13065\n2020\nC\n\n\n4\nSophia\nF\n13036\n2020\nS\n\n\n\n\n\n\n\nNow, we’re ready to merge the two tables. pd.merge is the pandas method used to merge DataFrames together.\n\nmerged = pd.merge(left = elections, right = names_2020, \\\n                  left_on = \"First Name\", right_on = \"Name\")\nmerged.head()\n# Notice that pandas automatically specifies `Year_x` and `Year_y` \n# when both merged DataFrames have the same column name to avoid confusion\n\n\n\n\n\n\n\n\nYear_x\nCandidate\nParty\nPopular vote\nResult\n%\nFirst Name\nName\nSex\nCount\nYear_y\nFirst Letter\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\nAndrew\nAndrew\nF\n12\n2020\nA\n\n\n1\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\nAndrew\nAndrew\nM\n6036\n2020\nA\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\nAndrew\nAndrew\nF\n12\n2020\nA\n\n\n3\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\nAndrew\nAndrew\nM\n6036\n2020\nA\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\nAndrew\nAndrew\nF\n12\n2020\nA\n\n\n\n\n\n\n\nLet’s take a closer look at the parameters:\n\nleft and right parameters are used to specify the DataFrames to be merge.\nleft_on and right_on parameters are assigned to the string names of the columns to be used when performing the merge. These two on parameters tell pandas what values should act as pairing keys to determine which rows to merge across the DataFrames. We’ll talk more about this idea of a pairing key next lecture.",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Concatenation and Merging"
    ]
  },
  {
    "objectID": "pandas/pandas3.html",
    "href": "pandas/pandas3.html",
    "title": "Selection, Filtering and Dropping",
    "section": "",
    "text": "In this section, we will learn how to extract and remove a subset of rows and columns in pandas. The two primary operations of data extraction are:\nLet’s start by loading the dataset.\nimport pandas as pd \n\nurl = \"https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv\"\n\nelections = pd.read_csv(url)",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Selection, Filtering and Dropping"
    ]
  },
  {
    "objectID": "pandas/pandas3.html#selection-subset-of-columns",
    "href": "pandas/pandas3.html#selection-subset-of-columns",
    "title": "Selection, Filtering and Dropping",
    "section": "Selection: subset of columns",
    "text": "Selection: subset of columns\nTo select a column in a DataFrame, we can use the bracket notation. That is, name of the DataFrame followed by the column name in square brackets: df['column_name'].\n\n\n\nFor example, to select a column named Candidate from the election DataFrame, we can use the following code:\n\ncandidates = elections['Candidate']\nprint(candidates)\n\n0         Andrew Jackson\n1      John Quincy Adams\n2         Andrew Jackson\n3      John Quincy Adams\n4         Andrew Jackson\n             ...        \n177           Jill Stein\n178         Joseph Biden\n179         Donald Trump\n180         Jo Jorgensen\n181       Howard Hawkins\nName: Candidate, Length: 182, dtype: object\n\n\nThis extracts a single column as a Series. We can confirm this by checking the type of the output.\n\ntype(candidates)\n\npandas.core.series.Series\n\n\nTo select multiple columns, we can pass a list of column names. For example, to select both Candidate and Votes columns from the election DataFrame, we can use the following line of code:\n\nelections[['Candidate', 'Party']]\n\n\n\n\n\n\n\n\nCandidate\nParty\n\n\n\n\n0\nAndrew Jackson\nDemocratic-Republican\n\n\n1\nJohn Quincy Adams\nDemocratic-Republican\n\n\n2\nAndrew Jackson\nDemocratic\n\n\n3\nJohn Quincy Adams\nNational Republican\n\n\n4\nAndrew Jackson\nDemocratic\n\n\n...\n...\n...\n\n\n177\nJill Stein\nGreen\n\n\n178\nJoseph Biden\nDemocratic\n\n\n179\nDonald Trump\nRepublican\n\n\n180\nJo Jorgensen\nLibertarian\n\n\n181\nHoward Hawkins\nGreen\n\n\n\n\n182 rows × 2 columns\n\n\n\nThis extracts multiple columns as a DataFrame. We can confirm as well this by checking the type of the output.\n\ntype(elections[['Candidate', 'Party']])\n\nThis is how we can select columns in a DataFrame. Next, let’s learn how to filter rows.",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Selection, Filtering and Dropping"
    ]
  },
  {
    "objectID": "pandas/pandas3.html#a-filtering-condition",
    "href": "pandas/pandas3.html#a-filtering-condition",
    "title": "Selection, Filtering and Dropping",
    "section": "A Filtering Condition",
    "text": "A Filtering Condition\nPerhaps the most interesting (and useful) method of selecting data from a Series is with a filtering condition.\nFirst, we apply a boolean condition to the Series. This create a new Series of boolean values.\n\nseries = pd.Series({'a': 1, 'b': 2, 'c': 3, 'd': 4})\nseries &gt; 2\n\na    False\nb    False\nc     True\nd     True\ndtype: bool\n\n\n\n\n\nWe then use this boolean condition to index into our original Series. pandas will select only the entries in the original Series that satisfy the condition.\n\nseries[series &gt; 2]\n\nc    3\nd    4\ndtype: int64",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Selection, Filtering and Dropping"
    ]
  },
  {
    "objectID": "pandas/pandas3.html#filtering-subset-of-rows",
    "href": "pandas/pandas3.html#filtering-subset-of-rows",
    "title": "Selection, Filtering and Dropping",
    "section": "Filtering: subset of rows",
    "text": "Filtering: subset of rows\nExtracting a subset of rows from a DataFrame is called filtering.\nWe can filter rows based on a boolean condition, similar to conditional statements (e.g., if, else) in Python.\n\n\n\nFor example, to filter rows of candidates who ran for elections since 2010, we can use the following code:\n\ncondition = election['Year'] &gt; 2010\n\nelection[condition]\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n168\n2012\nBarack Obama\nDemocratic\n65915795\nwin\n51.258484\n\n\n169\n2012\nGary Johnson\nLibertarian\n1275971\nloss\n0.992241\n\n\n170\n2012\nJill Stein\nGreen\n469627\nloss\n0.365199\n\n\n171\n2012\nMitt Romney\nRepublican\n60933504\nloss\n47.384076\n\n\n172\n2016\nDarrell Castle\nConstitution\n203091\nloss\n0.149640\n\n\n173\n2016\nDonald Trump\nRepublican\n62984828\nwin\n46.407862\n\n\n174\n2016\nEvan McMullin\nIndependent\n732273\nloss\n0.539546\n\n\n175\n2016\nGary Johnson\nLibertarian\n4489235\nloss\n3.307714\n\n\n176\n2016\nHillary Clinton\nDemocratic\n65853514\nloss\n48.521539\n\n\n177\n2016\nJill Stein\nGreen\n1457226\nloss\n1.073699\n\n\n178\n2020\nJoseph Biden\nDemocratic\n81268924\nwin\n51.311515\n\n\n179\n2020\nDonald Trump\nRepublican\n74216154\nloss\n46.858542\n\n\n180\n2020\nJo Jorgensen\nLibertarian\n1865724\nloss\n1.177979\n\n\n181\n2020\nHoward Hawkins\nGreen\n405035\nloss\n0.255731\n\n\n\n\n\n\n\nTo filter rows based on multiple conditions, we can use the & operator for AND and the | operator for OR.\nFor example, to filter rows of candidates who won the elections with less than 50% of the votes, we can use the following code:\n\ncondition = (election['Result'] == 'win') & (election['%'] &lt; 50)\n\nelection[condition]\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n16\n1848\nZachary Taylor\nWhig\n1360235\nwin\n47.309296\n\n\n20\n1856\nJames Buchanan\nDemocratic\n1835140\nwin\n45.306080\n\n\n23\n1860\nAbraham Lincoln\nRepublican\n1855993\nwin\n39.699408\n\n\n33\n1876\nRutherford Hayes\nRepublican\n4034142\nwin\n48.471624\n\n\n36\n1880\nJames Garfield\nRepublican\n4453337\nwin\n48.369234\n\n\n39\n1884\nGrover Cleveland\nDemocratic\n4914482\nwin\n48.884933\n\n\n43\n1888\nBenjamin Harrison\nRepublican\n5443633\nwin\n47.858041\n\n\n47\n1892\nGrover Cleveland\nDemocratic\n5553898\nwin\n46.121393\n\n\n70\n1912\nWoodrow Wilson\nDemocratic\n6296284\nwin\n41.933422\n\n\n74\n1916\nWoodrow Wilson\nDemocratic\n9126868\nwin\n49.367987\n\n\n100\n1948\nHarry Truman\nDemocratic\n24179347\nwin\n49.601536\n\n\n117\n1968\nRichard Nixon\nRepublican\n31783783\nwin\n43.565246\n\n\n140\n1992\nBill Clinton\nDemocratic\n44909806\nwin\n43.118485\n\n\n144\n1996\nBill Clinton\nDemocratic\n47400125\nwin\n49.296938\n\n\n152\n2000\nGeorge W. Bush\nRepublican\n50456002\nwin\n47.974666\n\n\n173\n2016\nDonald Trump\nRepublican\n62984828\nwin\n46.407862",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Selection, Filtering and Dropping"
    ]
  },
  {
    "objectID": "pandas/pandas3.html#extracting-subset-of-values",
    "href": "pandas/pandas3.html#extracting-subset-of-values",
    "title": "Selection, Filtering and Dropping",
    "section": "Extracting subset of values",
    "text": "Extracting subset of values\nTo extract a subset of values, we can use .loc[] or .iloc[] with row and column indices and labels respectively.\n\n\n\nThe .loc[] method is used to access a group of rows and columns by labels or a boolean array.\n\n.loc[row_labels, col_labels]\nThe .loc operator selects rows and columns in a DataFrame by their row and column label(s), respectively. The row labels (commonly referred to as the indices) are the bold text on the far left of a DataFrame, while the column labels are the column names found at the top of a DataFrame.\n\n\n\nTo grab data with .loc, we must specify the row and column label(s) where the data exists. The row labels are the first argument to the .loc function; the column labels are the second. For example, we can select the the row labeled 0 and the column labeled Candidate from the elections DataFrame.\n\nelections.loc[0, 'Candidate']\n\nTo select multiple rows and columns, we can use Python slice notation. Here, we select the rows from labels 0 to 3 and the columns from labels \"Year\" to \"Popular vote\".\n\nelections.loc[0:3, 'Year':'Popular vote']\n\nSuppose that instead, we wanted every column value for the first four rows in the elections DataFrame. The shorthand : is useful for this.\n\nelections.loc[0:3, :]\n\nThere are a couple of things we should note. Firstly, unlike conventional Python, Pandas allows us to slice string values (in our example, the column labels). Secondly, slicing with .loc is inclusive. Notice how our resulting DataFrame includes every row and column between and including the slice labels we specified.\nEquivalently, we can use a list to obtain multiple rows and columns in our elections DataFrame. elections.loc[[0, 1, 2, 3], [‘Year’, ‘Candidate’, ‘Party’, ‘Popular vote’]]\nLastly, we can interchange list and slicing notation. elections.loc[[0, 1, 2, 3], :]\n\n\n.iloc[row_indices, col_indices]\nThe .iloc[] method is used to access a group of rows and columns by integer position.\n\n\n\n\n\n\nCaution\n\n\n\nIf you find yourself needing to use .iloc then stop and think if you are about to implement a loop. If so, there is probably a better way to do it.\n\n\nSlicing with .iloc works similarily to .loc, however, .iloc uses the index positions of rows and columns rather the labels (think to yourself: loc uses labels; iloc uses indices). The arguments to the .iloc function also behave similarly -– single values, lists, indices, and any combination of these are permitted.\n\n\n\nLet’s begin reproducing our results from above. We’ll begin by selecting for the first presidential candidate in our elections DataFrame:\n\n# elections.loc[0, \"Candidate\"] - Previous approach\nelections.iloc[0, 1]\n\nNotice how the first argument to both .loc and .iloc are the same. This is because the row with a label of 0 is conveniently in the 0th index (equivalently, the first position) of the elections DataFrame. Generally, this is true of any DataFrame where the row labels are incremented in ascending order from 0.\nHowever, when we select the first four rows and columns using .iloc, we notice something.\n\n# elections.loc[0:3, 'Year':'Popular vote'] - Previous approach\nelections.iloc[0:4, 0:4]\n\nSlicing is no longer inclusive in .iloc -– it’s exclusive. In other words, the right-end of a slice is not included when using .iloc. This is one of the subtleties of pandas syntax; you will get used to it with practice.\n\n#elections.loc[[0, 1, 2, 3], ['Year', 'Candidate', 'Party', 'Popular vote']] - Previous Approach\nelections.iloc[[0, 1, 2, 3], [0, 1, 2, 3]]\n\nThis discussion begs the question: when should we use .loc vs .iloc? In most cases, .loc is generally safer to use. You can imagine .iloc may return incorrect values when applied to a dataset where the ordering of data can change.\n\n\n[]\nThe [] selection operator is the most baffling of all, yet the most commonly used. It only takes a single argument, which may be one of the following:\n\nA slice of row numbers\nA list of column labels\nA single column label\n\nThat is, [] is context dependent. Let’s see some examples.\nSay we wanted the first four rows of our elections DataFrame.\n\nelections[0:4]\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Selection, Filtering and Dropping"
    ]
  },
  {
    "objectID": "pandas/pandas3.html#dropping-rows-and-columns",
    "href": "pandas/pandas3.html#dropping-rows-and-columns",
    "title": "Selection, Filtering and Dropping",
    "section": "Dropping rows and columns",
    "text": "Dropping rows and columns\nTo drop rows and columns in a DataFrame, we can use the drop() method.\nFor example, to drop the first row from the election DataFrame, we can use the following code:\n\nelections.head()\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n\n\n\n\n\n\nelections.drop(columns=['Popular vote'])\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\nwin\n54.574789\n\n\n...\n...\n...\n...\n...\n...\n\n\n177\n2016\nJill Stein\nGreen\nloss\n1.073699\n\n\n178\n2020\nJoseph Biden\nDemocratic\nwin\n51.311515\n\n\n179\n2020\nDonald Trump\nRepublican\nloss\n46.858542\n\n\n180\n2020\nJo Jorgensen\nLibertarian\nloss\n1.177979\n\n\n181\n2020\nHoward Hawkins\nGreen\nloss\n0.255731\n\n\n\n\n182 rows × 5 columns\n\n\n\n\n# Drop the first row\nelections.drop(index=0)\n\n# Drop the first two rows\nelections.drop(index=[0, 1])\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n5\n1832\nHenry Clay\nNational Republican\n484205\nloss\n37.603628\n\n\n6\n1832\nWilliam Wirt\nAnti-Masonic\n100715\nloss\n7.821583\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n177\n2016\nJill Stein\nGreen\n1457226\nloss\n1.073699\n\n\n178\n2020\nJoseph Biden\nDemocratic\n81268924\nwin\n51.311515\n\n\n179\n2020\nDonald Trump\nRepublican\n74216154\nloss\n46.858542\n\n\n180\n2020\nJo Jorgensen\nLibertarian\n1865724\nloss\n1.177979\n\n\n181\n2020\nHoward Hawkins\nGreen\n405035\nloss\n0.255731\n\n\n\n\n180 rows × 6 columns",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Selection, Filtering and Dropping"
    ]
  },
  {
    "objectID": "pandas/pandas4.html",
    "href": "pandas/pandas4.html",
    "title": "Aggregation",
    "section": "",
    "text": "In this notebook, we will learn how to aggregate data using pandas. This generally entails grouping data by a certain column’s values and then applying a function to the groups.",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Aggregation"
    ]
  },
  {
    "objectID": "pandas/pandas4.html#aggregation-with-.groupby",
    "href": "pandas/pandas4.html#aggregation-with-.groupby",
    "title": "Aggregation",
    "section": "Aggregation with .groupby",
    "text": "Aggregation with .groupby\nUp until this point, we have been working with individual rows of DataFrames. As data scientists, we often wish to investigate trends across a larger subset of our data. For example, we may want to compute some summary statistic (the mean, median, sum, etc.) for a group of rows in our DataFrame. To do this, we’ll use pandas GroupBy objects.\n\n\n\n\n\n\n\n\n\nA groupby operation involves some combination of splitting a DataFrame into grouped subframes, applying a function, and combining the results.\nFor some arbitrary DataFrame df below, the code df.groupby(\"year\").sum() does the following:\n\nSplits the DataFrame into sub-DataFrames with rows belonging to the same year.\nApplies the sum function to each column of each sub-DataFrame.\nCombines the results of sum into a single DataFrame, indexed by year.\n\n\n\n\nLet’s say we had baby names for all years in a single DataFrame names\n\nimport urllib.request\nimport os.path\nimport pandas as pd \n\n# Download data from the web directly\ndata_url = \"https://www.ssa.gov/oact/babynames/names.zip\"\nlocal_filename = \"../data/names.zip\"\nif not os.path.exists(local_filename): # if the data exists don't download again\n    with urllib.request.urlopen(data_url) as resp, open(local_filename, 'wb') as f:\n        f.write(resp.read())\n\n        \n# Load data without unzipping the file\nimport zipfile\nnames = [] \nwith zipfile.ZipFile(local_filename, \"r\") as zf:\n    data_files = [f for f in zf.filelist if f.filename[-3:] == \"txt\"]\n    def extract_year_from_filename(fn):\n        return int(fn[3:7])\n    for f in data_files:\n        year = extract_year_from_filename(f.filename)\n        with zf.open(f) as fp:\n            df = pd.read_csv(fp, names=[\"Name\", \"Sex\", \"Count\"])\n            df[\"Year\"] = year\n            names.append(df)\nnames = pd.concat(names)\n\nnames\n\n\n\n\n\n\n\n\nName\nSex\nCount\nYear\n\n\n\n\n0\nMary\nF\n7065\n1880\n\n\n1\nAnna\nF\n2604\n1880\n\n\n2\nEmma\nF\n2003\n1880\n\n\n3\nElizabeth\nF\n1939\n1880\n\n\n4\nMinnie\nF\n1746\n1880\n\n\n...\n...\n...\n...\n...\n\n\n31677\nZyell\nM\n5\n2023\n\n\n31678\nZyen\nM\n5\n2023\n\n\n31679\nZymirr\nM\n5\n2023\n\n\n31680\nZyquan\nM\n5\n2023\n\n\n31681\nZyrin\nM\n5\n2023\n\n\n\n\n2117219 rows × 4 columns\n\n\n\n\nnames.to_csv(\"../data/names.csv\", index=False)\n\nNow, if we wanted to aggregate all rows in names for a given year, we would need names.groupby(\"Year\")\n\nnames.groupby(\"Year\")\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f7a30b64bb0&gt;\n\n\nWhat does this strange output mean? Calling .groupby has generated a GroupBy object. You can imagine this as a set of “mini” sub-DataFrames, where each subframe contains all of the rows from names that correspond to a particular year.\nThe diagram below shows a simplified view of names to help illustrate this idea.\n\n\n\nWe can’t work with a GroupBy object directly – that is why you saw that strange output earlier, rather than a standard view of a DataFrame. To actually manipulate values within these “mini” DataFrames, we’ll need to call an aggregation method. This is a method that tells pandas how to aggregate the values within the GroupBy object. Once the aggregation is applied, pandas will return a normal (now grouped) DataFrame.\nAggregation functions (.min(), .max(), .mean(), .sum(), etc.) are the most common way to work with GroupBy objects. These functions are applied to each column of a “mini” grouped DataFrame. We end up with a new DataFrame with one aggregated row per subframe. Let’s see this in action by finding the sum of all counts for each year in names – this is equivalent to finding the number of babies born in each year.\n\nnames.groupby(\"Year\").sum().head(5)\n\n\n\n\n\n\n\n\nCount\n\n\nYear\n\n\n\n\n\n1880\n201484\n\n\n1881\n192690\n\n\n1882\n221533\n\n\n1883\n216944\n\n\n1884\n243461\n\n\n\n\n\n\n\nWe can relate this back to the diagram we used above. Remember that the diagram uses a simplified version of names, which is why we see smaller values for the summed counts.\n\n\n\n\nCalling .agg has condensed each subframe back into a single row. This gives us our final output: a DataFrame that is now indexed by \"Year\", with a single row for each unique year in the original names DataFrame.\nYou may be wondering: where did the \"State\", \"Sex\", and \"Name\" columns go? Logically, it doesn’t make sense to sum the string data in these columns (how would we add “Mary” + “Ann”?). Because of this, pandas will simply omit these columns when it performs the aggregation on the DataFrame. Since this happens implicitly, without the user specifying that these columns should be ignored, it’s easy to run into troubling situations where columns are removed without the programmer noticing. It is better coding practice to select only the columns we care about before performing the aggregation.\n\n# Same result, but now we explicitly tell pandas to only consider the \"Count\" column when summing\nnames.groupby(\"Year\")[[\"Count\"]].sum().head(5)\n\n\n\n\n\n\n\n\nCount\n\n\nYear\n\n\n\n\n\n1880\n201484\n\n\n1881\n192690\n\n\n1882\n221533\n\n\n1883\n216944\n\n\n1884\n243461\n\n\n\n\n\n\n\nThere are many different aggregations that can be applied to the grouped data. The primary requirement is that an aggregation function must:\n\nTake in a Series of data (a single column of the grouped subframe)\nReturn a single value that aggregates this Series\n\nBecause of this fairly broad requirement, pandas offers many ways of computing an aggregation.\nIn-built Python operations – such as sum, max, and min – are automatically recognized by pandas.\n\n# What is the maximum count for each name in any year?\nnames.groupby(\"Name\")[[\"Count\"]].max().head()\n\n\n\n\n\n\n\n\nCount\n\n\nName\n\n\n\n\n\nAaban\n16\n\n\nAabha\n9\n\n\nAabid\n6\n\n\nAabidah\n5\n\n\nAabir\n5\n\n\n\n\n\n\n\n\n# What is the minimum count for each name in any year?\nnames.groupby(\"Name\")[[\"Count\"]].min().head()\n\n\n\n\n\n\n\n\nCount\n\n\nName\n\n\n\n\n\nAaban\n5\n\n\nAabha\n5\n\n\nAabid\n5\n\n\nAabidah\n5\n\n\nAabir\n5\n\n\n\n\n\n\n\n\n# What is the average count for each name across all years?\nnames.groupby(\"Name\")[[\"Count\"]].mean().head()\n\n\n\n\n\n\n\n\nCount\n\n\nName\n\n\n\n\n\nAaban\n10.000000\n\n\nAabha\n6.375000\n\n\nAabid\n5.333333\n\n\nAabidah\n5.000000\n\n\nAabir\n5.000000\n\n\n\n\n\n\n\npandas also offers a number of in-built functions for aggregation. Some examples include:\n\n.sum()\n.max()\n.min()\n.mean()\n.first()\n.last()\n\nThe latter two entries in this list – \"first\" and \"last\" – are unique to pandas. They return the first or last entry in a subframe column. Why might this be useful? Consider a case where multiple columns in a group share identical information. To represent this information in the grouped output, we can simply grab the first or last entry, which we know will be identical to all other entries.\nLet’s illustrate this with an example. Say we add a new column to names that contains the first letter of each name.\n\n# Imagine we had an additional column, \"First Letter\". We'll explain this code next week\nnames[\"First Letter\"] = names[\"Name\"].apply(lambda x: x[0])\n\n# We construct a simplified DataFrame containing just a subset of columns\nnames_new = names[[\"Name\", \"First Letter\", \"Year\"]]\nnames_new.head()\n\n\n\n\n\n\n\n\nName\nFirst Letter\nYear\n\n\n\n\n0\nMary\nM\n1880\n\n\n1\nAnna\nA\n1880\n\n\n2\nEmma\nE\n1880\n\n\n3\nElizabeth\nE\n1880\n\n\n4\nMinnie\nM\n1880\n\n\n\n\n\n\n\nIf we form groups for each name in the dataset, \"First Letter\" will be the same for all members of the group. This means that if we simply select the first entry for \"First Letter\" in the group, we’ll represent all data in that group.\nWe can use a dictionary to apply different aggregation functions to each column during grouping.\n\n\n\n\n\nnames_new.groupby(\"Name\").agg({\"First Letter\":\"first\", \"Year\":\"max\"}).head()\n\n\n\n\n\n\n\n\nFirst Letter\nYear\n\n\nName\n\n\n\n\n\n\nAaban\nA\n2019\n\n\nAabha\nA\n2021\n\n\nAabid\nA\n2018\n\n\nAabidah\nA\n2018\n\n\nAabir\nA\n2018\n\n\n\n\n\n\n\nWe can also define aggregation functions of our own! This can be done using either a def or lambda statement. Again, the condition for a custom aggregation function is that it must take in a Series and output a single scalar value.\n\ndef ratio_to_peak(series):\n    return series.iloc[-1]/max(series)\n\nnames.groupby(\"Name\")[[\"Year\", \"Count\"]].apply(ratio_to_peak)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nInput In [2], in &lt;cell line: 4&gt;()\n      1 def ratio_to_peak(series):\n      2     return series.iloc[-1]/max(series)\n----&gt; 4 names.groupby(\"Name\")[[\"Year\", \"Count\"]].apply(ratio_to_peak)\n\nNameError: name 'names' is not defined\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nlambda functions are a special type of function that can be defined in a single line. They are also often refered to as “anonymous” functions because these functions don’t have a name. They are useful for simple functions that are not used elsewhere in your code.\n\n\n\n# Alternatively, using lambda\nnames.groupby(\"Name\")[[\"Year\", \"Count\"]].agg(lambda s: s.iloc[-1]/max(s))\n\n\n\n\n\n\n\n\nYear\nCount\n\n\nName\n\n\n\n\n\n\nAaban\n1.0\n0.375000\n\n\nAabha\n1.0\n0.555556\n\n\nAabid\n1.0\n1.000000\n\n\nAabidah\n1.0\n1.000000\n\n\nAabir\n1.0\n1.000000\n\n\n...\n...\n...\n\n\nZyvion\n1.0\n1.000000\n\n\nZyvon\n1.0\n1.000000\n\n\nZyyanna\n1.0\n1.000000\n\n\nZyyon\n1.0\n1.000000\n\n\nZzyzx\n1.0\n1.000000\n\n\n\n\n101338 rows × 2 columns\n\n\n\n\nAggregation with lambda Functions\nWe’ll work with the elections DataFrame again.\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv\"\nelections = pd.read_csv(url)\nelections.head(5)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n\n\n\n\n\nWhat if we wish to aggregate our DataFrame using a non-standard function – for example, a function of our own design? We can do so by combining .agg with lambda expressions.\nLet’s first consider a puzzle to jog our memory. We will attempt to find the Candidate from each Party with the highest % of votes.\nA naive approach may be to group by the Party column and aggregate by the maximum.\n\nelections.groupby(\"Party\").agg(max).head(10)\n\n\n\n\n\n\n\n\nYear\nCandidate\nPopular vote\nResult\n%\n\n\nParty\n\n\n\n\n\n\n\n\n\nAmerican\n1976\nThomas J. Anderson\n873053\nloss\n21.554001\n\n\nAmerican Independent\n1976\nLester Maddox\n9901118\nloss\n13.571218\n\n\nAnti-Masonic\n1832\nWilliam Wirt\n100715\nloss\n7.821583\n\n\nAnti-Monopoly\n1884\nBenjamin Butler\n134294\nloss\n1.335838\n\n\nCitizens\n1980\nBarry Commoner\n233052\nloss\n0.270182\n\n\nCommunist\n1932\nWilliam Z. Foster\n103307\nloss\n0.261069\n\n\nConstitution\n2016\nMichael Peroutka\n203091\nloss\n0.152398\n\n\nConstitutional Union\n1860\nJohn Bell\n590901\nloss\n12.639283\n\n\nDemocratic\n2020\nWoodrow Wilson\n81268924\nwin\n61.344703\n\n\nDemocratic-Republican\n1824\nJohn Quincy Adams\n151271\nwin\n57.210122\n\n\n\n\n\n\n\nThis approach is clearly wrong – the DataFrame claims that Woodrow Wilson won the presidency in 2020.\nWhy is this happening? Here, the max aggregation function is taken over every column independently. Among Democrats, max is computing:\n\nThe most recent Year a Democratic candidate ran for president (2020)\nThe Candidate with the alphabetically “largest” name (“Woodrow Wilson”)\nThe Result with the alphabetically “largest” outcome (“win”)\n\nInstead, let’s try a different approach. We will:\n\nSort the DataFrame so that rows are in descending order of %\nGroup by Party and select the first row of each sub-DataFrame\n\nWhile it may seem unintuitive, sorting elections by descending order of % is extremely helpful. If we then group by Party, the first row of each groupby object will contain information about the Candidate with the highest voter %.\n\nelections_sorted_by_percent = elections.sort_values(\"%\", ascending=False)\nelections_sorted_by_percent.head(5)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n114\n1964\nLyndon Johnson\nDemocratic\n43127041\nwin\n61.344703\n\n\n91\n1936\nFranklin Roosevelt\nDemocratic\n27752648\nwin\n60.978107\n\n\n120\n1972\nRichard Nixon\nRepublican\n47168710\nwin\n60.907806\n\n\n79\n1920\nWarren Harding\nRepublican\n16144093\nwin\n60.574501\n\n\n133\n1984\nRonald Reagan\nRepublican\n54455472\nwin\n59.023326\n\n\n\n\n\n\n\n\nelections_sorted_by_percent.groupby(\"Party\").agg(lambda x : x.iloc[0]).head(10)\n\n# Equivalent to the below code\n# elections_sorted_by_percent.groupby(\"Party\").agg('first').head(10)\n\n\n\n\n\n\n\n\nYear\nCandidate\nPopular vote\nResult\n%\n\n\nParty\n\n\n\n\n\n\n\n\n\nAmerican\n1856\nMillard Fillmore\n873053\nloss\n21.554001\n\n\nAmerican Independent\n1968\nGeorge Wallace\n9901118\nloss\n13.571218\n\n\nAnti-Masonic\n1832\nWilliam Wirt\n100715\nloss\n7.821583\n\n\nAnti-Monopoly\n1884\nBenjamin Butler\n134294\nloss\n1.335838\n\n\nCitizens\n1980\nBarry Commoner\n233052\nloss\n0.270182\n\n\nCommunist\n1932\nWilliam Z. Foster\n103307\nloss\n0.261069\n\n\nConstitution\n2008\nChuck Baldwin\n199750\nloss\n0.152398\n\n\nConstitutional Union\n1860\nJohn Bell\n590901\nloss\n12.639283\n\n\nDemocratic\n1964\nLyndon Johnson\n43127041\nwin\n61.344703\n\n\nDemocratic-Republican\n1824\nAndrew Jackson\n151271\nloss\n57.210122\n\n\n\n\n\n\n\nHere’s an illustration of the process:\n\n\n\nNotice how our code correctly determines that Lyndon Johnson from the Democratic Party has the highest voter %.\nMore generally, lambda functions are used to design custom aggregation functions that aren’t pre-defined by Python. The input parameter x to the lambda function is a GroupBy object. Therefore, it should make sense why lambda x : x.iloc[0] selects the first row in each groupby object.\nIn fact, there’s a few different ways to approach this problem. Each approach has different tradeoffs in terms of readability, performance, memory consumption, complexity, etc. We’ve given a few examples below.\nNote: Understanding these alternative solutions is not required. They are given to demonstrate the vast number of problem-solving approaches in pandas.\n\n# Using the idxmax function\nbest_per_party = elections.loc[elections.groupby('Party')['%'].idxmax()]\nbest_per_party.head(5)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n22\n1856\nMillard Fillmore\nAmerican\n873053\nloss\n21.554001\n\n\n115\n1968\nGeorge Wallace\nAmerican Independent\n9901118\nloss\n13.571218\n\n\n6\n1832\nWilliam Wirt\nAnti-Masonic\n100715\nloss\n7.821583\n\n\n38\n1884\nBenjamin Butler\nAnti-Monopoly\n134294\nloss\n1.335838\n\n\n127\n1980\nBarry Commoner\nCitizens\n233052\nloss\n0.270182\n\n\n\n\n\n\n\n\n# Using the .drop_duplicates function\nbest_per_party2 = elections.sort_values('%').drop_duplicates(['Party'], keep='last')\nbest_per_party2.head(5)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n148\n1996\nJohn Hagelin\nNatural Law\n113670\nloss\n0.118219\n\n\n164\n2008\nChuck Baldwin\nConstitution\n199750\nloss\n0.152398\n\n\n110\n1956\nT. Coleman Andrews\nStates' Rights\n107929\nloss\n0.174883\n\n\n147\n1996\nHoward Phillips\nTaxpayers\n184656\nloss\n0.192045\n\n\n136\n1988\nLenora Fulani\nNew Alliance\n217221\nloss\n0.237804\n\n\n\n\n\n\n\n\n\nOther GroupBy Features\nThere are many aggregation methods we can use with .agg. Some useful options are:\n\n.mean: creates a new DataFrame with the mean value of each group\n.sum: creates a new DataFrame with the sum of each group\n.max and .min: creates a new DataFrame with the maximum/minimum value of each group\n.first and .last: creates a new DataFrame with the first/last row in each group\n.size: creates a new Series with the number of entries in each group\n.count: creates a new DataFrame with the number of entries, excluding missing values.\n\nNote the slight difference between .size() and .count(): while .size() returns a Series and counts the number of entries including the missing values, .count() returns a DataFrame and counts the number of entries in each column excluding missing values. Here’s an example:\n\ndf = pd.DataFrame({'letter':['A','A','B','C','C','C'], \n                   'num':[1,2,3,4,None,4], \n                   'state':[None, 'tx', 'fl', 'hi', None, 'ak']})\ndf\n\n\n\n\n\n\n\n\nletter\nnum\nstate\n\n\n\n\n0\nA\n1.0\nNone\n\n\n1\nA\n2.0\ntx\n\n\n2\nB\n3.0\nfl\n\n\n3\nC\n4.0\nhi\n\n\n4\nC\nNaN\nNone\n\n\n5\nC\n4.0\nak\n\n\n\n\n\n\n\n\ndf.groupby(\"letter\").size()\n\nletter\nA    2\nB    1\nC    3\ndtype: int64\n\n\n\ndf.groupby(\"letter\").count()\n\n\n\n\n\n\n\n\nnum\nstate\n\n\nletter\n\n\n\n\n\n\nA\n2\n1\n\n\nB\n1\n1\n\n\nC\n2\n2\n\n\n\n\n\n\n\nYou might recall that the value_counts() function in the previous note does something similar. It turns out value_counts() and groupby.size() are the same, except value_counts() sorts the resulting Series in descending order automatically.\n\ndf[\"letter\"].value_counts()\n\nC    3\nA    2\nB    1\nName: letter, dtype: int64\n\n\nhese (and other) aggregation functions are so common that pandas allows for writing shorthand. Instead of explicitly stating the use of .agg, we can call the function directly on the GroupBy object.\nFor example, the following are equivalent:\n\nelections.groupby(\"Candidate\").agg(mean)\nelections.groupby(\"Candidate\").mean()\n\nThere are many other methods that pandas supports. You can check them out on the pandas documentation.\n\n\n\n\n\n\nFiltering by Group\nAnother common use for GroupBy objects is to filter data by group.\ngroupby.filter takes an argument \\(\\text{f}\\), where \\(\\text{f}\\) is a function that:\n\nTakes a DataFrame object as input\nReturns a single True or False for the each sub-DataFrame\n\nSub-DataFrames that correspond to True are returned in the final result, whereas those with a False value are not. Importantly, groupby.filter is different from groupby.agg in that an entire sub-DataFrame is returned in the final DataFrame, not just a single row. As a result, groupby.filter preserves the original indices.\nTo illustrate how this happens, consider the following .filter function applied on some arbitrary data. Say we want to identify “tight” election years – that is, we want to find all rows that correspond to elections years where all candidates in that year won a similar portion of the total vote. Specifically, let’s find all rows corresponding to a year where no candidate won more than 45% of the total vote.\nIn other words, we want to:\n\nFind the years where the maximum % in that year is less than 45%\nReturn all DataFrame rows that correspond to these years\n\nFor each year, we need to find the maximum % among all rows for that year. If this maximum % is lower than 45%, we will tell pandas to keep all rows corresponding to that year.\n\nelections.groupby(\"Year\").filter(lambda sf: sf[\"%\"].max() &lt; 45).head(9)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n23\n1860\nAbraham Lincoln\nRepublican\n1855993\nwin\n39.699408\n\n\n24\n1860\nJohn Bell\nConstitutional Union\n590901\nloss\n12.639283\n\n\n25\n1860\nJohn C. Breckinridge\nSouthern Democratic\n848019\nloss\n18.138998\n\n\n26\n1860\nStephen A. Douglas\nNorthern Democratic\n1380202\nloss\n29.522311\n\n\n66\n1912\nEugene V. Debs\nSocialist\n901551\nloss\n6.004354\n\n\n67\n1912\nEugene W. Chafin\nProhibition\n208156\nloss\n1.386325\n\n\n68\n1912\nTheodore Roosevelt\nProgressive\n4122721\nloss\n27.457433\n\n\n69\n1912\nWilliam Taft\nRepublican\n3486242\nloss\n23.218466\n\n\n70\n1912\nWoodrow Wilson\nDemocratic\n6296284\nwin\n41.933422\n\n\n\n\n\n\n\nWhat’s going on here? In this example, we’ve defined our filtering function, \\(\\text{f}\\), to be lambda sf: sf[\"%\"].max() &lt; 45. This filtering function will find the maximum \"%\" value among all entries in the grouped sub-DataFrame, which we call sf. If the maximum value is less than 45, then the filter function will return True and all rows in that grouped sub-DataFrame will appear in the final output DataFrame.\nExamine the DataFrame above. Notice how, in this preview of the first 9 rows, all entries from the years 1860 and 1912 appear. This means that in 1860 and 1912, no candidate in that year won more than 45% of the total vote.\nYou may ask: how is the groupby.filter procedure different to the boolean filtering we’ve seen previously? Boolean filtering considers individual rows when applying a boolean condition. For example, the code elections[elections[\"%\"] &lt; 45] will check the \"%\" value of every single row in elections; if it is less than 45, then that row will be kept in the output. groupby.filter, in contrast, applies a boolean condition across all rows in a group. If not all rows in that group satisfy the condition specified by the filter, the entire group will be discarded in the output.",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Aggregation"
    ]
  },
  {
    "objectID": "pandas/pandas4.html#aggregation-with-.pivot_table",
    "href": "pandas/pandas4.html#aggregation-with-.pivot_table",
    "title": "Aggregation",
    "section": "Aggregation with .pivot_table",
    "text": "Aggregation with .pivot_table\nWe know now that .groupby gives us the ability to group and aggregate data across our DataFrame. The examples above formed groups using just one column in the DataFrame. It’s possible to group by multiple columns at once by passing in a list of column names to .groupby.\nLet’s consider the names dataset. In this problem, we will find the total number of baby names associated with each sex for each year. To do this, we’ll group by both the \"Year\" and \"Sex\" columns.\n\nnames.head()\n\n\n\n\n\n\n\n\nName\nSex\nCount\nYear\nFirst Letter\n\n\n\n\n0\nMary\nF\n7065\n1880\nM\n\n\n1\nAnna\nF\n2604\n1880\nA\n\n\n2\nEmma\nF\n2003\n1880\nE\n\n\n3\nElizabeth\nF\n1939\n1880\nE\n\n\n4\nMinnie\nF\n1746\n1880\nM\n\n\n\n\n\n\n\n\n# Find the total number of baby names associated with each sex for each year in the data\nnames.groupby([\"Year\", \"Sex\"])[[\"Count\"]].sum().head(6)\n\n\n\n\n\n\n\n\n\nCount\n\n\nYear\nSex\n\n\n\n\n\n1880\nF\n90994\n\n\nM\n110490\n\n\n1881\nF\n91953\n\n\nM\n100737\n\n\n1882\nF\n107847\n\n\nM\n113686\n\n\n\n\n\n\n\nNotice that both \"Year\" and \"Sex\" serve as the index of the DataFrame (they are both rendered in bold). We’ve created a multi-index DataFrame where two different index values, the year and sex, are used to uniquely identify each row.\nThis isn’t the most intuitive way of representing this data – and, because multi-indexed DataFrames have multiple dimensions in their index, they can often be difficult to use.\nAnother strategy to aggregate across two columns is to create a pivot table. One set of values is used to create the index of the pivot table; another set is used to define the column names. The values contained in each cell of the table correspond to the aggregated data for each index-column pair.\nThe best way to understand pivot tables is to see one in action. Let’s return to our original goal of summing the total number of names associated with each combination of year and sex. We’ll call the pandas .pivot_table method to create a new table.\n\n# The `pivot_table` method is used to generate a Pandas pivot table\nnames.pivot_table(\n    index = \"Year\", \n    columns = \"Sex\", \n    values = \"Count\", \n    aggfunc = sum).head(5)\n\n\n\n\n\n\n\nSex\nF\nM\n\n\nYear\n\n\n\n\n\n\n1880\n90994\n110490\n\n\n1881\n91953\n100737\n\n\n1882\n107847\n113686\n\n\n1883\n112319\n104625\n\n\n1884\n129019\n114442\n\n\n\n\n\n\n\nLooks a lot better! Now, our DataFrame is structured with clear index-column combinations. Each entry in the pivot table represents the summed count of names for a given combination of \"Year\" and \"Sex\".\nLet’s take a closer look at the code implemented above.\n\nindex = \"Year\" specifies the column name in the original DataFrame that should be used as the index of the pivot table\ncolumns = \"Sex\" specifies the column name in the original DataFrame that should be used to generate the columns of the pivot table\nvalues = \"Count\" indicates what values from the original DataFrame should be used to populate the entry for each index-column combination\naggfunc = sum tells pandas what function to use when aggregating the data specified by values. Here, we are summing the name counts for each pair of \"Year\" and \"Sex\"\n\n\n\n\n\nWe can even include multiple values in the index or columns of our pivot tables.\n\nnames_pivot = names.pivot_table(\n    index=\"Year\",     # the rows (turned into index)\n    columns=\"Sex\",    # the column values\n    values=[\"Count\", \"Name\"], \n    aggfunc=max,   # group operation\n)\nnames_pivot.head(6)\n\n\n\n\n\n\n\n\nCount\nName\n\n\nSex\nF\nM\nF\nM\n\n\nYear\n\n\n\n\n\n\n\n\n1880\n7065\n9655\nZula\nZeke\n\n\n1881\n6919\n8769\nZula\nZeb\n\n\n1882\n8148\n9557\nZula\nZed\n\n\n1883\n8012\n8894\nZula\nZeno\n\n\n1884\n9217\n9388\nZula\nZollie\n\n\n1885\n9128\n8756\nZula\nZollie",
    "crumbs": [
      "Home",
      "VECTORIZED OPERATIONS",
      "Aggregation"
    ]
  }
]